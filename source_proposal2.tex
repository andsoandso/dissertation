\documentclass[doc,12pt]{apa}        % use: 'man' for submission type; 'jou' for
                                % journal type, and 'doc' for typical latex
                                % but with figures inline with text
\usepackage{geometry} 
%\geometry{a4paper} 
\usepackage[parfill]{parskip}   % paragraphs delimited by an empty line

\usepackage{graphicx} 
\usepackage{amssymb}            % no idea what this does...
\usepackage{epstopdf}           % no idea what this does...
%\usepackage{gensymb}            % no idea what this does...

\usepackage{setspace}

\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png} \setcounter{secnumdepth}{0}  % no idea what this does...

\usepackage{apacite}
%%%%%%%%% END HEADER %%%%%%%%%

\title{Categories of rewards.} 
\author{Erik J. Peterson} \affiliation{Dept. of Psychology \\ Colorado State University \\ Fort Collins, CO} 

%%%%%%%%%%%%%%%%
\begin{document} 
%%%%%%%%%%%%%%%%
\maketitle
\doublespacing

\section{Introduction} % (fold)
\label{sec:introduction}
Birds will peck repeatedly, as mice will push levers, monkeys will hit buttons, and men will buy flowers, if each of these actions is followed by a primary reward -- food and sex.  Buttons, levers and flowers have no value alone, so reinforcement theory goes, it is only by the \emph{statiscally regular pairing} with primary rewards that value is transfered.  This is, what I'll call, the classical view.  And while it is an inadaquate theory in some other regards I'll not discuss (REFS), it has in general held up many years now.  In fact, the neural basis of such learning currently receives much attention, notable progress is bieng made.  However, reinforcement learning theories can't account for two new key findings in the nueral correlates of human learning. (1) Rewards seem to neurally appear by cognition alone. (2) Value can be transfered by inference, no pairing is needed.  For the first time using a mixture of fMRI and computational modeling, I examine possible united mechanisms of both of these aspects. By treating rewards as a kind of category I also offer a framework for extending formal theories of human reinforcement learning to some cognitive and inferential cases.

This introduction has six parts.  First I discuss classical rewards and their neural correlates.  Second I make a case for cognitive rewards, discussing as an example novelty, then moving onto other examples, and finally arguing for the necessity of generalizable reward representations.  Third is a discussion of prior studies of reward generalization in pigeons and other non-human animals as well as in humans, though the literature on the latter is sparse.   All of the above will be done under the banner of the reward prediction error hypothesis of phasic dopamine function (``RPE hypothesis'' from here on).  Which brings us to the fourth section, a diversion discussing alternative non-rewarding theories of phasic dopamine.  Fifth, I breifly review formal models of categorization, which leads into the sixth and final section, the specfic goals and methods of this work.

\subsubsection{The classics.} % (fold)
\label{sub:the_classics}
% * Phasic DA has something to do with signaling rewards - discuss past best links.
% How DA effects striatum and the importance of their interaction on learning.
Classically rewards and reinforcers have been linked to (or simply were) food \cite{ODoherty:2006p2875}, pain \cite{Becerra:2011p7581,schultz:2007aa} and sex though for, err, logistical reasons this is less often used in the laboratory, especially in human subjects; however physical contact with a loved one has been shown to activate the reward circuitry \cite{Izuma:2008p2822,Fliessbach:2007gf}).  These rewards are certainly potent, being used for over 50 successful years in studying learning in animal models \cite{iversen:2007aa} and people \cite{Kim:2010p7248,Montague:2006mz}.  That animals want food and sex is hardly a revelation, but that there may be a singluar chemical signal for the reciept of such items was a revelation, and one of the first  

TODO -- add section on past theories of reward and dopamine... context for the RPE hyptothesis

% Below 

The VTA/SNc is a small brainstem nucleus whose dopamine-releasing neurons project strongly to both the striatum and the hippocampus.  Electrophysiological recordings of VTA/SNc neurons show two firing modes -- tonic and phasic \cite{DawNW:2006p6343}.  The phasic mode is of interest here.  It has long been known that phasic firing in VTA/SNc immediately follows reward delivery \cite{iversen:2007aa}. \citeNP{Mirenowicz:1994p7185}, observed that the magnitude of phasic activity was dependent not on the absolute or relative value of a reward, as was previously thought, but instead was related to both the value of the reward and how expected that reward was.  This dependence on expectation was similar, the authors noted, to the reward prediction error signal generated in reinforcement learning models.  

\citeNP{Fiorillo:2003p6375}, along with \citeNP{Bayer:2005ul}, quantified the relationship between the unexpectedness of a reward and phasic activity in dopamine neurons.  Both groups showed that the RPE from their reinforcement learning models was strongly correlated to the observed dopamine response.  Complementing these electrophysiological recordings of monkey VTA/SNc, fMRI experiments in humans (for example, \citeNP{ODoherty:2003p6329}), as well as recordings in rat \cite{Roesch:2007p2519}, have also found the characteristic patterns of the RPE hypothesis: a phasic increase with unexpected rewards and a phasic depression when expected rewards were omitted.  These latter studies have also showed another important consistency between the RPE signal and phasic dopamine activity -- back-propagation.  For example, if an image of a green arrow often precedes reward delivery, recordings in VTA/SNc will initially show phasic activity immediately after reward delivery.  However, trial after trial this reward-related response will decrease, while simultaneously a phasic response to the green arrow will develop \cite{Roesch:2007p2519}.  That is, the reward response back-propagates to an informative stimulus. 

\subsection{RL Computations.} % (fold)
\label{sub:rk_computations}

% TODO Change the below to RW rule - the one I use in the work...
One of the simplest reinforcement learning models, the Rescorla-Wagner rule, proceeds as follows: At time $t+1$ an agent selects an action prompted by a stimulus, denoted here as an S-R (stimulus-response) pair or as $v(s,a,t+1)$. Action selection is followed by reward $r(t+1)$, confined to values of either 1 or 0, present or absent.  Reward is then compared to the current estimate of value.  This comparison is the reward prediction error (RPE) seen in \emph{Eq 1}.   If the reward is greater than expected a positive RPE results.  If the reward is less than expected, a negative RPE is generated.  If expectations are perfectly met the RPE signal is zero.  The RPE is then used by the agent to update the value of the S-R pair, increasing or decreasing its value, respectively, if the RPE was positive or negative (\emph{Eq. 2}).  If the RPE was zero the value of the S-R pair is unchanged.  The rate at which S-R pair's value changes depends (in nearly all models of human learning) on one free parameter, the learning rate ($\alpha$).  Just as the RPE signal is necessary for a computational agent to improve its performance, so too, the theory goes, is the phasic firing of dopamine neurons necessary for an animal to learn from reward.

\begin{equation} RPE = r(t+1) - v(s,a,t) \end{equation}
\begin{equation} v(s,a,t) \leftarrow v(s,a,t) + \alpha*RPE \end{equation}
  
% subsection phasic_dopamine_and_the_reward_prediction_error_hypothesis (end)

\subsection{Physiological basis of VTA/SNc RPE-like firing} % (fold)
\label{sub:physiological_basis_of_vta_snc_phasic_firing}
VTA/SNc receives input from the internal globus pallidus or GPi (a major output structure of the basal ganglia with widespread cortical connections), thalamus, and the central nucleus of the amygdala \cite{Botvinick:2008p6594}.  Recent work though has highlighted the habenula (a small nucleus posterior to the thalamus) as being especially important in generating RPE-like phasic activity.  The lateral habenula has reciprocal connections to the GPi and projections into the VTA/SNc. Based on this anatomy, it has been suggested that this nucleus serves a point of intersection between the striatum and the limbic system (e.g. the amygdala, hippocampus and the serotonergic dorsal raphe nucleus) \cite{Hikosaka:2008p4455}.  The habenula acts to tonically inhibit or disinhibit dopamine release in VTA/SNc neurons.  As habenula activity decreases, burst firing in VTA/SNc results; as habenula firing increases VTA/SNc firing is temporarily paused.  Dual recordings of the habenula and GPi suggest they form a functional loop capable of calculating the value of S-R pairs \cite{BrombergMartin:2010p7221}.  Reversible chemical inhibition of the habenula also increases VTA/SNc phasic activity.  Lesions to the habenula also result in marked increases in dopamine levels in the dorsal and ventral striatum \cite{BrombergMartin:2010p7221}.  In summary, the GPi (withs its access to cortical inputs via the striatum) and habenula (with its capability for altering VTA/SNc activity) may form the physiological loop necessary to calculate the RPE. Though the precise origins of the terms of \emph{Eq 1} and \emph{2}. remain unclear, \citeNP{BrombergMartin:2010p7218}, hint that this loop can signal both initial value estimates ($v(s,a,t)$, \emph{Eq 2}.) and rewarding outcomes ($r(t+1)$, \emph{Eq 1}.).

Add section on Superior Colliculus?
% subsection physiological_basis_of_vta_snc_phasic_firing (end)

\subsection{Dopamine and the striatum} % (fold)
\label{sub:dopamine_and_the_basal_ganglia}
The striatum is an input area of the basal ganglia, a brain region highly involved in categorization, logical inference, habit formation, working memory and feedback mediated S-R learning \cite{Frank:2001p1996,Jin:2010p7199,SchmitzerTorbert:2004p5410,Seger:2008p6401,Seger:2010p7189,Yin:2006p5080}.  In S-R learning, two of the five striatal subregions (the head of the caudate and the ventral striatum) process reward information \cite{Yin:2005p5101,Yin:2008p6347,Schonberg:2009p6669}.  These two are highly innervated by projections from the VTA/SNc, but only the ventral striatum correlates with the RPE signal \cite{Haruno:2006p3979,Seger:2010p7189}.  The remaining three regions (the body and tail of the caudate and the putamen) are involved with S-R pair formation, visual categorization and response selection, respectively \cite{Seger:2008p6401,Seger:2010p7189}.  Though these three also receive VTA/SNc projections and are sometimes sensitive to reward level \cite{BischoffGrethe:2009p4570}, the BOLD signal does not correlate with the RPE \cite{Seger:2010p7189}; dopamine's exact role in these areas is less clear.  Overall though, intact dopamine projections and complete striatal function is necessary for rapid S-R learning.
   
Administering dopamine antagonists to human and non-human animals adversely affects S-R learning \cite{Pizzagalli:2010p7205}, as does lesioning the VTA/SNc.  Complete lesions of the striatum also prevent S-R learning \cite{Packard:2002p5074}.  Administering dopamine agonists or the readily converted precursor L-DOPA leads to increases in response vigor and the ability of a Pavlovian-conditioned stimulus to bias unrelated instrumental responses (i.e. pavlovian instrumental transfer or PIT) \cite{Winterbauer:2007p6352}. Both PIT and response vigor are, in part, facilitated by phasic dopamine increasing activity in the ventral striatum.  Unmedicated Parkinson's patients, who have low striatal dopamine levels, show marked decreases in S-R learning with rewarding outcomes when compared to patients on medication and healthy age and intellect matched controls \cite{Pizzagalli:2010p7205}.  These same patients show an enhanced capability to learn from negative feedback which suggests that decreases in dopamine convey negative outcome information \cite{Frank:2004p4709}.  Finally, there is a solid body of evidence suggesting that phasic dopamine alters the plasticity of neurons in the striatum which presumably facilitates stimulus-response learning \cite{Calabresi:2007p4284}.

Section on Frank.  

Section on Gurney.  ?
% subsection dopamine_and_the_basal_ganglia (end)

% TODO MORE CONTENT, integrate the above...

% LEAD IN for next
However these kinds of rewards are not the only event that can lead to phasic firing the in the VTA/SNc accompanied by activity changes in the striatum (i.e. activation of the reward circuitry).  Novelty is an example.
% subsection the_classics (end)

\subsection{It's novel} % (fold)
\label{sub:it_s_novel}
% KEY:  Its is not primary, and it can't be secondary - it's novel, there are no posssible past associations.
% * define novelty
% * discuss Reed (both visual and auditory; reed showed novelty did not require reward nearby) then Shultz initial finding and later supporting work - just 			
%	recordings/fMRI (Zink) first.
% * then move onto memory studies of rewards and novelty

Anything in principle, and so far I am aware in practice, can be novel (i.e. contextually unexpected) but if anything can be novel and novelty is some kind of reward, this implies that every new experience is somehow (instantly) linked to another primary or secondary reinforcer (i.e. reward \citeNP{Bjorklund:2007p2585}).  This would require a flexible rapid abstract remapping of the novel experience to a previously learned reward.  However it is difficult to see which reward episode would be used.  It could be any really, say a sip of raspberry juice taken after a long run on the afternoon of July the 2nd 1982; however, memory is not nearly so precise and this seems unlikely.  Perhaps instead the brain searches the current context for a relevant rewarding episode to bind the novelty to?  If this is case, then does similarity of the current situation to the past affect the reward's value?  It is unclear.  In any case though it would be simpler if novelty were conceptualized as a cognitive reward, one that requires evaluation of current sense experience and past memories, but is ultimately derived endogenously.  Reviewing again the literature with these types of cognitive rewards in mind led to other examples (see \emph{Introduction}).  In fact, that are more than enough examples to assert that cognitive rewards are fact.  So then I began to wonder what other under-appreciated or undiscovered complexities reward representations might possess.  Cognitive rewards could certainly be more flexible than evolutionarily primitive rewards; perhaps instead of being coded as individual exemplars (e.g., each tasty sip of juice is a wholly independent neural entity), rewards are actually represented as categories.  That is while rewards can facilitate category learning perhaps, quasi-paradoxically, they are categories themselves\ldots.
% subsection it_s_novel (end)

\subsection{Thinking about other rewards} % (fold)
\label{sub:other_cognitive_rewards}

\citeNP{Tricomi:2008p6663}, showed ventral striatum BOLD signal changes in a declarative memory task in which subjects were initially trained with feedback (``Right'' or ``Wrong'') to distinguish 60 correct from incorrect word pairs.  In the subsequent two rounds explicit feedback was withheld but activity in the caudate was observed when correct pairings were matcheds based on memory alone.  Correct matches, that is goal achievement, led to strong activity.  In two economic decision making tasks strong ventral striatum signals were observed when participants were required merely to imagine or consider alternative outcomes  \cite{Hayden:2009p6545,Lohrenz:2007p7240}.  Information about the future is rewarding as well; \citeNP{BrombergMartin:2009p7220}, showed that complex visual clues about an upcoming outcome were in themselves sufficient to cause bursts of firing the in VTA/SNc.  Even the relatively simple cases of temporal discounting of rewards and the assessment of their uncertainty likely requires cognitive intervention, which is reflected in several reports of complex, multi-valued, reward-related signals in both dorsal and ventral-medial prefrontal cortices \cite{Tobler:2009p8302,Wallis:2010p8303,Kim:2009p8304,Seymour:2008p6518}.

Given that (some) reward representations are mediated by complex cognition, notably including goal-achievement and imagined outcomes,  then it is quite possible that these same reward representations may have a categorical (i.e. generalizable) aspect.  Pigeons have long been used to study perceptual categorization in rewarding contexts, including many studies where the effect of altering a previously conditioned stimuli was assessed.  For example, \citeNP{Guttman:1956p8355}, varied a preconditioned 570 nm light from 480-610, showing that while the bird's pecking rate (i.e. response vigor) decreased as one moved farther from 540, the birds still responded, that they is generalized.  When novel variations of conditioned stimuli from two sensory modalities were mixed similar graded changes in vigor were observed. However, not all combinations were effective; some combinations produced no responses at all \cite{Blough:2001p8408,Simmons:2008p8405,Urcuioli:2001p8359}.  Unfortunately though much of this behavioral work has never been examined in other model systems nor with modern neural recording and imaging technologies.  One of the few (perhaps the only) fMRI studies to examine conditioned stimuli in new contexts was that of \citeNP{Kahnt:2010p7677}. They first trained participants on the independent values of several colors, shapes and patterns (e.g. a white diamond, a green background, and a set of leftward moving dots).  Then they presented a pair of distinct combinations of the initial stimuli.  The participants then had to select the most valuable based on each stimuli's combined value.  By employing machine learning methods they showed that the combined value was encoded in the ventro-medial PFC while the variation in value among the combined stimuli of was encoded in the dorsolateral prefrontal cortex (dlPFC).  Unfortunately they did not examine activity in either the striatum or VTA/SNc.  It has however been shown that when a novel combination of previously studied options is presented to rats in a stimulus-response learning task the resulting dopaminergic firing correlates with the better option, indicating generalization of reward knowledge to the new context \cite{Roesch:2007p2519}.

\subsection{A salient diversion} % (fold)
\label{sub:sidebar_for_saliance}

% subsection sidebar_for_saliance (end)

% * Do a section in Sign's intrinsically motivated rewards. Or at least read this anyway to see if it fits.

% subsection other_cognitive_rewards (end)


\subsection{Goals} % (fold)
\label{sub:goals}
Finally, the goals of this proposal are two fold.  Goal one (which is arguably complete, see \emph{Behavioral Results}) is to establish that categorical reward representations can be used to infer rewarding properties for never before seen exemplars, so mediating successful learning.  Goal 2 is to examine the computational underpinnings of reward inference: is reward value assigned as a function of category alone or instead does degree of similarity of an exemplar to the category prototype affect its value? Said concretely, if a participant is trained on a perceptual category consisting of sinusoidal gratings (as is the case here) are gratings from the category ``+\$1'' equivalent irrespective of their distance from the category center or does the value of a grating change depending on classification certainty (i.e. distance from center).  This goal will be approached from two directions.  In the first approach, the fit of two reinforcement learning models to BOLD data from the dopaminergic midbrain and ventral striatum will be compared.  Model 1 will use binomial (constant) rewards. In model 2 reward value will be diminished as similarity to the category prototype decreases.  The second approach will be to examine how the RPEs from the two Rescorla-Wagner models mediate the coupling between the (dorsal-lateral and ventral-medial) PFC regions of interest and the ventral and dorsal striatum.  Recent electrophysiological and fMRI studies have suggested that reinforcement value is calculated (and perhaps) stored in the aforementioned areas of the PFC \cite{Daw:2011p7995,Bornstein:2011p7996,Frank:2011p8152,OReilly:2010p7612,OReilly:2006p1161,OReilly:2006p2615}.  The exact underlying functional connectivity of striatal, VTA/SNc, and cortical regions remains uncertain \cite{Daw:2011p7995,Bornstein:2011p7996,Frank:2011p8152}.  It is well established however that PFC directly influences striatal activity and these regions are coupled or mediated by VTA/SNc.   As such several simple (3-node) mediation models, using both constant and similarity adjusted RPEs, will be compared.
% subsection goals (end)

% section introduction (end)

\section{Methods and Analyses} % (fold)
\label{sec:methods}
\subsection{Task} % (fold)
\label{sub:task}
As discussed at the end of the introduction, the experimental procedure consists of two parts or tasks.  Depicted in Fig 1. (top), the first is a passive classical conditioning task where participants will learn reward categories by viewing randomly selected (without replacement) black and white sinusoidal gratings followed by ``Gain \$1'' or ``Lose \$1'' in, respectively, green or red letters.  Reward categories will be derived from a information integration parameter distribution (Fig. 2, borrowed from \cite{Spiering:2008p5008}).  The grating is onscreen for 0.5 seconds, followed by 0.5 seconds gap, with the outcome displayed for another 0.5 seconds with a 1 second fixation cross between each trial. Task 1, which will be completed outside the fMRI scanner, lasts just over 6 minutes, and includes 175 stimuli approximately evenly divided among the two categories.  Each participant will be instructed to ``Attend to the screen in order to learn which types of gratings lead to winning money and which types lead to losses''.  The category distribution to value (gain or loss) mapping will be randomized for each participant.

The second task (Fig 1, bottom) is an abstract deterministic category learning task that replaces direct verbal feedback or reward with an appropriate grating from task 1; gratings associated with monetary wins will be used for positive reinforcement, and gratings associated with losses for negative reinforcement.   Each trial begins with an abstract black and white ``tree'' stimuli, which belongs to one of two arbitrarily named categories (``q'' or ``w'').  Subjects will indicate their categorization response by button press using either the right index or middle finger (respectively) on a magnet compatible response box placed on the participant's thigh.  The response window lasts up to 2.5 seconds.  During the response period the ``tree'' remains onscreen.  Once either time has elapsed or the participant responds, the ``tree'' disappears and 1-8 seconds later (as defined by the jitter optimization routine, below) is replaced by a sinusoidal grating.  If the response was correct a new, that is never before experienced, exemplar grating from the ``gain'' distribution is used; if it was incorrect, a new ``loss'' grating appears.  The feedback grating stays onscreen for 1 second and is then immediately replaced by a fixation cross, whose duration is again governed by the jitter optimization routine.  Participants will learn to classify 6 ``trees'' (randomly selected at the start of the experiment out of a pool of 16). Each of the 6 are experienced a total of 40 times for total of 240 trials/scanning session. In this task participants are in part instructed to, ``Use what you learned about the rewarding properties of the gratings to try and earn as much money as possible in this portion of the experiment''.  Participants will be instructed about both tasks orally by the experimenter using Fig. 1 as a visual aid.

\begin{figure}[tp]
	\label{fig:task}
	\fitfigure{f1task}
	\caption{Depiction of the behavioral task. The top is the (passive) classical conditioning participants learn the reward categories.  The bottom is the active abstract two-choice category learning.}
\end{figure}

\begin{figure}[tp]
	\label{fig:II}
	\fitfigure{f3II}
	\caption{A diagram of sinusoidal grating distributions for an information integration (II) category.  As II categories span the diagonal of the gratings parameter space (line width ($W$) and angle ($\theta$)) successful learning requires consideration of both dimensions preventing participants from solving the categorization problem with simple rule based strategies (e.g. if the lines are wide is category ``a'')}
\end{figure}

In fMRI (as in time-series signal analysis in general) there is an intrinsic tradeoff between simply detecting that a signal has occurred in the presence of noise and then estimating the timecourse (i.e. shape) of that signal \cite{Dale:1999p7901,Birn:2002p1777,Liu:2004p2141}.   The state of the art method for setting trial ordering in an attempt to maximize both signal detection and estimation is a genetic algorithm design by \citeNP{Kao:2009p7899}.  Without extensive modification unfortunately their methodology cannot account for epoch-style designs\footnote{This flaw is common to all methods of stimulus timing optimization, so far as I am aware.} that this experiment requires; each trial needs to contain stimulus-response, jitter and feedback delivery periods.  To account for this, Koa's methods will be applied twice, once using a 6.5 second ISI (the estimated average length of a trial) and once with a 1 second ISI (the length of the feedback display in task 2).  These two designs will then be manually interpolated to create a series of stimulus-response, jitter, feedback, and ITI events.  To create the final trial ordering each stimulus-response event will then be subsequently randomly recoded to match one of the 6 ``tree'' stimuli.  This randomization step will be performed independently for each subject so that stimulus assignment to events will be counterbalanced across subjects to control for specific item effects.  

Finally, because not all participants can successfully learn simple two-choice category tasks like task 2 (see \emph{Behavioral results} below) subjects will be prescreened with a variation of task 2, using colored fractals as stimuli, ``a'' and ``b'' as category labels, and written feedback (e.g. ``Correct'').  To be included in the study participants must reach 0.65 accuracy (measured with a rolling average) within 60 training examples during prescreening.  Prescreening will occur immediately following Task 1 training and immediately preceding the scanning session.
% subsection task (end)

\subsection{fMRI acquisition and analyses} % (fold)
\label{sub:fmri}

fMRI data for 12-15 participants will be acquired (approximately half female, age range 18-35, right handed).   All participants will be prescreened for the typical exclusion factors (e.g. metal implants, mental disorders, etc) and will be compensated at a base rate of \$15.00 earning up to \$30 more depending on behavioral performance. 30 trials will be randomly selected from the 240 possible and the participant will be paid an additional dollar for every correct response and will lose a dollar for every incorrect response on these 30 trials. Scanning data will be acquired at the Intermountain Neuroimaging Consortium (INC) facility located at the University of Colorado at Boulder The exact scanning parameters (TR, TE, in-plane resolution, etc) are to be determined as this is the Lab's first use of the INC machine but will be typical of a whole-brain rapid event-related fMRI acquisition.

Standard fMRI data preprocessing (motion correction, slice-time correction, drift correction, temporal, spatial smoothing and Tailarach normalization) will be carried out in BrainVoyager, version 2.1.  The main focus of this work will be to test how reward categories are represented and processed in specific regions of interest (ROIs).  \emph{A priori} regions of interest are the VTA/SNc, ventral and dorsal striatum, and ventromedial and dorsolateral PFC (the latter two have been show to correlate with reinforcement learning value computation \citeNP{Kahnt:2010p7677}).  All regions will be defined by anatomical tracings on an average image formed from of all participants' normalized anatomical MRI scans.  The regions of interest will be used to compare computational models of categorical reward representations as described in \emph{Computational analyses} below.

Since this is, so far as I am aware, the first time rewards have been treated as categories in an fMRI experiment, whole brain activation mapping is also of interest.  This will be done in BrainVoyager with the final maps reported as a set of two overlaid probability maps - heat maps where the value in each voxel is the fraction of participants who had significant activity (defined here as either $p < 0.05$ or $p < 001$; RFX correction) at that voxel.   This is an improvement over the typical thresholded t-value maps as probability maps include information not just about the strength of the effect but also its consistency.  \emph{A priori} whole brain contrasts of interest are all trials compared to fixation (the most powerful contrast which will provide an overall picture of activity patterns), all stimulus-response periods compared to fixation as well as all feedback periods compared to fixation, and feedback period contrasted to stimulus-response period (and the reverse). In addition the effect of outcome valance (gain or loss) will be compared for each of the prior contrasts.  Finally probability maps will be qualitatively compared to results from a similar experiment (that used verbal feedback - ``correct'' or ``incorrect'') carried out by Lopez-Paniagua, PhD, for his masters work \cite{LopezPaniagua:2011p8296}. 
% subsection fmri (end)

\subsection{Computational analyses} % (fold)
\label{sub:Computational}
Reinforcement learning measures will be derived from a set of Rescorla-Wagner models (Eq.~\ref{eq:V} and \ref{eq:rpe}.), with decision making approximated by the logistic function (i.e. softmax, Eq.~\ref{eq:softmax}) with the parameters ($\alpha$ and $\beta$) minimized by maximum log-likelihood.  Two separate Rescorla-Wagner models will be considered to examine two different reward representations (Eq.~\ref{eq:r1} and \ref{eq:r2}).  The first reward representation is all or none, whereas the other incorporates the categorical structure by incorporating a distance parameter, D.  D is the Euclidean distance between the average angle ($\bar{\theta}$) and width ($\bar{W}$) in one of the two reward categories and the angle and width of the individual example grating (i.e. $\theta$ and $w$).   I will test whether the BOLD signal in the ventral striatum and VTA/SNc is best described by the prediction error term (Eq~\ref{eq:rpe}) combined with with Eq.~\ref{eq:r1} or with Eq.~\ref{eq:r2}.    Similarly BOLD changes in the dorsal striatum and in the two prefrontal regions of interest (see \emph{fMRI}) will be modeled by $V(s,t)$ (Eq~\ref{eq:V}), again comparing the two methods for calculating $r(t)$.

To restate, Rescorla-Wagner value updates are defined by,
\begin{equation} \label{eq:V} V(s,t) \leftarrow V(s,t) + \alpha*\delta \end{equation} where 
\begin{equation} \label{eq:rpe} \delta = r(t) - V(s,t) \end{equation}
with $r(t)$ calculated as either
\begin{equation}
	\label{eq:r1}
	r_{c}(t) = \{1,0\}
\end{equation}
or
\begin{equation}
	\label{eq:r2}
    r_{d}(t) = \frac{r_{c}(t)}{D}
\end{equation}
where
\begin{equation}
	\label{eq:D}\\
	D = \sqrt{(\bar{\theta} - \theta)^2 + (\bar{W}-w)^2}
\end{equation}
and in all cases
\begin{equation} \label{eq:V0} V_{initial}(s,t) = 0. \end{equation}
\begin{equation}
	\label{eq:softmax}
	p(s_1) = {e^{\beta V(s_1,t)}\over{e^{\beta V(s_1,a)} + e^{\beta V(s_2,a)}}};\ s_1 = (s_{i},q), \ s_2 = (s_{i}, w).
\end{equation}

Prior to regression analysis, for each regions of interest outlined above, all reinforcement measures will be convolved with the canonical (``double-gamma'') hemodynamic response function and, to be consistent with the treatment of BOLD data, z-scored and 4Hz high-pass filtered.  Model fits will be compared by AIC and BIC\footnote{There is so far as I can tell some debate over the best criterion, in general, and it seems there is no harm in calculating both and hoping for agreement.  In case of disagreement I propose using their F1 score to decide the best model fit ($F1 := \frac{AIC*BIC}{AIC+BIC}$) }. Assuming AIC and BIC agree (as is likely) the best fit model will only be accepted as valid if it is also significant predictor in the regression.

Eight mediation models, representing the complete set of unique combinations of PFC (either ventral or dorsal) connected to striatum (ventral or dorsal) with the two RPE models (Eq.~\ref{eq:r1} and \ref{eq:r2}) as mediators will be compared.   It is of particular interest whether the best models for ventral and dorsal striatal activity will correspond well to regression fits outline above. While only half of these models are theoretically or empirically motivated, recent simulations evaluating the robustness and reliability of structural equation models (of which mediation models area subset) demonstrated that if BOLD models of functional architecture are to be meaningfully compared all possible models need be considered, not just those believed best \emph{a priori} \cite{Lohmann:2011p8418}.

Two quick notes: First, all models in this work assume reward related activity in both VTA/SNc and ventral striatum is bivalent, i.e. positive outcomes lead to an increase in firing and negative outcomes lead to a suppression \cite{DArdenne:2008p1505,Cooper:2008p5238,Menon:2007p6529}.  This is assumption has been made by nearly all RPE work to date so is not unjustified.  However recent recordings in monkeys suggest that there are multiple sub-populations of cells, some of which are bivalent in the predicted direction, but others of which fire positively to both both positive and negative outcomes, and yet others with an inverted bivalent pattern -- suppression to positive outcomes and increased firing to negative outcomes \cite{Matsumoto:2009p7219,Levita:2009p7280}.  Even among these subpopulations individual neurons showed trial-by-trial variance.  So without further information it would difficult, nigh impossible, to model these complex and opposing patterns as a single timecourse model as is necessary for regression analysis.  Second, data will be exported from BrainVoyager in the NIFTI format (http://nifti.nimh.nih.gov/) and all computational analyses will be carried out using custom Python (2.7.1) code developed for this proposal, as well as ongoing unrelated machine learning (MVPA) and fMRI simulation experiments.  Code is complete and ready for use, excepting that to be used for statistical mediation testing, which will be ported from the Wager Lab's Matlab toolbox (available at http://wagerlab.colorado.edu/tools).
% subsection Computational (end)
% section methods (end)

\section{Behavioral results} % (fold)
\label{sec:behavioral_results}

\begin{figure}[tp]
	\label{fig:acc}
	\fitfigure{f2acc}
	\caption{Mean accuracy (black), averaged for all 6 stimuli by trial, blue line and grey represent a binomial regression fit of the data and bootstrapped 95\% confidence intervals, respectively.  53\% were learners (left) defined by a mean accuracy in the last 10 trials (for all 6 stimuli) greater than 63\%. Note: the trial order randomization procedure in this pilot did not enforce equal number of trials in for each of the 6 stimuli (\emph{M}=38) leading to artifactual increases in variability above trial 35 or so.  This oversight will be corrected prior to fMRI data acquisition.}
\end{figure}

\begin{figure}[tp]
	\label{fig:rt}
	\fitfigure{f2rt}
	\caption{Mean reaction time (black), averaged for all 6 stimuli by trial, blue line and grey represent a linear regression fit of the data and bootstrapped 95\% confidence intervals, respectively. See Fig 3 for learning criterion and other relevant details.}
\end{figure}

The experiment protocol outlined above was completed by 33 participants (2/3 female; see Fig 3 and 4).  Using 63\% as the cutoff which is near the $p < 0.05$ threshold of the binomial test, learning significantly exceeded chance at trial 19.  This learning rate is consistent with past work in the lab using just verbal or monetary feedback.  Also consistent with prior work was the mean reaction time remaining steady (near 750 ms) as learning proceeded.  In summary, the categorical rewards appear behaviorally very similar to direct verbal or monetary rewards; the next step is fMRI data acquisition.
% section behavioral_results (end)

\newpage
\bibliography {bibmin}

%%%%%%%%%%%%%
\end{document}
%%%%%%%%%%%%%
