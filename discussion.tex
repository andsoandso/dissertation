\documentclass[doc,12pt]{apa}        % use: 'man' for submission type; 'jou' for
                                % journal type, and 'doc' for typical latex
                                % but with figures inline with text
\usepackage{geometry} 
%\geometry{a4paper} 
\usepackage[parfill]{parskip}   % paragraphs delimited by an empty line

\usepackage{graphicx} 
\usepackage{amssymb}            % no idea what this does...
\usepackage{epstopdf}           % no idea what this does...
%\usepackage{gensymb}            % no idea what this does...

\usepackage{setspace}

\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png} \setcounter{secnumdepth}{0}  % no idea what this does...

\usepackage{apacite}
\usepackage{longtable}
%%%%%%%%% END HEADER %%%%%%%%%

\title{Rewards are categories.} 
\author{Erik J. Peterson} \affiliation{Dept. of Psychology \\ Colorado State University \\ Fort Collins, CO} 

%%%%%%%%%%%%%%%%
\begin{document} 
%%%%%%%%%%%%%%%%
\maketitle
\doublespacing
\section{Discussion} % (fold)
\label{sec:dicussion}
To review, I wanted to know whether or not cognitive rewards are represented as categories in the human brain.  And whether such a representation might impact the reinforcement learning process.  To start to answer these two interrelated questions, I collected fMRI data while participants completed a stimulus-response task using with pre-trained perceptual categories as rewards, one category for gains and one for losses.  The behavioral and neural findings of this work, which I'll now discuss in detail, show that cognitive rewards can be categories, categories which do substantively impact reinforcement learning signals.  I'll further argue that category representations would be a reasonable mechanistic explanation for the generalization of (classical) secondary reinforcers.  Synthesizing all of the above, I'll ultimately conclude that rewards are categories.  To build this case, I'll now step through both confirmatory and inconsistent results, beginning with the behavioral and ending with select regions of interest.

\subsection{Taking us to can}
\label{sub:tocan}
In the behavioral task reward were categories, information integration (II) categories to be specific.  II is classic category structure, much studied in humans and other animals \cite{Smith:2011p9101,Ashby:2011p9148,Smith:2010p9713}.  II categories are distinct from their contemporaries by requiring integration of multi-dimensional stimulus information, and so are difficult to verbally describe. II learning recruits procedural memory, which relies heavily on the dorsal striatum \cite{Ashby:1998p9716}.   This lack of verbalizeability, and multi-dimensional structure, make the reward categories irreconcilable with the classical rewards almost universally used human studies of reward (e.g ''Win \$1'', ``Correct!'', ``Yes!'').  Despite this large difference, participants easily and rapidly learned using the II categories.  Performance, as measured by both accuracy and reaction times, were nearly identical to similar tasks using verbal rewards (p\pageref{subsub:wellbehaved}).  

Further arguing for homology between the reward kinds, the overall pattern of BOLD activity, i.e. all trials compared to the rest trials (p\pageref{sub:blob}), was also markedly similar to that observed in nearly identical tasks using classical rewards (for several examples see, \citeA{LopezPaniagua:2011p8296,Seger:2010p7188,Cincotta:2007p6672,Seger:2006p5447,Seger:2005pd}).

The behavioral and neurological consistency observed in stimulus-response learning using classical and II reward categories means that perceptual categories can act as rewards and so, reversing that logic, rewards \emph{can be} categories.   Which leads naturally to the next analysis, whether the same neural algorithm(s) that mediate classical reward learning facilitate reward category learning as well.

\subsection{Are, Reflected in Error(s)}
\label{sub:inerror}
\subsubsection{A Known Pair's Logic}
\label{subsub:onestep}
However before drawing any conclusions from the modeling data, I need to get some logical preliminaries out of the way.  Many of the models of interest are both covariate and dependent.  Under generic statistical circumstance it would be difficult, or even impossible, to compare such models.  However in limited cases strong, even causal, conclusions are possible.   Inside the same family and coding scheme, there is a single change between many of the models.  For example, ``rpe\_acc'' and ``rpe\_acc\_guass'' differ only by the similarity adjustment of the reward (i.e. Eq~\ref{eq:rg} and \ref{eq:Sgauss}).  Because both models are fit to the same data\footnote{Using the same deterministic loss function} and so have identical signal-to-noise ratios, the 1.5\footnote{Bilateral average} fold increase in information that comes from using ``rpe\_acc\_guass'' in the dorsal caudate \emph{must} be caused by that single change \cite{Pearl:2010p9726}.  So while 1.5 would be small increase when comparing two noisy random variables \cite{Anderson:2000p9475,Forster:2000p9623}, I argue that, (1) because uncertainty is constant between the fits, and (2) because we also know the exact relation between two models, and (3) as the model's predictions only sometimes diverge (compare columns in Figure~\ref{fig:rpeacc}), 1.5 should instead be considered strong evidence.

\subsubsection{Categories, in All the Right Spots}
\label{subsub:rightspots}
In most of the regions of interest, the reward prediction family (``rpe'') was the most informative, ranging from 2.3-5.1 times more likely than the non-parametric ``boxcar'' model, p\pageref{subsub:belowctx}.  This alone strongly suggests that like classical rewards, the learning driven by reward categories are also mediated a dopaminergic reward prediction signal.  Even more important is the fact that many of the most reward sensitive areas are best described by the Gaussian-similarity adjusted reward (``rpe\_acc\_gauss'' in Figures~\ref{fig:caudate},~\ref{fig:ant},~\ref{fig:post},~\ref{fig:insula}, and~\ref{fig:ofc}), demonstrating that category parameters (i.e. the similarity metrics) directly effect reward valuation.  This is a direct confirmation of my hypothesis that cognitive rewards have an underlying category representation.

Outside the of VTA/SNc, striatal BOLD activity has been, time after time, shown to reflect the dopaminergic reward prediction error signal making it a, if not the, key test of novel reward prediction hypotheses (see the \emph{Introduction} for much supporting evidence on this point).  The fact then that in the dorsal caudate the Gaussian-adjusted reward prediction error term offered a substantively more informative account than the unadjusted models is a is crucially important result (compare ``rpe\_acc\_gauss'' to ``rpe\_acc'' in Figure~\ref{fig:caudate}), combined that is with the fact that the dorsal caudate was strongly active (Figure~\ref{fig:fvalcaudate}) and best described by the ``rpe'' family (Figure~\ref{fig:caudate}).  
 
The ventral striatum was also expected to play a strong role in this task, as it is both the ventral and dorsal striatum that have been most often correlated with reward prediction activity \cite{ODoherty:2003p6329,Knutson:2007p1687,Schonberg:2007p518,Seger:2010p7188}.  This is not to say dorsal and ventral areas are functionally homogeneous \cite{Schonberg:2009p6669,ODoherty:2004p1269,Atallah:2007p1746}.  The dorsal caudate has been repeatedly linked to more abstract kinds of rewarding activity (e.g. task outcomes, fictive rewards, money compared to juice (\citeA{Tricomi:2008p6663,Lohrenz:2007p7240,Valentin:2009p7202}, for a review see \citeA{Grahn:2008p4654}).  While ventral activity as been associated with hedonic valuations \cite{ODoherty:2004p1269}.  Given this functional divide, and the dorsal caudate's established role in II category learning \cite{Ashby:1998p9716}, in hindsight perhaps then it is no surprise that only dorsal striatum was found to be active.

The dorsal striatum and ACC have several telling similarities.  Both, in part due to dopaminergic projections from the VTA/SNc that modulate LTP via D1 receptors \cite{Schweimer:2006p9780}, are strongly involved in cognitive reward learning \cite{Atlas:2010p7566,Hayden:2009p6545,Rudebeck:2008p4712,Rolls:2008p7577,Quilodran:2008p2645,Hampton:2007p2983,Ernst:2004p3998}, with the BOLD signal often reflecting prediction errors in higher-order conditioning experiments \cite{seymour:2004aa} and fictive rewards \cite{Hayden:2009p6545}.  The ACC though appears to specialize in mediating between competing \emph{future} alternatives, especially in the context of effort required to achieve each option \cite{Quilodran:2008p2645}.  The fact then the ACC also is most informatively described by the Guassian-adjusted reward prediction error is another strong piece of evidence supporting reward category representations.  

While generally consistent with the reward category interpretation, the insula was the one region that was equally well described by both reward codes (i.e. ``acc'': ${1,0}$ or ``gl'': ${-1,1}$).  All others strongly preferred ``acc'' only.  While it is not clear the functional role of both codes, which are quite different in their predictions (compare Figure~\ref{fig:rpeacc} to~\ref{fig:rpegl}, see also Figure~\ref{denrpe}), the ``gl'' coding is consistent insula's established role in the processing and prediction of aversive outcomes (CITE).  Additionally, the finding of dual codes in the insula is the first confirmation of my secondary hypothesis (p\pageref{subsub:codesandfits}), that the reported complex reward codes shown single cell recordings of VTA/SNc \cite{Kim:2006p1063,Matsumoto:2009p7219,Smith:2011p8133} will be reflected in the BOLD signal.

While, as reviewed in the \emph{Introduction}, the middle frontal (i.e. dorsallateral) cortex plays role a in estimating future reward probabilities, the singular relation between activity in this region and the ``rpe\_acc'' model (p\pageref{subsub:onsheet}, see also Figure~\ref{fig:dlpfc}) is best explained another of this regions well established roles, encoding of abstract rules \cite{Wallis:2001p8129}.  While prefrontal regions have been previously shown to reflect prediction errors \cite{Ramnani:2004p5390}, I speculate that the reward categories are transformed in dorsallateral PFC into reward rules, something akin to ``this category of gratings is worth \$1.''.  And that these rule encoded rewards have their own reward prediction errors calculations.

\subsubsection{A fit inconsistency}
\label{subsub:inconsistency}
Both ``rpe\_acc'' and ``rpe\_gl'' fit the behavioral data better either of the corresponding similarity-adjusted models (Figure~\ref{fig:logL}).  If rewards are in fact categories the opposite pattern would be expected.  This inconsistency though has an strong alternative explanation.  Even with perfect performance, the largest possible value estimate is smaller for the adjusted models (as suggested by Figure~\ref{fig:fig:denvalue}, compare the maximum value peaks for ``rpe\_acc'' compared to ``rpe\_acc\_exp'' and ``rpe\_acc\_exp'').  These smaller value estimates result in lower probability estimates (via the softmax transform, Eq~\ref{eq:softmax}) and so result in lower log-likelihood scores (i.e. worse fits).  Despite this inherent limitation the adjusted models could be modified to give equivalent performance.  As the task is deterministic, once the optimal choices were learned the models could switch strategies and rely on a ``working memory'' strategy: just do what you did last time.  This kind of working memory has recently been shown to be quite entangled with human reinforcement learning \cite{Collins:2012p9779}.  Alternatively the reward prediction errors could be renormalized based on the cumulative variance, following observations of just such behavior \cite{Tobler:2005p6373}.


\subsubsection{Back to the Secondary}
\label{sub:generalsense}
When conditioned as secondary reinforcers simple stimuli generalize well in humans and animals (for a review see p\pageref{subsub:birds}).  This generalization is by inference, i.e no direct reinforcement is needed \cite{Guttman:1956p8355,Nakamura:2006p9093,Smith:2011p9101}.  Mechanistically how such generalization occurs has not been studied.  Based on the success of the similarity-adjusted reward prediction errors above, I speculate that the even simple stimuli have fundamentally categorical representations.  And that these representations, via similarity-adjusted prediction errors, facilitate stimulus generalization.  In addition to perfectly matching Shepard's (1987) theoretical predictions for exponential of Gaussian decays in the degree of generalization (p\pageref{subsub:curves}), categorical representations of the kind studied here (p\pageref{subsub:catquant}) for secondary rewards would implicitly allow animals to generalize on the first new example, matching the observed behavior.  A categorical basis for even simple stimuli is advantageous in non-generalization trials as well. The intrinsic noise in neuronal encoding means that second viewing a stimulus must have a (slightly) different representation \cite{Ashby:1986p9783}.  A categorical representation would easily overcome such noisy encodings. 

\subsection{The big conclusion}
Based on the consistency between classical and reward categories, both behaviorally and in overall BOLD activity patterns, I first concluded that rewards \emph{can} be categories.  This, combined with the fact that reward categories generate reward predictions errors, and these errors strongly reflect category structure, and that categories offer a powerful and parsimonious explanation for the generalization of secondary reinforcers, I finally conclude that rewards \emph{are} categories.


\subsection{Future Work}
\label{sub:future}

\emph{TODO -- flesh these out, in some cases lots}

If rewards are categories, the next question is whether rewards are \emph{only} categories.  While generalization (and so categories) may be universal \cite{Shepard:1987p9102}, specifics matter too.  In fact memory for specifics is at odds with generalizable (i.e. abstract) memories \cite{Atallah:2004p5466}. Given item and categories must always diverge, both item and category reward representations would be useful.  However these is a marked degree of overlap between the reward processing and category learning systems \cite{Seger:2010p7189,Ashby:2011p9148}.  While this overlap might be due to the fact that most categories (in the lab) are learned using rewards, there is an alternative if rewards are just a kind of category.  The overlapped activity could reflect one category building another (dissimilar) category, though see the discussion of dorsallateral PFC above.

Behavioral predictions (however see scaling, WM).

Reward categories would be useful for robots.

\clearpage
\newpage
\bibliography{bibmin}
%%%%%%%%%%%%%
\end{document}
%%%%%%%%%%%%%
