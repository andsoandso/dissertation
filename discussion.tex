\section{Discussion}
\label{sec:dicussion}
The question is, are cognitive rewards represented as categories in the human brain?  And does such a representation impact the reinforcement learning process?  To start to answer these two interrelated questions, fMRI data was collected while participants completed a stimulus-response task using with pre-trained perceptual categories as rewards, one category for gains and one for losses.  Each trial's reward, then, was a never before or again experienced examplar from one of the two reward categories.  The use of only new exemplars distinguishes this task from higher-order conditioning paradigms were the same stimulus is repeatedly paired or presented.  The behavioral and neural findings of this work, which are now discussed in detail, show that cognitive rewards can be categories, categories which do substantively impact reinforcement learning signals in the brain.  It will be further argued that category representations would be a reasonable mechanistic explanation for the generalization of (classical) secondary reinforcers, leading to ultimate conclusion: rewards are categories.  To build this case, this discussion will now step through both confirmatory and inconsistent results, beginning with the behavioral and ending with select regions of interest.

\subsection{Taking Us to Can}
\label{sub:tocan}
In the behavioral task rewards were drawn from perceptual categories, information integration (II) categories to be specific (p\pageref{subsub:whatwhen}).  II is classic category structure, much studied in humans and other animals \cite{Smith:2011p9101,Ashby:2011p9148,Smith:2010p9713}.  II categories are distinct from their contemporaries by requiring integration of multi-dimensional stimulus information, and so are difficult to verbally describe. II learning also recruits procedural memory, which relies heavily on the dorsal striatum \cite{Ashby:1998p9716}.   The lack of verbalizeability and the multi-dimensional structure make the reward categories irreconcilable with the classical rewards almost universally used human studies of reward (e.g ``Win \$1'', ``Correct!'', or ``Yes!'').  Despite the marked difference between classical and the reward categories employed here, participants easily and rapidly learned using the II categories.  Performance measures, both accuracy and reaction times, were quite similar to tasks using classical rewards (p\pageref{subsub:wellbehaved}; for comparison see, \citeA{ODoherty:2003p6329,Ramnani:2000p6515,Aron:2004p1375,Seger:2010p7188,Seger:2005pd}).

Further arguing for homology between the reward kinds, the overall pattern of BOLD activity, i.e., all trials compared to the rest trials (p\pageref{sub:blob}), was also markedly similar to that observed in nearly identical tasks using classical rewards (for several examples see, \citeA{LopezPaniagua:2011p8296,Seger:2010p7188,Cincotta:2007p6672,Seger:2006p5447,Seger:2005pd}).

Taking in account both the behavioral and neurological consistency observed between classical rewards and the reward categories provides strong initial evidence that previously unexperienced exemplars, from studied categories, \emph{can} act as rewards.  Reversing that logic, rewards therefore \emph{can} be categories.  Having established that the next logical question is, do the same neural algorithm(s) that mediate classical reward learning facilitate reward category learning as well?.  That is, as reviewed above (starting on p\pageref{subsub:expectations}), the established mechanistic role for classically rewarding stimuli to drive stimulus-response learning is the reward prediction error hypothesis of phasic dopaminergic firing.  So do reward categories effect reward prediction error signals?  To ask this question, three kinds of rewards prediction error models were compared (p\pageref{sub:threemodels}).  One treated reward exemplars as classical rewards, the other two diminished the reward's worth, and thus the reward prediction error, based on how far that exemplar was from its category mean (i.e., similarity, see p\pageref{subsub:incantations} for mathematical details).

\subsubsection{A fit inconsistency, made consistent}
\label{subsub:inconsistency}
Both ``rpe\_acc'' and ``rpe\_gl'' fit the behavioral data better than either of the corresponding similarity-adjusted models (Figure~\ref{fig:logL}).  If rewards are in fact categories the opposite pattern would be expected.  This inconsistency though has a strong alternative explanation.  Even with perfect performance, the largest possible value estimate is smaller for the adjusted models compared to the unadjusted (as suggested by Figure~\ref{fig:denvalue}, compare the maximum value peaks for ``rpe\_acc'' compared to ``rpe\_acc\_exp'' and ``rpe\_acc\_exp'').  These smaller value estimates result in lower probability estimates (via the softmax transform, Eq~\ref{eq:softmax}) and thus in lower log-likelihood scores (i.e., worse fits).  Despite this inherent limitation the adjusted models could be modified to give equivalent performance.  As the task is deterministic, once the optimal choices were learned the models could switch strategies and rely on a ``working memory'' strategy: just do what you did last time.  This kind of working memory has recently been shown to be quite entangled with human reinforcement learning \cite{Collins:2012p9779}.  Alternatively the reward prediction errors could be renormalized based on the cumulative variance, following observations of just such behavior \cite{Tobler:2005p6373}.

\subsection{Are, Reflected in Error(s)}
\label{sub:inerror}
\subsubsection{A known pair's logic}
\label{subsub:onestep}
However before drawing any conclusions from the modeling data, there are some logical preliminaries to get out of the way.  Many of the models of interest are both covariate and dependent.  Under generic statistical circumstances it would be difficult, or even impossible, to compare such models.  However in limited cases strong, even causal, conclusions are possible.   Inside the same family and coding scheme, there is a single change between many of the models.  For example, ``rpe\_acc'' and ``rpe\_acc\_gauss'' differ only by the similarity adjustment of the reward (i.e., Eq~\ref{eq:rg} and \ref{eq:Sgauss}).  Because both models are fit to the same data\footnote{Using the same deterministic loss function} and so have identical signal-to-noise ratios, the 1.5\footnote{Bilateral average} fold increase in information that comes from using ``rpe\_acc\_gauss'' in the dorsal caudate \emph{must} be caused by that single change \cite{Pearl:2010p9726}.  So while 1.5 would be small increase when comparing two noisy random variables \cite{Anderson:2000p9475,Forster:2000p9623}, it is argued that, (1) because uncertainty is constant between the fits, and (2) because we also know the exact relation between two models, and (3) as the model's predictions only sometimes diverge (compare columns in Figure~\ref{fig:rpeacc}), 1.5 should instead be considered strong evidence.

\subsubsection{Categories, in all the right spots}
\label{subsub:rightspots}
Overall, in most of the regions of interest, the reward prediction error family (``rpe'') was the most informative, ranging from 2.3-5.1 times more likely than the non-parametric ``boxcar'' model (p\pageref{subsub:belowctx}).  This alone strongly suggests that like classical rewards, the learning driven by reward categories is mediated by the dopaminergic reward prediction error.  Even more important is the fact that many of the most reward sensitive areas are best described by the Gaussian-similarity adjusted reward (``rpe\_acc\_gauss'' in Figures~\ref{fig:caudate},~\ref{fig:ant},~\ref{fig:post},~\ref{fig:insula}, and~\ref{fig:ofc}), demonstrating that category parameters (i.e., the similarity metrics) directly affect a reward exemplar's worth.

Outside the of VTA/SNc, striatal BOLD activity has been, time after time, shown to reflect the dopaminergic reward prediction error signal making it a, if not the, key test of novel reward prediction hypotheses (see the \emph{Introduction} for supporting evidence).  The fact that in the dorsal caudate the Gaussian-adjusted reward prediction error term offered a substantively more informative account than the unadjusted models is a crucially important result (compare ``rpe\_acc\_gauss'' to ``rpe\_acc'' in Figure~\ref{fig:caudate}), combined with the fact that the dorsal caudate was strongly active (Figure~\ref{fig:fvalcaudate}) and best described by the ``rpe'' family (Figure~\ref{fig:caudate}).  

The dorsal striatum and ACC have several telling similarities.  Both, in part due to dopaminergic projections from the VTA/SNc that modulate LTP via D1 receptors \cite{Schweimer:2006p9780}, are strongly involved in cognitive reward learning \cite{Atlas:2010p7566,Hayden:2009p6545,Rudebeck:2008p4712,Rolls:2008p7577,Quilodran:2008p2645,Hampton:2007p2983,Ernst:2004p3998}, with the BOLD signal often reflecting prediction errors in higher-order conditioning experiments \cite{seymour:2004aa} and fictive rewards \cite{Hayden:2009p6545}.  The ACC though appears to specialize in mediating between competing \emph{future} alternatives, especially in the context of effort required to achieve each option \cite{Quilodran:2008p2645}.  The fact that ACC is most informatively described by the Gaussian-adjusted reward prediction error (i.e., ``rpe\_acc\_gauss'') is another strong piece of evidence supporting reward category representations.  

 The BOLD signals in the two regions most strongly implicated in cognitive reward processing and learning, the dorsal caudate and the ACC, were best described by the ``rpe\_acc\_gauss'' model.  This, combined with the behavioral and whole-brain data, is a definitive result.  Not only can participants employ new reward category exemplars as classical rewards, the reward prediction error model that posits a underlying category representations most informatively describes their respective BOLD time-courses, i.e., rewards are inferred and the strength of that inference (i.e. similarity) directly affects it value.

\subsubsection{Not always categories; not a disagreement}
\label{subsub:conana}
The ventral striatum was not well described by any of the models, nor was it significantly active bilaterally (Figure~\ref{fig:accumbens} and ~\ref{fig:fvalaccumbens}).  This is a concern as the ventral striatum was expected to play a strong role in this task, as it is both the ventral and dorsal striatum that have been most often correlated with reward prediction activity \cite{ODoherty:2003p6329,Knutson:2007p1687,Schonberg:2007p518}.  This is not to say that the dorsal and ventral areas are functionally homogeneous \cite{Schonberg:2009p6669,ODoherty:2004p1269,Atallah:2007p1746}.  The dorsal caudate has been repeatedly linked to more abstract kinds of rewarding activity (e.g., task outcomes, fictive rewards; \citeA{Tricomi:2008p6663,Lohrenz:2007p7240}; for a review see, \citeA{Grahn:2008p4654}).  Ventral activity is often associated with primary rewards, or other hedonic valuations \cite{ODoherty:2004p1269}.  Given this functional divide, and the dorsal caudate's established role in II category learning \cite{Ashby:1998p9716}, in hindsight perhaps then it is no surprise that only dorsal striatum was found to be active.

While generally consistent with the reward category interpretation, the insula was the one region that was equally well described by both reward codes (i.e., ``acc'': $\{1,0\}$ or ``gl'': $\{1,-1\}$).  All others strongly preferred ``acc''.  While the functional role of both codes, which are quite different in their predictions (compare Figure~\ref{fig:rpeacc} to~\ref{fig:rpegl}, see also Figure~\ref{fig:denrpe}), is obscure the ``gl'' coding is consistent with insula's established role in the processing and prediction of aversive outcomes \cite{Chua:1999p9833,Phillips:1998p9834,Buchel:1998p9836,Elliott:2000p1637}.  Additionally, the finding of dual codes in the insula is the first confirmation of the secondary hypothesis (p\pageref{subsub:codesandfits}), the reported complex reward codes found in single cell recordings of VTA/SNc will be present in the BOLD signal \cite{Kim:2006p1063,Matsumoto:2009p7219,Smith:2011p8133}.

As reviewed in the \emph{Introduction}, the middle frontal (i.e., dorsolateral) cortex plays a role in estimating future reward probabilities.  The singular relation between activity in this region and the ``rpe\_acc'' model (p\pageref{subsub:onsheet}, see also Figure~\ref{fig:dlpfc}) is best explained by another of this region's well established roles, the encoding of abstract rules \cite{Wallis:2001p8129}.  While prefrontal regions have been previously shown to reflect prediction errors \cite{Ramnani:2004p5390}, it is speculated that the reward categories are transformed in dorsolateral PFC into reward rules, something akin to ``this category of gratings is worth \$1''.  These rule-encoded rewards appear then to have their own (distinct?) reward prediction error calculations.

\subsubsection{Back to the secondary}
\label{sub:generalsense}
When conditioned as secondary reinforcers simple stimuli generalize well, both in humans and in other animals (for a review see p\pageref{subsub:birds}).  This generalization is by inference, i.e no direct reinforcement is needed \cite{Guttman:1956p8355,Nakamura:2006p9093,Smith:2011p9101}.  Mechanistically how such generalization occurs has not been studied.  Based on the success of the similarity-adjusted reward prediction errors above, it is speculated that the even simple stimuli have fundamentally categorical representations, and that these representations, via similarity-adjusted prediction errors, facilitate stimulus generalization.  In addition to perfectly matching Shepard's (1987) theoretical predictions of exponential or Gaussian decays in the degree of generalization (p\pageref{subsub:curves}), categorical representations, of the kind studied here (p\pageref{subsub:catquant}), for secondary rewards would implicitly allow animals to generalize on the first new example, matching the observed behavior.  A categorical basis for even simple stimuli is advantageous in non-generalization trials as well. The intrinsic noise in neuronal coding causes the second viewing a stimulus to have a (slightly) different representation than the first \cite{Ashby:1986p9783}.  A categorical representation would easily overcome such noisy encodings. 

\subsection{The Big Conclusions}
% Expand this, make it more poignant
Based on the consistency between classical and reward categories, in both behavioral and overall BOLD activity patterns, it was first concluded that rewards \emph{can} be categories.  This, combined with the fact that reward categories generate reward predictions errors, and these errors strongly reflect category structure (i.e. similarity), and that categories offer a powerful and parsimonious explanation for the generalization of secondary reinforcers, leading to the final conclusion: rewards \emph{are} categories.
That reward are categories is not just a semantic or definitional issue, it represents a substantial change in how rewards are conceived, and modeled: the primary, to secondary, to higher-order conditioning paradigm is incomplete, perhaps even incorrect.  That though may be just the beginning.  Two defining feature of categories are (1) they are arbitrary, containing, in principle, any given set of features and (2) the provide a mechanism for generalization.  While (2) was reviewed in the \emph{Introduction} (specifically p\pageref{sub:gen}), (1) has an interesting implications for rewards.  In the task studied here, the reward categories were built on the back of monetary rewards (p\pageref{subsub:whatwhen}), however if rewards are just a kind of category, so (1) holds true, it should be possible to find or train on reward categories that have no prior classically rewarding connotations, i.e. rewards can be simply made up, invented, \emph{de novo}.

The second major finding of these experiments, is the confirmation that the complex phasic activities reported at the neuronal level in the VTA/SNc \cite{Kim:2006p1063, Matsumoto:2009p7219,Smith:2011p9101} contribute meaningfully to the BOLD signal, which represents the aggregate activities of thousands and thousands of cells -- a notable advance that suggests analyzing detailed, multiple sub-population, neuronal models may be possible using fMRI.

\subsection{Future Work}
\label{sub:future}
If rewards are categories, the next question is whether rewards are \emph{only} categories.  While generalization (and so categories) may be universal \cite{Shepard:1987p9102}, specifics matter too.  In fact memory for specifics is at odds with generalizable (i.e., abstract) memories \cite{Atallah:2004p5466}. Given that item and categories must always diverge, rewards with both item and category representations would be useful.  However there is a marked degree of overlap between the reward processing and category learning systems \cite{Seger:2010p7189,Ashby:2011p9148}.  While this overlap might be due to the fact that most categories (in the lab) are learned using rewards, there is an alternative, if rewards are just a kind of category.  The overlapping activity could reflect only the process of one category building another (dissimilar) category, though see the discussion of dorsolateral PFC above for a first counter to this category-only hypothesis.

If it really is the case that rewards are categories, and category similarity metrics affect reward valuations, then the degree of similarity should be a useful parameter in shaping the learning rate.  For example, using the same II category structure (Figure~\ref{fig:II}) and in a between-groups design, one could select gratings from either closer (group 1) or farther (group 2) to the category means. If learning were slower for group 2 compared to 1, this would be a direct casual confirmation that reward categories drive learning. 

Just as there are too many outcomes (i.e., rewards) for a human agent to explore them all, making reward categories such a potentially useful tool for humans and other animals (reviewed on p\pageref{sub:gen}), reward categories could be just as useful for robots, or other computational agents, operating in complex environments.   However categories are not naively consistent with the theoretical necessities found in Markov state spaces, into which much of reinforcement learning theory is embedded \cite{Sutton:1998p9247}.  Fortunately the need and study of state generalization is quite an active area or research (?, ?), making it both possible and prudent (for both further theoretical and experimental development) to extend these methods to reward representations as well. 
\clearpage