\section{Introduction} % (fold)
\label{sec:introduction}
Birds will peck repeatedly, as mice will push levers, monkeys will hit buttons, and men will buy flowers, if each of these actions is followed by a primary reward -- food, drink or sex.  Buttons, levers and flowers have no value alone, so reinforcement theory goes, it is only through the \emph{statistically regular pairing} with primary rewards that value is transferred \cite{Rescorla:1988p8743}.  This is the classical view, and it has, in general, held up many years now \cite{iversen:2007aa}.  And indeed, the neural mechanisms of reinforcement learning are becoming increasingly clear following years of exciting and intense inquiry (for reviews see, \citeA{Dayan:2008p6342,Dayan:2008p6479,Montague:2006mz}).  However, due to their reliance on primary and secondary reward concepts, reinforcement learning theories can't account for two related facts.  One, rewarding effects are observed in the absence of primary and secondary rewards \cite{Hayden:2009p6545, Lohrenz:2007p7240, Tricomi:2008p6663, Jimura:2010p8305}. Two, value can be transferred by inference; no pairing is needed \cite{BrombergMartin:2010p7223, Hampton:2006p2577}.  These two facts, it will be argued, are irreconcilable with traditional secondary or higher-order conditioning, and so require a new nomenclature.  Primary, secondary and other higher-order rewards, will be denoted as ``classical rewards'', while other kinds of rewarding activities will be labeled as a ``cognitive rewards'' (dropping the quotes for convenience).  The difference though is more than semantic.  Cognitive rewards may represent a totally new conceptualization of rewards.

Using fMRI, behavioral, and computational data, the efficacy of one model system for cognitive rewards was examined, as was one possible computational mechanism for the creation and use of cognitive rewards: by treating and modeling cognitive rewards as a kind of category, arbitrary kinds of reward knowledge can be combined and transferred (by similarity-based inference) to new situations, thus accounting for two facts above.

This introduction has five parts.  First is a review classical rewards and the reward prediction error account of dopamine function, along with its anatomical basis.  Second, a critique of the rewards prediction error account is offered.  Third is the case for cognitive rewards.  Fourth is an argument for the logical and empirical necessity of generalizable reward representations, covering stimulus generalization and categorization along the way.  In the fifth and final section, the exact goals and methods of this work are laid out.  

Much time is spent in the first three parts reviewing reward-driven learning, particularly the reward prediction error hypothesis of phasic activity in the ventral tegmental area/substantia nigra pars compacta (VTA/SNc) and its neural substrates.  This time is warranted as all computational models considered herein are viable and interesting only if the reward prediction error hypothesis is true.  That is, the primary question of interest here, ``Are rewards categories?'', is asked through the lens of the reward prediction error hypothesis.  Additionally, many of the neural regions of interest are of interest as they play key roles in reward prediction and reward prediction error calculations.  For convenience, the phasic dopaminergic signal (i.e., the reward prediction error signal) from VTA/SNc to basal ganglia and cortical areas will often be referred to as just ``dopamine'' or ``dopaminergic activity''.  There are of course other kinds and roles of dopamine release (e.g., tonic activity driven by VTA/SNc \cite{schultz:2007aa} or dendritic back-propagation in cortex \cite{jay:2003aa}), however none of these are of immediate interest here.

\subsection{Classics, Expectations, and Tissues} % (fold)
\label{sub:cet}
\subsubsection{A pleasurable start} % (fold)
\label{subsub:start}
Classically rewards and reinforcers were linked to or operationally defined as food, water \cite{ODoherty:2006p2875,schultz:2007aa} and sex though for, err, logistical reasons this is less often used in the laboratory.  Classic rewards are certainly potent, having been used for over 50 successful years to study learning in animal models \cite{iversen:2007aa} and people \cite{Kim:2010p7248,Montague:2006mz}.  In the 1950's the first clue how rewards cause reinforcement arose in the electrical self-stimulation studies of Olds \& Milner (1956), and, \citeA{Crow:1972p8748}.  Olds and colleges observed that when electrodes were placed in the dopaminergic midbrain, animals would vigorously and repeatedly self-stimulate by pressing the available button.  By the 1970s Old's shocking work, combined with data from pharmacological studies of rats, electrochemical recordings, knowledge of the signaling mechanisms of dopamine receptors, as well as neuroleptic drug actions in Schizophrenic patients, lead to the first major theoretical proposal for dopamine's cognitive function - a signal for pleasure, sometimes called the anhedonia hypothesis \cite{Wise:1978p8771}.  Within 10 years however it became clear that dopamine's role extends beyond signaling primary rewards and pleasure. Activity was seen following secondary rewards, novelty, salience, and in other manipulations \cite{Spanagel:1999p8515, Salamone:2005p8774, BrombergMartin:2010p8834}.  More importantly, dopamine depleted animals continued to enjoy rewards, i.e., they still developed taste preferences, enhanced response vigor \cite{Cannon:2003p8513} and continued to respond to opiates \cite{Hnasko:2005p8832}.  In 1994 there was a surprise that would eventually account for many of anhedonia's deficits.  \citeA{Mirenowicz:1994p7185} reported that dopamingeric firing depended on how expected a reward was; this observation blossomed into the reward prediction error theory under review here.

\subsubsection{Expectations matter} % (fold)
\label{subsub:expectations}
Continuing to work in Macaque \citeA{Hollerman:1998qy} more fully explored Mirenowicz and Schultz's (1994) observation, showing that unexpected rewards lead to increases in dopamine neurons' firing rates, fully expected rewards elicit no response, and expected rewards that fail to arrive lead to a dip in the baseline firing rate.   \citeA{Roesch:2007p2519} found the same patterns in rats while \citeA{ODoherty:2003p6329} found them too in fMRI studies of the human striatum.  Striatal BOLD changes reflect phasic dopamine activity \cite{Schonberg:2009p6669,Surmeier:2007p4435}\footnote{
    Though this has recently come under question in rat models comparing single unit and field recordings to high resolution blood flow changes \cite{Mishra:2011p9095}}
. In ground breaking work, \citeA{Waelti:2001p6523} showed that stimulus-response learning and the dopaminergic signal are maximized not by reward reliability, but instead when rewards were intermittent (i.e., more rewards are not always better).  \citeA{Waelti:2001p6523}, along with, \citeA{Fiorillo:2003p6375} and \citeA{Bayer:2005ul}, successfully modeled changes in reward expectancy with a reward prediction error term derived from a reinforcement learning model\footnote{
    Specifically the Rescorla-Wagner model, more on that later.
} fit to each animal's behavior. The reward prediction error from the model strongly correlated with the dopamine response, both its increases and decreases. However expectancy related changes are not the only important prediction reinforcement learning models make for dopaminergic activity.  

Value must transfer.  If an initially neutral cue reliably predicts a reward the reinforcement learning equations require the prediction error (and thus the dopaminergic response) to transfer to the cue, thus mimicking Pavlovian conditioning.  This behavior was observed in the dopamine response as well \cite{Roesch:2007p2519, McClure:2003p3346}.  In sum, all characteristic predictions made by the reinforcement learning equations\footnote{
    Or to be precise, the Rescorla-Wagner and Temporal Difference family of reinforcement learning models} 
have been observed in the dopaminergic signal.  

Outside of correlational evidence, reinforcement learning models are also statistically predictive of non-human animal's choice behaviors \cite{Hampton:2007p2983}.  Single doses of dopamine antagonists and agonists have demonstrated a casual relationship between dopamine levels and learning rate \cite{Pizzagalli:2008p6521, Diaconescu:2010p7631}, which is broadly, though not exclusively, consistent with a reinforcement theory interpretation.  Reward prediction terms have also been shown to statistically mediate cortical-striatal coupling \cite{denOuden:2010p7203}.  Not limited to the above predictions and confirmatory findings, the reinforcement learning account has extended substantially, both theoretically and empirically.

Based on novel findings about novelty \cite{Bunzeck:2006p5319, Blatter:2006p6372, GuitartMasip:2010p7244} the reward prediction hypothesis has been extended to incorporate activity observed following presentation of novel stimuli \cite{Kakade:2002p6414} as well as to explain reward anticipatory firing via an average reward prediction error \cite{Knutson:2007p1687}. Another variation allowed for simultaneous neural implementations of model-free and model-based reinforcement learning \cite{Smith:2006p7627, Daw:2011p7995}. Alternative, but reconcilable, reinforcement learning equations have also been offered that allow for dissociation of first and second order conditioning, as well as explain Pavlovian to instrumental transfer \cite{OReilly:2007p827}. The reward prediction hypothesis has also been incorporated into theoretical accounts of addiction \cite{Redish:2004p2531} and been used to predict the salience of upcoming stimuli \cite{Behrens:2007p8839}.   

There are however several aspects of dopamine function which are, as yet, are unaccounted for theoretically.  \citeA{Matsumoto:2009p7219}, reported a very broad set of dopaminergic firing patterns. In the classical (bivalent) view dopamine neurons should fire more for unexpected rewards or omission of punishment, less for reward omission and in response to aversive events.   Instead \citeA{Kim:2006p1063, Matsumoto:2009p7219}, found that some neurons respond bivalently but many others showed an inverse coding scheme, responding positively to aversive stimuli increasing as the punishment grew larger than expected, and decreasing as it grew smaller than expected.  Yet other neurons responded positively to \emph{both} appetitive and aversive conditions.  In a separate experiment \citeA{Smith:2011p8133} demonstrated \emph{simultaneous yet separate} tunings to reward value, reward expectancy, salience as well as to novelty.   The dopamine response also appears to adaptively scale with past reward magnitudes.  These dynamic range adjustments appeared similar to the reward value divided by the cumulative variance \cite{Tobler:2005p6373}.  If the reports above hold up, i.e., the dopaminergic response is complex, the bivalent view needs substantial refinement, as do perhaps our analysis techniques; many different models or neural coding schemes may \emph{correctly} fit the same data, an issue which has received some prior attention outside of neuroimaging\footnote{
    For a (first) take on addressing this problem in fMRI, see p\pageref{subsub:tomany}).
} \cite{Chamberlin:1965p8873}.

\subsubsection{Networked plausibility}
\label{subsub:plausibility}
The dopaminergic firing patterns outlined above originate in the VTA/SNc, a small brainstem nucleus whose neurons project strongly to both the striatum, prefrontal cortex (PFC) and the hippocampus.  Electrophysiological recordings of VTA/SNc neurons show two firing modes -- tonic and phasic (for a review see \citeA{schultz:2007aa}).  The phasic mode is of interest here, as it is this that reflects the reward predictions error.  A reward prediction \emph{error} signal of course requires a prediction, in this case a prediction future reward value and probability.  How exactly such predictions are made is only partly understood.  Candidate regions for this calculation include the striatum, the limbic system routed through the habenula, as well at the orbital, ventral medial and the dorsal lateral frontal cortices.  Each is presented in turn, leaving the totality largely unintegrated, thus accurately reflecting the literature's state.

\subsubsection{Selecting striatum}
\label{subsub:selstr}
The striatum is the input area of the basal ganglia, a brain region involved in categorization, logical inference, habit formation, working memory and feedback mediated stimulus-response learning \cite{Frank:2001p1996,Jin:2010p7199,SchmitzerTorbert:2004p5410,Seger:2008p6401,Seger:2010p7189,Yin:2006p5080}.  In stimulus-response learning, two of the four striatal subregions (the head of the caudate and the ventral striatum) process reward information \cite{Yin:2005p5101,Yin:2008p6347,Schonberg:2009p6669}.  These two are highly innervated by projections from the VTA/SNc, but only the ventral striatum correlates with the reward prediction error signal \cite{Haruno:2006p3979,Seger:2010p7189}.  The remaining two regions (the body and tail of the caudate and the putamen) are involved with stimulus-response pair formation, specifically visual categorization and response selection, respectively \cite{Seger:2008p6401,Seger:2010p7189}.  Though these regions also receive VTA/SNc projections and are sometimes sensitive to reward level \cite{BischoffGrethe:2009p4570} the BOLD signal does not correlate with the reward prediction error \cite{Seger:2010p7189}.  Dopamine's exact role in these areas is less clear.  In addition to projecting to basal ganglia, VTA/SNc also receives input from the internal globus pallidus or GPi (a major output structure of the basal ganglia with widespread cortical connections), thalamus, and the central nucleus of the amygdala \cite{Botvinick:2008p6594}.   Thus the striatum may form a value evaluation \emph{and} action selecting loop, similar to the classic actor-critic reinforcement learning architecture \cite{Bornstein:2011p7996,Ito:2011p8146} though this view has recently received some strong criticisms \cite{Joel:2002p6593}.

Intact dopamine projections and striatal function is necessary for rapid stimulus-response learning.  Administering dopamine antagonists to human and non-human animals adversely affects stimulus-response learning \cite{Pizzagalli:2010p7205}, as does lesioning the VTA/SNc.  Complete lesions of the striatum also prevent stimulus-response learning \cite{Packard:2002p5074}.  Administering dopamine agonists or the readily converted precursor L-DOPA leads to increases in response vigor and the ability of a Pavlovian-conditioned stimulus to bias unrelated instrumental responses (i.e., Pavlovian instrumental transfer) \cite{Winterbauer:2007p6352}. Both Pavlovian instrumental transfer and response vigor are, in part, facilitated by phasic dopamine increasing activity in the ventral striatum.  Unmedicated Parkinson's patients, who have low striatal dopamine levels, show marked decreases in stimulus-response learning with rewarding outcomes when compared to patients on medication and healthy age and intellect matched controls \cite{Pizzagalli:2010p7205}.  These same patients show an enhanced capability to learn from negative feedback which suggests that decreases in dopamine convey negative outcome information \cite{Frank:2004p4709}.  Finally, there is a solid body of evidence suggesting that phasic dopamine alters the plasticity of neurons in the striatum which presumably facilitates stimulus-response learning \cite{Calabresi:2007p4284}.

\subsubsection{Linking with the limbic}
\label{subsub:limbic}
Recent work has highlighted the habenula (a small nucleus posterior to the thalamus) as being especially important in generating reward prediction error-like phasic activity in VTA/SNc.  Besides its limbic connections (to the amygdala, hippocampus and the serotonergic dorsal raphe nucleus \cite{Hikosaka:2008p4455}) the lateral habenula has reciprocal connections with the GPi and projects to the VTA/SNc.  Based on this anatomy, it has been suggested that the habenula could serve a point of intersection between the striatum and the limbic system.  Inline with this proposed role, the habenula can tonically inhibit or disinhibit dopamine release in VTA/SNc neurons, making VTA/SNc activity inversely correlated with the habenula.  As habenula activity decreases, burst firing in VTA/SNc increases.  Likewise as habenula firing increases, VTA/SNc activity temporarily pauses.  Reversible chemical inhibition of habenula also increases VTA/SNc phasic activity \cite{Hikosaka:2008p4455}.  Lesions to the habenula result in marked increases in dopamine levels in the dorsal and ventral striatum \cite{BrombergMartin:2010p7221}.  Detailed dual recording studies of both areas \emph{hint} that, combined, these areas calculate the value of stimulus-response pairs \cite{BrombergMartin:2010p7221}.  In summary, the GPi, with its access to cortical inputs via the striatum, and habenula, with its capability for altering VTA/SNc activity, may form the physiological loop necessary to calculate of the reward prediction error.

\subsubsection{Front and center, lateral too}
\label{subsub:fclt}
Orbital frontal cortex has been repeatedly shown in neuroimaging \cite{ODoherty:2001p2423} and lesion \cite{Hornak:2004p6234} studies to encode the absolute value of rewarding or punishing outcomes, thus playing a pivotal role in the new field of neuroeconomics \cite{Glimcher:2005p863}.  However orbital frontal areas are more than a simple value store.  They also play a role in response and outcome recall, responses selection \cite{Rudebeck:2008p4712, Furuyashiki:2008p1631}, motivation, pain and pleasure \cite{Atlas:2010p7566}, outcome anticipation and prediction \cite{Tanaka:2006fk, Roesch:2007p7182} and causal attribution \cite{Tanaka:2008p3265}.  Additionally, orbital function is highly dependent on the basolateral amygdala \cite{ODoherty:2003p2616} offering a path for orbital activity to inform reward prediction error calculation.  

Besides value, estimating the likelihood a reward will occur is the other key reward prediction calculation.  Correlations with the chance of receiving a reward \cite{Tobler:2009p8297}, the variance of expected value \cite{Kahnt:2010p7677} as well as with risk-seeking behaviors \cite{Tobler:2007p1562} have all been reported in dorsolateral prefrontal areas.  These same areas have also been implicated in inter-temporal choice, i.e., deciding between immediate and delayed rewards \cite{Kim:2009p8304,Kim:2008p2984}.

Unlike orbital and dorsolateral cortices, the nearby ventromedial prefrontal cortex encodes information on both reward magnitude and reward probability (Knutson et al, 2005), suggesting this region may synthesize orbital and dorsolateral activities.  Unlike orbital cortex, whose activity is linked only to stimuli, value in the ventromedial PFC is associated with actions (Glascher, Hampton \& O’Doherty 2009).   Furthering a possible integrative interpretation of dorsolateral activities, values in this area seem to be embedded in a ``higher order''  or abstract representation that reflects the statistical structure of the underlying task.  This abstract representation seems to support the inference of value \cite{Hampton:2006p2577}.  However ventromedial cortex's role in inference may be more general. It also plays a role in inferring the valued expectations of an opponent in two-player game of strategy (Hampton, Bossaerts \& O'Doherty, 2008) as well as inferring others intentions outside of economic games (Cooper, Kreps, Wiebe, Pirkl \& Knutson, 2010).

% --
\subsection{Bad Prediction, No Cookie}
\label{sub:bad}
The hypothesis that rewards are represented as categories in the brain assumes that dopamine, specifically that phasic activity in VTA/SNc leads to brief elevations in the concentration of dopamine in striatal and cortical areas, acts to stamp in stimulus-response relationships.  What follows is a critical look at that assumption.

\subsubsection{Not cortex, colliculus}
\label{subsub:colliculus}
Recent work by \citeA{Dommett:2005p7263} is a \emph{potentially} deadly issue for the reward prediction hypothesis.  The reward prediction theory requires very specific timing in order to map rewards to states and actions.  This requirement is satisfied by dopamine activity patterns, which are typically seen as 100 ms long burst about 100 ms after the initial stimulus.  However 100 ms is not much time for a saccade, and for detailed visual processing \cite{Dommett:2005p7263}, let alone prefrontal examination/valuation (a step likely required for reward prediction formulation, see p\pageref{subsub:fclt}).  Concerned about the plausibility of such rapid processing, Dommet et al (2005) disinhibited neurons in the brains of anesthetized rats in both the superior colliculus, a brainstem visual processing area, and early visual cortex with a GABA antagonist (which temporarily restores neural activity in the normally unresponsive neurons of an anesthetized animal) then exposed the animals opened eyes to a set of 2 Hz light pulses while recording dopamine cells in the SNc/VTA as well as neurons in both visual cortex and the superior colliculus.  Disinhibition of the visual cortex lead to no changes in dopamine firing.  Superior colliculus disinhibition caused about half the recorded  VTA/SNc neurons to fire, firing which was similar in character to that typically observed following an unexpected rewarding event.  Another third of the dopamine cells displayed a pause in activity, similar to that observed when an expected reward fails to arrive \cite{Mirenowicz:1994p7185}.  The remaining cells responded first positively then negatively.  From this the authors concluded that the superior colliculus and not the visual cortex is an effective activator of VTA/SNC neurons\footnote{
    Another reason for more cautious interpretation: Dommet et al's (2005) disinhibition experiments in visual cortex were very limited (N=4 cells compared to 30 for the superior colliculus experiments).  Nor did they assess the extent of disinhibition in visual cortex.
}. This is a problem as the superior colliculus responds only to very limited range of visual stimuli -- appearance, disappearance, or movement of objects as well as luminance changes.  It does not respond to contrast, velocity, wavelength or the geometric configuration of stimuli \cite{Dommett:2005p7263}. That is, the superior colliculus couldn't realistically extract sufficient information from the visual stimuli used in nearly all the studies of reward to date.  Instead, Dommett et al (2005) argue that ``[all of the many reward studies] can be solved based on luminance changes and/or of the position of specific reward-related visual stimuli''.  In other words, the expectancy-of-reward related changes in phasic dopamine are an artifact of the task design.  

 Dommet et al's (2005) interpretation is however too strong.  At best they have shown that there is not substantive direct connection between visual areas and the VTA/SNc.  If there was only a single downstream synapse between visual cortex and SNc/VTA (which would add around 2-10 ms in lag) their protocol would not have disinhibited it and so would have failed to elicit a dopamine response during visual stimulation.   Given the brain's high degree of inter-connectivity and probable small-world architecture \cite{bassett:2006aa}, a failure to find a direct anatomical relation is not in and of itself conclusive.  That said, how the dopamine signal can be so quick and consistent remains an open and very important question.

\subsubsection{They want, I want}
\label{subsub:salience}
Standing in opposition to both the anhedonia and reward prediction hypotheses is the incentive salience account, which is derived from the brute fact that addicts often greatly want drugs of abuse, but once drugs are received addicts, or their animal model counterparts, do not report an excess of pleasure or liking \cite{Robinson:1993p8987}.  Indeed pharmacological investigation of striatal areas support a distinction between wanting and liking \cite{Berridge:2003p8998}.  Recent experiments with dopamine deficient mice lead Berridge (2007) to argue that the putative dopaminergic reward signals instead signal degree of desire (i.e., wanting or, in their parlance, incentive salience).  Tyrosine hydroxylase knockout (DD) mice, mice who lack dopamine, still can learn reward contingent tasks, but they do not act on that (latent) learning until dopamine is restored \cite{Berridge:2007p7235}. In a similar vein, DD mice also display a reward preference when given the choice of a sweetened water versus untreated water, but unless dopamine is restored their overall desire for both is greatly decreased.  Based on these findings they argue that dopamine is not necessary nor sufficient for reward driven learning.  By combining studies of addicts (and their non-human animal equivalents) who display increased ``wanting'' of drugs but not ``liking'' (people rate the experience as no more pleasurable than controls, mice consume no more of the substance that controls) \citeA{Berridge:2007p7235} argue that phasic dopamine signals a stimulus' incentive salience.  Conclusions based on DD mice, though, should be cautious.  DD mice are extraordinarily lethargic.  To pep them up, caffeine is administered.  Caffeine, through a cascade driven by adenosine A2A and cannabinoid CB1 receptors in ventral striatum, can have biochemical effects similar to dopamine \cite{Lazarus:2011p8137, Rossi:2010p7252}.

Still, neglecting concerns about caffeine, the experiments in DD mice are quite damning to the theory that dopamine plays a casual role in stimulus-response learning.  There is, however, a substantial body of work supporting causation.  Administering human and non-human animals dopamine antagonists adversely affects stimulus-response learning \cite{Pizzagalli:2010p7205}, as does lesioning either the VTA/SNc or portions of the striatum.  Complete lesions of the striatum prevent stimulus-response learning \cite{Packard:2002p5074}.  This is relevant as it is the interaction between phasic dopamine and the striatum that is proposed to guide (drive) stimulus-response learning. Administering dopamine agonists, or the readily converted dopamine precursor L-DOPA, leads to increased Pavlovian instrumental transfer as well as response vigor \cite{Winterbauer:2007p6352}, both of which are thought to be facilitated by the interaction between phasic dopamine and activity in the ventral striatum.  Parkinson's patients when off medication, and so lacking striatal dopamine, show marked decreases in stimulus-response learning \cite{Pizzagalli:2010p7205}.  These same off-medication patients show an enhanced (compared to normal age and intellect matched controls) capability to learn from negative feedback, suggesting their ability and desire to act is intact \cite{Frank:2004p4709}.

While it not clear how to reconcile, theoretically or functionally, the evidence for incentive salience and reward prediction, it might not be necessary.  \citeA{Smith:2011p8133} showed distinct semi-overlapping tuning in VTA/SNc for both reward prediction, incentive salience as well as with measures of the animals enjoyment of the in-task reward (i.e., liking). There may in fact be no one correct theoretical accounting; the neurons of the VTA/SNc may signal instead a family of functions - for other supporting examples see, \citeA{Ito:2011p8146,Smith:2011p8133, Bornstein:2011p7996,BrombergMartin:2010p7218,Matsumoto:2009p7219}.


% --
\subsection{Thinking About Thinking Rewarding Thoughts}
\label{sub:cogrew}
As was stated at the outset, cognition alone can generate activity similar in appearance and effect to that seen following primary and secondary rewards.  For example, \citeA{Tricomi:2008p6663} showed ventral striatum BOLD signal changes in a declarative memory task in which subjects were initially trained with feedback to distinguish 60 correct from incorrect word pairs.  In the subsequent two rounds explicit feedback was withheld but activity in the caudate was observed when correct pairings were matched based on memory alone, i.e goal achievement led to strong activity.  In two economic decision making tasks strong ventral striatum signals were observed when participants were required merely to imagine or consider alternative outcomes  \cite{Hayden:2009p6545, Lohrenz:2007p7240}.  Information about the future is rewarding as well; \citeA{BrombergMartin:2009p7220} showed that complex visual clues about an upcoming outcome were sufficient to cause bursts of firing in the VTA/SNc.  Inversely, neutral stimuli can prevent decreases in responding that normally accompany repeated delays in reward presentation \cite{Reed:1992p9094}, a phenomenon that was reproduced using a robotic rat, wherein the neutral stimuli were treated as intrinsically rewarding \cite{Fiore:2008p7249}.

Informative, or to change terms to keep with other literatures, salient\footnote{
    Not to be confused with the ``incentive salience'', discussed above.
}, stimuli have been observed to have rewarding-like effects in people as well, though supporting data is limited to fMRI experiments.  Striatal BOLD increases have been observed in response to infrequently presented flashing images \cite{Zink:2003p5107}, and unexpected alarming tones (e.g., a siren replacing a constant 60Hz tone) \cite{Zink:2006p7210}.  Activity in the ventral striatum appeared in these tasks due to the stimuli alone, while dorsal activity was seen only when the stimuli had behavioral relevance.  A control task suggested this increase was not in response to the additional motor demands of the response but was, the authors argued, due to the increased salience of the active versus passive condition.  The context of behavioral responses made the reward more salient.  A similar dorsal/ventral division has been reported when comparing passive reward receipt to reward receipt requiring a response \cite{ODoherty:2006p2875} though these were attributed directly to the need for an instrumental response and not (necessarily) contextual salience.  Additionally, like reward expectations, salience-related activity scaled with intensity \cite{Zink:2006p7210}.

Novel (but not necessarily salient) stimuli also elicit reward prediction-like dopaminergic firing in monkeys \cite{Blatter:2006p6372} and in people \cite{Bunzeck:2006p5319}. Indeed, reward and novelty appear interchangeable.  \citeA{GuitartMasip:2010p7227} showed that when novel images precede rewarding outcomes ventral striatal activity is enhanced compared to reward alone.  Rewards preceding a visual stimulus or word-pair leads to enhanced memory for that pair \cite{Lisman:2005p5455}.  Building on that finding \citeA{Wittmann:2007p3328} showed enhanced recognition of natural scenes when images were preceded by novel images.  This effect was reproduced using high-resolution imaging of the VTA/SNc, with that area demonstrating a marked reward prediction signal during the task \cite{Krebs:2011p8134}.  This effect was extended further to the anticipation of novel images, similar to reward anticipation studies of striatal function \cite{Knutson:2001p5234}.  Novelty driven exploitation/exploration decision making relies on the striatum as well \cite{Wittmann:2008p541}.

In sum, task completion, imagined rewards, neutrally valued informative cues, behaviorally salient but non-rewarding events, as well as novel images, have all been shown to act as reinforcers, and to stimulate the dopaminergic midbrain into phasic firing.  None of these, individually or as a group, can be explained parsimoniously as secondary rewards\footnote{
    Nor as primary rewards, but this is a definitional problem.
}.  They were never \emph{statistically regularly} paired with a primary reward.

% --
\subsection{Generally Generalizable.}
\label{sub:gen}
In a recent article \citeA{Wimmer:2012p8836} made the argument that a stimulus' reinforcement derived value must be generalizable to other similar stimuli as there are far too many possible states in the world to explore each individually, \emph{and} therefore you rarely if ever encounter the same exact same stimulus twice.  In essence, they make an argument that categories are necessary for reinforcement learning to be a reliable guide in our complex changing world.  That reasoning is now extended.  There are at least as many outcomes as there are stimuli, i.e., any stimulus can be an outcome, in principle anyway. There are therefore too many possible outcomes to ever experience them all, \emph{and} you'll rarely if ever see the same outcome twice.  As a result, you often can't predict exactly what will happen. Therefore like stimuli, outcome expectations must generalize.  As desirable outcomes are rewarding (see p\pageref{sub:cogrew} in the \emph{Introduction}), reward representations generalize.  Skipping ahead a little, it is  hypothesized that the act of generalization will impact reward prediction error calculations.

\subsubsection{Needed similarities}
\label{subsub:needed}
And indeed many of the cognitive rewards outlined above may employ or even require generalization, often in the form of similarity assessment.  For example, to find that successful task completion is rewarding, \citeA{Tricomi:2008p6663} used a cued recall task, which in part relies on familiarity \cite{Jacoby:1991p9096} and thus similarity \cite{Nosofsky:1988p9098}.  Likewise studies of fictive rewards -- rewards created only in the participants imaginations -- require an act of inference; past desires must be extrapolated into a present experience \cite{Hayden:2009p6545,Lohrenz:2007p7240}. \citeA{Roesch:2007p2519} demonstrated rewarding inferences in rats as well.  When novel stimuli were presented in the context of a familiar task with the same action options previously available, the dopamine spikes that resulted were significantly correlated with the most valued past action.  Likewise assessing both salience and incentive salience requires participants to extrapolate past context and goals and into future experiences.  In fact \citeA{Zink:2004p5108}, showed that rewards sans salience elicited no striatal BOLD activity.  This finding suggests that rewarding activity must be driven by more than just the current hedonic stimulation.

\subsubsection{Birds do it}
\label{subsub:birds}
The argument for cognitive-type rewards and their hypothesized category representation is novel, however there are several non-human animal studies examining the generalizable properties of secondary reinforcers, though the neural mechanisms for such are unexamined.  As an example, \citeA{Guttman:1956p8355}, varied an instrumentally conditioned 570 nm light from 480-610, showing that while the bird's pecking rate (i.e., response vigor) decreased as one moved farther from 540, the birds still responded.  They generalized across the wavelengths.  When novel variations of conditioned stimuli from two sensory modalities were mixed similar graded changes in vigor were observed  \cite{Guttman:1956p8355}.  Pigeons' capability for generalization are not limited even to mixtures of simple cues: \cite{Nakamura:2006p9093} taught pigeons to discriminate categories of male and female birds and \citeA{Smith:2011p9101}, taught pigeons perceptually complex yet abstract categories.  Interestingly though, not all experiments were effective at producing generalization behavior.  Some combinations produced no responses at all \cite{Blough:2001p8408,Simmons:2008p8405,Urcuioli:2001p8359}.  Interestingly, the degree of generalization was not uniform across stimuli, e.g., generalization between shapes decreased more rapidly than colors \cite{Shepard:1987p9102}.  The source of this variability remained unknown until Shepard's seminal insight.

\subsubsection{Curves and categories}
\label{subsub:curves}
\citeA{Shepard:1987p9102} demonstrated that the variability among instrumental response generalization curves in pigeons and other animals could be accounted for if one no longer measured generalized responding by an objective metric (e.g., wavelength) but instead estimated the psychological space of the animal's perception.  In his own words: ``I propose to start with the generalization data and to ask: Is there a unique monotone function whose inverse will transform those data into numbers interpretable as distances in some appropriate metric space''. The metric space he proposed required ``[...] a very strong additivity condition: For each subset of three points, the distance between the two most widely separated points equals the sum of the distances between those two points and the third point that lies between them''.  After making some additional assumptions\footnote{
    $1.$ that a given stimulus is equiprobable over the psychological space and $2.$ that categories in the space are centrally symmetric and convex
} Shepard shows that the large variability among the many objective spaces he examined only minimally impacted the estimated (negative exponential or Gaussian) psychological space.  In other words, in many cases the objective space can be mapped to psychological distances by simply taking the negative exponential of their Euclidean distances\footnote{
    Though this geometrical relation is not necessary.  Shepard's insight can be re-derived using information theory \cite{Chatera:2003p9103}}.  Shepard's work suggests that whether the psychological space is exponential or Gaussian depends on whether the stimuli were distinct or perceptually confusable, which was partly supported in theoretical studies of perceptual noise on generalization \cite{Ennis:1988p9359}.  Outside of Shepard's theory, though, the Gaussian metric has substantial empirical and theoretical support in studies of human category learning, even when confusability of the stimuli is low \cite{Nosofsky:1985p9356,Medin:2012p9358}.  As such the divide between exponential (as seen in the Pigeon studies above) and Gaussian metrics (often seen in human studies) remains unbridged.

Based partly on Shepard's work the fields of perceptual and cognitive categorization have since made an ever evolving study of how animals create and use categories.  However there are still substantial controversies with no one consensus view.  The two extreme theoretical positions are exemplar models and prototype models \cite{Ashby:2005p4764}.  Exemplar models, in their simplest form, are feature counters, i.e., their categories are completely defined as the sum of previously observed information \cite{Nosofsky:1988p9104}.  Prototype models instead hold only information about one ideal template \cite{Rosch:1973p9108}, often implemented as probability density \cite{Ashby:1995p9109}, i.e., a set of sufficient statistics over the relevant information.  A common example is the mean and covariance, assuming prototype following matching a Gaussian distribution.  In between the two extremes are hybrid approaches, of which there are many.  These include using mixture models \cite{Rossee:2002p9112}, neural networks implementing self-organised maps \cite{Love:2004p9110}, and other clustering techniques \cite{Kruschke:2012p9111} as well as kernel methods \cite{Jakel:2008p9113}, among other increasingly esoteric approaches \cite{Martin:2012p9185}. There are many more examples, as categorization has been an active area of investigation for over 40 years.

This deep mathematical literature is further complicated by empirical investigations of human category learning, which suggest that regardless of the underlying representation, humans treat different category structures in different ways \cite{Ashby:2011p9148}.  In their hands, category learning has been subdivided into systems to handle rule-based (i.e., verbalizable), information-integration (implicit, forces consideration of $>1$ dimension of information), prototype-distortion (perceptual categorization), and unstructured category (categories without perceptual or semantic overlap, akin to sets in that their only necessary commonality is the fact they're grouped together).  Like finding the neural correlates of these systems \cite{Ashby:2005p9152,Ashby:2006p9153}, integrating them with the many formal approaches is in its early days \cite{Ashby:2011p9148}.

% --
\subsection{Why Are We Here Again?}
\label{sub:goals}
Are cognitive rewards represented as categories in the human brain?  And does such a representation impact the reinforcement learning process?  To start to answer these two questions, participants completed a stimulus-response task where classical rewards (e.g., ``Correct!'' or ``Win \$1.'') were replaced with pre-trained visually and cognitively complex categories, one category for gains and one for losses.  To learn the correct stimulus-response relations subjects then had to infer each outcome's value from these reward categories.  Behavioral and fMRI data collected under this task was then compared to a series of computational and functional connectivity models representing various possible algorithmic strategies for reward inference and assessing category membership.  These analyses are spread across three chapters.  The first covers the details of the task itself, the computational models and their parameter optimization.  Chapter two covers the fMRI methods and basic brain-mapping analyses, as well as the BOLD-signal-to-model comparisons.  The final chapter concludes that rewards are categories, as demonstrated by behavioral, fMRI, and modeling results.  Rewards as categories represents a substantial change in how rewards are conceived, and modeled: the primary, to secondary, to higher-order conditioning paradigm is incomplete, perhaps even incorrect.   Secondly, these experiments represent the first confirmation that the complex phasic activities reported at the neuronal level in the VTA/SNc \cite{Kim:2006p1063, Matsumoto:2009p7219,Smith:2011p9101} contribute meaningfully to the BOLD signal, which represents the aggregate activities of thousands and thousands of cells -- a notable advance that suggests analyzing detailed, multiple sub-population, neuronal models may be possible using fMRI.
\clearpage