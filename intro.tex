\documentclass[doc,12pt]{apa}        % use: 'man' for submission type; 'jou' for
                                % journal type, and 'doc' for typical latex
                                % but with figures inline with text
\usepackage{geometry} 
%\geometry{a4paper} 
\usepackage[parfill]{parskip}   % paragraphs delimited by an empty line

\usepackage{graphicx} 
\usepackage{amssymb}            % no idea what this does...
\usepackage{epstopdf}           % no idea what this does...
%\usepackage{gensymb}            % no idea what this does...

\usepackage{setspace}

\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png} \setcounter{secnumdepth}{0}  % no idea what this does...

\usepackage{apacite}
%%%%%%%%% END HEADER %%%%%%%%%

\title{Rewards are categories.} 
\author{Erik J. Peterson} \affiliation{Dept. of Psychology \\ Colorado State University \\ Fort Collins, CO} 

%%%%%%%%%%%%%%%%
\begin{document} 
%%%%%%%%%%%%%%%%
\maketitle
\doublespacing

\section{Introduction} % (fold)
\label{sec:introduction}
Birds will peck repeatedly, as mice will push levers, monkeys will hit buttons, and men will buy flowers, if each of these actions is followed by a primary reward -- food, drink or sex.  Buttons, levers and flowers have no value alone, so reinforcement theory goes, it is only through the \emph{statistically regular pairing} with primary rewards that value is transfered \cite{Rescorla:1988p8743}.  This is the classical view, and it has, in general, held up many years now \cite{iversen:2007aa}.  And indeed, the neural mechanisms of reinforcement learning are becoming increasingly clear following years of exciting and intense inquiry \cite{Glimcher:2011p8464, Montague:2006mz}.  However, due to their reliance on primary and secondary reward concepts, reinforcement learning theories can't account for two important facts.  One, rewarding effects are observed in the absence of primary and secondary reinforcers \cite{Hayden:2009p6545, Lohrenz:2007p7240, Tricomi:2008p6663, Jimura:2010p8305}. Two, value can be transfered by inference, no pairing is needed \cite{BrombergMartin:2010p7223, Hampton:2006p2577}.  Based on fMRI and behavioral data, I examine possible mechanisms of both of these aspects by modeling rewards as categories.  Categories are intrinsically cognitive and inferential.

This introduction has five parts.  First I review classical rewards and the reward prediction error account of dopamine function, along with its anatomical basis.  Second, I switch hats and critique these accounts. Third, I make a case for cognitive rewards -- defined as reward-like activity observed outside primary and secondary reinforcement. Fourth I argue for the necessity of generalizable reward representations, covering stimulus generalization and categorization along the way.  In the fifth and final section, the exact goals and methods of this work are laid out.

\subsection{Classics, Expectations and Tissues} % (fold)
\label{sub:cet}
\subsubsection{A pleasurable start} % (fold)
\label{subsub:start}
Classically rewards and reinforcers were linked to or operationally defined as food, water, pain (for modern examples see, \citeNP{Daw:2006p6592,ODoherty:2006p2875,Becerra:2011p7581,schultz:2007aa} and sex though for, err, logistical reasons this is less often used in the laboratory; Classic rewards are certainly potent, having been used for over 50 successful years to study learning in animal models \cite{iversen:2007aa} and people \cite{Kim:2010p7248,Montague:2006mz}.  In the 1950's the first clue how food and water neuronally cause reinforcement arose in the electrical self-stimulation studies of \citeNP{OLDS:1954p8747}, and, \citeNP{Crow:1972p8748}.  Olds and colleges observed that when electrodes were placed in the dopaminergic midbrain animals would vigorously and repeated self-stimulate by pressing the available button.  By the 1970s Old's shocking work, combined with data from pharmacological studies of rats, electrochemical recordings, knowledge of the signalling mechanisms of dopamine receptors, as well as neoleptic drug actions in Schizophrenic patients, lead to the first major theoretical proposal for dopamine's cognitive function - a signal for pleasure, sometimes called the anhedonia hypothesis, \cite{Wise:1978p8771}.  However within 10 years it became clear that dopamine's role extends beyond signaling primary rewards and pleasure. Activity was seen following secondary rewards, novelty, salience, and other manipulations \cite{Spanagel:1999p8515, Salamone:2005p8774, BrombergMartin:2010p8834}.  And, more importantly, dopamine depleted animals continued enjoy rewards, i.e. they still developed taste preferences, enhanced response vigor \cite{Cannon:2003p8513} and continued to respond to opiates \cite{Hnasko:2005p8832}.  However in 1994 there was a surprise that would eventually account for many of anhedonia's deficits.  \citeNP{Mirenowicz:1994p7185}, reported that dopamengeric firing depended on how expected a reward was, and this observation blossomed into the reward prediction error theory under review here.

\subsubsection{Expectations Matter} % (fold)
\label{subsub:expectations}
Continuing to work in Macaque \citeNP{Hollerman:1998qy}, more fully explored, \citeNP{Mirenowicz:1994p7185}, observation showing that unexpected rewards lead to increases in dopamine neurons' firing rates, fully expected rewards elicit no response, and expected rewards that fail to arrive lead to a dip in the baseline firing rate.   \citeNP{Roesch:2007p2519} found the same patterns in rats while \citeNP{ODoherty:2003p6329}, found them too in fMRI studies of the human striatum; Striatal BOLD changes reflect phasic dopamine activity \cite{Schonberg:2009p6669,Surmeier:2007p4435}
\footnote{
    Though this has recently come under question in rat models comparing single unit and field recordings to high resolution blood flow changes \cite{Mishra:2011p9095}}. In ground breaking work, \citeNP{Waelti:2001p6523} showed that stimulus-response learning and the dopaminergic signal are maximized not by reward reliability but instead when rewards were intermittent, i.e. more rewards are not always better.  \citeNP{Waelti:2001p6523}, along with, \citeNP{Fiorillo:2003p6375} and \citeNP{Bayer:2005ul}, successfully modelled changes in reward expectancy with a reward prediction error term derived from a reinforcement learning model
\footnote{
    Specifically the Rescorla-Wagner model
} fit to each animal's behavior. The reward prediction error from the model strongly correlated with the dopamine response, both its increases and decreases. However expectancy related changes are not the only important prediction reinforcement learning models make for dopaminergic activity.  Reward value must transfer.  If an initially neutral cue reliably predicts a reward the reinforcement learning equations require the prediction error (and thus the dopaminergic response) to transfer to the cue, thus mimicking Pavlovian conditioning.  This behavior was observed in the dopamine response as well \cite{Roesch:2007p2519, McClure:2003p3346}.  In sum, all characteristic predictions made by the reinforcement learning equations
\footnote{Or to be precise, the Rescorla-Wagner and Temporal Difference family of reinforcement learning models} has been observed in the dopaminergic signal.

Furthermore reinforcement learning models are statistically predictive of non-human animal's choice behaviors \cite{Hampton:2007p2983}.  Single doses of dopamine antagonists and agonists have also demonstrated a casual relationship between dopamine levels and learning rate \cite{Pizzagalli:2008p6521, Diaconescu:2010p7631}, which is broadly though not exclusively consistent with a reinforcement theory interpretation.  Reward prediction terms have also been shown to statistically mediate cortical-striatal coupling \cite{denOuden:2010p7203}.  Not limited to the above predictions, the reinforcement learning account has extended substantially, both theoretically and empirically.

Based on novel findings about novelty \cite{Bunzeck:2006p5319, Blatter:2006p6372, GuitartMasip:2010p7244} the reward prediction hypothesis has been extended to incorporate activity observed following presentation of novel stimuli \cite{Kakade:2002p6414} as well as to explain reward anticipatory firing via an average reward prediction error \cite{Knutson:2007p1687}. Another variation allowed for the observation of simultaneous neural implementation of model-free and model-based reinforcement learning \cite{Smith:2006p7627, Daw:2011p7995}. Alternative but reconcilable accounts have also been offered that allow for dissociation of first and second order conditioning as well as Pavlovian to instrumental transfer \cite{OReilly:2007p827}. The reward prediction hypothesis has also been incorporated into theoretical accounts of addiction \cite{Redish:2004p2531} and been used to predict the salience of upcoming stimuli \cite{Behrens:2007p8839}.   

There are several aspects of dopamine function which are, as yet, are unaccounted for theoretically.  \citeNP{Matsumoto:2009p7219}, reported a very broad set of dopaminergic firing patterns. In the classical (bivalent) view dopamine neurons should fire more for unexpected rewards or omission of punishment and less for reward omission and in response to aversive events.   Instead \citeNP{Kim:2006p1063, Matsumoto:2009p7219}, found that some neurons respond as expected but many others showed an inverse coding scheme, responding positively to aversive stimuli increasing as the punishment grew larger than expected, and decreasing as it grew smaller than expected.  Yet other neurons also responded positively to \emph{both} appetitive and aversive conditions when expectations were exceeded and negatively when they predictions were optimistic.  In a separate experiment \citeNP{Smith:2011p8133} demonstrated \emph{simultaneous} tunings to reward value, reward expectancy, salience as well as to novelty.   The dopamine response also appears to adaptively scale with past reward magnitudes, similar to the reward value divided by the cumulative variance \cite{Tobler:2005p6373}  If these new reports hold up and the dopaminergic response is this complex, the bivalent view needs substantial refinement, as do perhaps our analysis techniques; many different models or neural coding schemes may \emph{correctly} fit the same data, an issue which has received some prior attention outside of neuroimaging
\footnote{
    For my take on solving this problem in fMRI, see the methods section.
} \cite{Chamberlin:1965p8873}.

\subsubsection{Networked Plausibility}
\label{subsub:plausibility}
The dopaminergic firing patterns outlined above originate in the VTA/SNc, a small brainstem nucleus whose neurons project strongly to both the striatum, prefrontal cortex and the hippocampus.  Electrophysiological recordings of VTA/SNc neurons show two firing modes -- tonic and phasic \cite{DawNW:2006p6343}.  The phasic mode is of interest here, as it is this that putatively reflects the reward predictions error. A reward prediction \emph{error} signal of course requires predictions.  In this case predictions of total future reward, which itself requires both estimates of reward value, as well as the probability of receiving a reward.  How exactly such predictions are made is only partly understood.  Candidate regions for this calculation include the striatum, the limbic system routed through the habenula, as well at the orbital, ventral medial and the dorsal lateral frontal cortices.  I present each in turn, leaving the totality largely unintegrated, thus accurately reflecting the literature's state.

\subsubsection{Selecting Striatum}
\label{subsub:selstr}
The striatum is the input area of the basal ganglia, a brain region involved in categorization, logical inference, habit formation, working memory and feedback mediated stimulus-response learning \cite{Frank:2001p1996,Jin:2010p7199,SchmitzerTorbert:2004p5410,Seger:2008p6401,Seger:2010p7189,Yin:2006p5080}.  In stimulus-response learning, two of the four striatal subregions (the head of the caudate and the ventral striatum) process reward information \cite{Yin:2005p5101,Yin:2008p6347,Schonberg:2009p6669}.  These two are highly innervated by projections from the VTA/SNc, but only the ventral striatum correlates with the reward prediction error signal \cite{Haruno:2006p3979,Seger:2010p7189}.  The remaining two regions (the body and tail of the caudate and the putamen) are involved with stimulus-response pair formation, specifically visual categorization and response selection, respectively \cite{Seger:2008p6401,Seger:2010p7189}.  Though these regions also receive VTA/SNc projections and are sometimes sensitive to reward level \cite{BischoffGrethe:2009p4570} the BOLD signal does not correlate with the reward prediction error \cite{Seger:2010p7189}.  Dopamine's exact role in these areas is less clear.  In addition to projecting to basal ganglia, VTA/SNc also receives input from the internal globus pallidus or GPi (a major output structure of the basal ganglia with widespread cortical connections), thalamus, and the central nucleus of the amygdala \cite{Botvinick:2008p6594}.   Thus the striatum may form a value evaluation \emph{and} action selecting loop, similar to the classic actor-critic reinforcement learning architecture \cite{Bornstein:2011p7996,Ito:2011p8146} though this view has recently received some strong criticisms \cite{Joel:2002p6593}.

Intact dopamine projections and striatal function is necessary for rapid stimulus-response learning.  Administering dopamine antagonists to human and non-human animals adversely affects stimulus-response learning \cite{Pizzagalli:2010p7205}, as does lesioning the VTA/SNc.  Complete lesions of the striatum also prevent stimulus-response learning \cite{Packard:2002p5074}.  Administering dopamine agonists or the readily converted precursor L-DOPA leads to increases in response vigor and the ability of a Pavlovian-conditioned stimulus to bias unrelated instrumental responses (i.e. Pavlovian instrumental transfer) \cite{Winterbauer:2007p6352}. Both Pavlovian instrumental transfer and response vigor are, in part, facilitated by phasic dopamine increasing activity in the ventral striatum.  Unmedicated Parkinson's patients, who have low striatal dopamine levels, show marked decreases in stimulus-response learning with rewarding outcomes when compared to patients on medication and healthy age and intellect matched controls \cite{Pizzagalli:2010p7205}.  These same patients show an enhanced capability to learn from negative feedback which suggests that decreases in dopamine convey negative outcome information \cite{Frank:2004p4709}.  Finally, there is a solid body of evidence suggesting that phasic dopamine alters the plasticity of neurons in the striatum which presumably facilitates stimulus-response learning \cite{Calabresi:2007p4284}.

\subsubsection{Linking with the Limbic}
\label{subsub:limbic}
Recent work has highlighted the habenula (a small nucleus posterior to the thalamus) as being especially important in generating reward prediction error-like phasic activity in VTA/SNc.  Besides its limbic connections (to the amygdala, hippocampus and the serotonergic dorsal raphe nucleus \cite{Hikosaka:2008p4455}) the lateral habenula has reciprocal connections with the GPi and projects to the VTA/SNc.  Based on this anatomy, it has been suggested that the habenula could serve a point of intersection between the striatum and the limbic system.  Inline with this proposed role, the habenula can tonically inhibit or disinhibit dopamine release in VTA/SNc neurons, making VTA/SNc activity inversely correlated with the habenula.  As habenula activity decreases, burst firing in VTA/SNc increases.  Likewise as habenula firing increases, VTA/SNc activity temporarily pauses.  Reversible chemical inhibition of habenula also increases VTA/SNc phasic activity \cite{Hikosaka:2008p4455}.  Lesions to the habenula result in marked increases in dopamine levels in the dorsal and ventral striatum \cite{BrombergMartin:2010p7221}.  Detailed dual recording studies of both areas \emph{hint} that, combined, these areas calculate the value of stimulus-response pairs \cite{BrombergMartin:2010p7221}.  In summary, the GPi, withs its access to cortical inputs via the striatum, and habenula with its capability for altering VTA/SNc activity, may form the physiological loop necessary to calculate of the reward prediction error.

\subsubsection{Front and Center, Lateral Too}
\label{subsub:fclt}
Orbital frontal cortex has been repeatedly shown in neuroimaging \cite{ODoherty:2001p2423} and lesion \cite{Hornak:2004p6234} studies to encode the absolute value of rewarding or punishing outcomes, thus playing a pivotal role in the new field of neuroeconomics \cite{Glimcher:2005p863}.  However orbital frontal areas are more than a simple value store.  They also play a role in response and outcome recall, responses selection \cite{Rudebeck:2008p4712, Furuyashiki:2008p1631}, motivation, pain and pleasure \cite{Atlas:2010p7566}, outcome anticipation and prediction \cite{Tanaka:2006fk, Roesch:2007p7182} and causal attribution \cite{Tanaka:2008p3265}.  Additionally, orbital function is highly dependent on the basolateral amygdala \cite{ODoherty:2003p2616} offering a path for orbital activity to inform reward prediction error calculation.  

Besides value, estimating the likelihood a reward will occur is the other key reward prediction calculation.  Correlations with the chance of receiving a reward \cite{Tobler:2009p8297}, the variance of expected value \cite{Kahnt:2010p7677} as well as with risk-seeking behaviors \cite{Tobler:2007p1562} have all been reported in dorsolateral prefrontal areas.  These same areas have also bee implicated in inter-temporal choice, i.e. deciding between immediate and delayed rewards \cite{Kim:2009p8304,Kim:2008p2984}.  

Like orbital areas and dorsal areas, the nearby ventral medial prefrontal areas (which lie dorsal to the orbtial areas and extend into inside the central fissure), encode aspects of value, encoding information on both reward magnitude and probability, i.e. expected value \cite{Knutson:2005p1627}.  Possibly acting then to synthesize orbital and dorsal-lateral activities.  Unlike orbital cortex, who's encoding is linked to only to visual stimuli, value in the ventral medial PFC is associated with actions \cite{Glascher:2009p9352}.   Furthering a possible integrative interpretation of dorsal lateral activities, values in this area seem to be embedded in a ``higher order''  or abstract representation that reflects the statistical structure of the underlying task.  This abstract representation seems to support the inference of value \cite{Hampton:2006p2577}.  However ventral medial cortex's role in inference may be more general. It also plays a role in inferring the valued expectations of an opponent in two-player game of strategy \cite{Hampton:2008p6345} as well as inferring others intentions outside of economic games \cite{Cooper:2010p9353}.

% --
\subsection{Bad Prediction, No Cookie}
\label{sub:bad}
My hypothesis that rewards are represented as categories in the brain assumes that dopamine, specifically that phasic activity in VTA/SNc leads to brief elevations in the concentration of dopamine in striatal and cortical areas, acts to stamp in stimulus-response relationships.  I'll now take a critical look at that assumption.

\subsubsection{Not cortex, colliculus}
\label{subsub:colliculus}
Recent work by \citeNP{Dommett:2005p7263}, is a \emph{potentially} deadly issue for the reward prediction hypothesis.  The reward prediction theory requires very specific timing in order to map rewards to states and actions.  This requirement is satisfied by dopamine activity patterns, which are typically seen as 100 ms long burst about 100 ms after the initial stimulus.  However 100 ms is not much time for visual processing to occer, let alone prefrontal examination.  Concerned about the plausibility of such rapid processing, Dommet \emph{et al} disinhibited neurons in the brains of anesthetized rats in both the superior colliculus, a brainstem visual processing area, and early visual cortex with as GABA antagonist (which temporarily restores neural activity in the normally unresponsive neurons of an anesthetized animal) then exposed the animals opened eyes to set of 2 Hz light pulses while recording dopamine cells in the SNc/VTA as well as neurons in both visual cortex and the superior colliculus.  Disinhibition of the visual cortex lead to no changes in dopamine firing.  Superior colliculus disinhibition resulted in about half the recorded cells in the VTA/SNc to fire, which was similar in character to that typically observed following an unexpected rewarding event.  Another third of the dopamine cells displayed a pause in activity, similar to that observed when an expected reward fails to arrive \cite{Mirenowicz:1994p7185}.  The remaining cells responded first positively then negatively.  From this the authors concluded that the superior colliculus and not the visual cortex is an effective activator of VTA/SNC neurons
\footnote{
    Another reason for more cautious interpretation: Dommet \emph{et al} disinhibition experiments in visual cortex were very limited (N=4 cells compared to 30 for the superior colliculus experiments).  Nor did they assess the extent of disinhibition in visual cortex.
}. This is a problem as the superior colliculus responds only to very limited range of visual stimuli -- appearance, disappearance, or movement of objects as well as luminance changes.  It does not respond to contrast, velocity, wavelength or the geometric configuration of stimuli \cite{Dommett:2005p7263}. That is the superior colliculus couldn't realistically extract sufficient information from the visual stimuli used in nearly all the studies of reward to date.  Instead, Dommett \emph{et al} argue that all of the many reward studies ``can be solved based on luminance changes and/or of the position of specific reward-related visual stimuli''.  In other words, the expectancy-of-reward related changes in phasic dopamine, are an artifact of the task design.  However this interpretation is too strong.  At best they have shown that there is not substantive direct connection between visual areas and the VTA/SNc.  If there was only a single downstream synapse between visual cortex and SNc/VTA (which would add around 2-10 ms in lag) their protocol would not have disinhibited it and so would have failed to elicit a dopamine response during visual stimulation.   Given the brain's high degree of inter-connectivity and probable small-world architecture \cite{bassett:2006aa} a failure to find a direct anatomical relation is not in and of itself conclusive.  That said, how the dopamine signal can be so quick and consistent remains an open and very important question.

\subsubsection{Oh, wanting....}
\label{subsub:salience}
Standing in opposition to both the ahedonia and reward prediction hypotheses is the incentive salience account, which is derived from the brute fact that addicts often greatly want drugs of abuse, but once drugs are received addicts, or their animal model counterparts, do not report an excess of pleasure or liking \cite{Robinson:1993p8987}.  And indeed pharmacological investigation of striatal areas support a distinction between wanting and liking \cite{Berridge:2003p8998}.  Recent experiments with dopamine deficient mice lead \citeNP{Berridge:2007p7235}, to argue the putative dopaminergic reward signal instead signals degree of desire, i.e. wanting or in their parlance incentive salience.  Tyrosine hydroxylase knockout (DD) mice who are without detectable levels of dopamine in the brains, can learn a reward contingent T-maze task (where the animal must move left or right at the end of a corridor) -- but they do not act on that learning until dopamine is restored \cite{Berridge:2007p7235}.  DD mice do display a reward preference when given the choice of a sweetened water versus untreated water, but unless dopamine is restored their overall desire for both is greatly decreased.  Based on these findings \citeNP{Berridge:2007p7235}, argue that dopamine is not necessary nor sufficient for reward driven learning.  That is, dopamine is not a casual agent in stimulus-response learning.  Combined with studies of addicts (and their non-human animal equivalents) who display increased ``wanting'' of drugs but not ``liking'' (people rate the experience as no more pleasurable than controls, mice consume no more of the substance that controls) \citeNP{Berridge:2007p7235}, argue that phasic dopamine signals a stimulus' incentive salience, a synonym for wanting or desire.  Conclusions based on DD mice though should be, I think, more constrained, as these mice extraordinarily lethargic.  To pep them up, caffeine is administered.  And caffeine, through a cascade driven by adenosine A2A and cannabinoid CB1 receptors in ventral striatum, can have biochemical effects similar to dopamine \cite{Lazarus:2011p8137, Rossi:2010p7252}.

Taken in isolation the experiments in DD mice are quite damning to the theory of dopamine plays a casual role in stimulus-response learning.  There is however a substantial body of work supporting a causation.  Administering human and non-human animals dopamine antagonists adversely affects stimulus-response learning \cite{Pizzagalli:2010p7205}, as does lesioning either the VTA/SNc or portions of the striatum.  Complete lesions of the striatum prevent stimulus-response learning \cite{Packard:2002p5074}.  This is relevant as it is the interaction between phasic dopamine and the striatum that is proposed to guide (drive) stimulus-response learning. Administering dopamine agonists, or the readily converted dopamine precursor L-DOPA, leads to increased Pavlovian instrumental transfer, as well as response vigor \cite{Winterbauer:2007p6352}, both of which are thought to be facilitated by the interaction between phasic dopamine and activity in the ventral striatum.  Parkinson's patients when off medication, and so are lacking striatal dopamine, show marked decreases in stimulus-response learning \cite{Pizzagalli:2010p7205}.  These same off-medication patients show an enhanced (compared to normal age and intellect matched controls) capability to learn from negative feedback, suggesting their ability and desire to act is intact \cite{Frank:2004p4709}.

While it not clear how theoretically or functionally the evidence for incentive salience and reward prediction might be reconciled, it might not be necessary.  \citeNP{Smith:2011p8133}, showed distinct semi-overlapping tuning in VTA/SNc for both reward prediction, incentive salience as well as with measures of the animals enjoyment of the in-task reward (i.e. liking).  I believe there may in fact be no one correct theoretical accounting; the neurons of the VTA/SNc may signal instead a family of functions - for other supporting examples see, \citeNP{Ito:2011p8146, Smith:2011p8133, Bornstein:2011p7996, BrombergMartin:2010p7218, Matsumoto:2009p7219}.


% --
\subsection{Thinking About Thinking Rewarding Thoughts}
\label{sub:cogrew}
As I stated at the outset, cognition alone can generate activity similar in appearance and effect to that seen following primary and secondary rewards.  For example, \citeNP{Tricomi:2008p6663}, showed ventral striatum BOLD signal changes in a declarative memory task in which subjects were initially trained with feedback to distinguish 60 correct from incorrect word pairs.  In the subsequent two rounds explicit feedback was withheld but activity in the caudate was observed when correct pairings were matched based on memory alone, i.e goal achievement led to strong activity.  In two economic decision making tasks strong ventral striatum signals were observed when participants were required merely to imagine or consider alternative outcomes  \cite{Hayden:2009p6545, Lohrenz:2007p7240}.  Information about the future is rewarding as well; \citeNP{BrombergMartin:2009p7220}, showed that complex visual clues about an upcoming outcome were in themselves sufficient to cause bursts of firing the in VTA/SNc.  Inversly, neutral stimuli can prevent decreases in responding that normally accompany repeated delays in reward presentation \cite{Reed:1992p9094}, a phenomenon that was reproduced using a robotic rat, wherein the neutral stimuli were treated as intrinsically rewarding \cite{Fiore:2008p7249}.

Informative, or to change terms to keep with other literatures, salient
\footnote{
    Not to be confused with the ``incentive salience'', discussed above.
}, stimuli have been observed to have rewarding-like effects in people as well, though supporting data is limited to fMRI experiments.  Striatal BOLD increases have been observed in response to infrequently presented flashing images \cite{Zink:2003p5107}, and unexpected alarming tones (e.g. a siren replacing a constant 60Hz tone) \cite{Zink:2006p7210}.  Activity in the ventral striatum appeared in these tasks due to the stimuli alone, while dorsal activity was seen only when the stimuli had behavioral relevance.  A control task suggested this increase was not in response to the additional motor demands of the response but was, the authors argued, due to the increased salience of the active versus passive condition.  The context of behavioral response made the reward more salient.  A similar dorsal/ventral division has been reported when comparing passive reward receipt to reward receipt requiring a response \cite{ODoherty:2006p2875} though these were attributed directly to the need for an instrumental response and not (necessarily) contextual salience.  Additionally, like reward expectations, salience-related activity scaled with intensity \cite{Zink:2006p7210}.

Novel (but not necessarily salient) stimuli also elicit reward prediction-like dopaminergic firing in monkeys \cite{Blatter:2006p6372} and in people \cite{Bunzeck:2006p5319}. Indeed, reward and novelty appear interchangeable.  \citeNP{GuitartMasip:2010p7227}, showed that when novel images proceed rewarding outcomes enhanced ventral striatal activity compared to reward alone.  Rewards proceeding a visual stimulus or word-pair leads to enhanced memory for that pair \cite{Lisman:2005p5455}.  Building on that finding \citeNP{Wittmann:2007p3328}, showed enhanced recognition of natural scenes when images were proceeded by novel images.  This effect was reproduced using high-resolution imaging of the VTA/SNc, with that area demonstrating a marked reward prediction signal during the task \cite{Krebs:2011p8134}.  This effect was extended further to the anticipation of novel images, similar to reward anticipation studies of striatal function \cite{Knutson:2001p5234}.  Novelty driven exploitation/exploration decision making relies on the striatum as well \cite{Wittmann:2008p541}.

In sum, task completion, imagined rewards, neutrally valued informative cues, behaviorally salient but non-rewarding events, as well as novel images, have all been shown to act as reinforcers, and to stimulate the dopaminergic midbrain into phasic firing.  None of these, individually or as a group, can be explained parsimoniously as secondary rewards
\footnote{
    Nor as primary rewards, but this is a definitional problem.
}.  They were never \emph{statistically regularly} paired with a primary reward.


% --
\subsection{Generally Generalizable.}
\label{sub:gen}
In a recent article \citeNP{Wimmer:2012p8836}, made the argument that a stimulus' reinforcement derived value must be generalizable to other similar stimuli as there are far too many possible states in the world to explore each individually, \emph{and} therefore you rarely if ever encounter the same exact same stimulus twice.  In essence, they make an argument that categories are necessary for reinforcement learning to be a reliable guide in our complex changing world.  I'll now extend that reasoning.  There are at least as many outcomes as there are stimuli, i.e. any stimulus can be an outcome, in principle anyway. There are therefore too many possible outcomes to ever experience them all, \emph{and} you'll rarely if ever see the same outcome twice.  As a result, you often can't predict exactly what will happen. Therefore like stimuli, outcome expectations must generalize.  As desirable outcomes are rewarding (see \ref{sub:cog_rew} in the \emph{Introduction}), reward representations generalize.  Skipping ahead a little, I then hypothesize that the act of generalization will impact reward prediction error calculations.

\subsubsection{Needed similarities}
\label{subsub:needed}
And indeed many of the cognitive rewards outlined above may employ or even require generalization, often in the form of similarity assessment.  For example, to find that successful task completion is rewarding \citeNP{Tricomi:2008p6663}, used a cued recall task, which in part relies on familiarity \cite{Jacoby:1991p9096} and thus similarity \cite{Nosofsky:1988p9098}.  Likewise \citeNP{Hayden:2009p6545, Lohrenz:2007p7240} studies of fictive rewards, rewards created only in the participants imaginations implicitly requires an act of inference; Past desires must be extrapolated into a present experience.  \citeNP{Roesch:2007p2519}, demonstrated rewarding inferences in rats as well.  When novel stimuli were presented in the context of a familiar task with the same action options previously available, the dopamine spikes that resulted were significantly correlated with the most valued past action.  Likewise assessing both salience and incentive salience requires participants to extrapolate past context and goals and into future experiences.  In fact \citeNP{Zink:2004p5108}, showed that rewards sans salience elicited no striatal BOLD activity.  This finding suggests that rewarding activity must be driven by more than just the current hedonic stimulation.

\subsubsection{Birds do it}
\label{subsub:birds}
While so far as I am aware my argument for cognitive-type rewards and their hypothesized category representation is novel, there are several non-human animal studies examining the generalizable properties of secondary reinforcers, though the neural mechanisms for such are unexamined.  As an example, \citeNP{Guttman:1956p8355}, varied an instrumentally conditioned 570 nm light from 480-610, showing that while the bird's pecking rate (i.e. response vigor) decreased as one moved farther from 540, the birds still responded.  They generalized across the wavelengths.  When novel variations of conditioned stimuli from two sensory modalities were mixed similar graded changes in vigor were observed  \cite{Guttman:1956p8355}.  Pigeons' capability for generalization was not limited to though even mixtures of simple cues: \cite{Nakamura:2006p9093} taught pigeons to discriminate categories of male and female birds and \citeNP{Smith:2011p9101}, taught pigeons perceptually complex yet abstract categories.  Interestingly though, not all experiments were effective at producing generalization behavior.  Some combinations produced no responses at all \cite{Blough:2001p8408,Simmons:2008p8405,Urcuioli:2001p8359}.  Interstingly, the degree of generalization was not uniform across stimuli, e.g. generalization between shapes decreased more rapidly than colors \citeNP{Shepard:1987p9102}.  The source of this variability remained unknown until \citeNP{Shepard:1987p9102} seminal insight.

\subsubsection{Curves and categories}
\label{subsub:curves}
\citeNP{Shepard:1987p9102} demonstrated that the variability among instrumental response generalization curves in pigeons and other animals could be accounted for if one no longer measured generalized responding by an objective metric (e.g. wavelength) but instead estimated the psychological space of the animal's perception.  In his own words: ``I propose to start with the generalization data and to ask: Is there a unique monotone function whose inverse will transform those data into numbers interpretable as distances in some appropriate metric space''. The metric space he proposed required ``[...] a very strong additivity condition: For each subset of three points, the distance between the two most widely separated points equals the sum of the distances between those two points and the third point that lies between them''.  After making some additional assumptions
\footnote{
    $1.$ that a given stimulus is equiprobable over the psychological space and $2.$ that categories in the space are centrally symmetric and convex
} Shepard then goes onto show that the large variability among the many objective spaces he examined only minimally impacted the estimated (negative exponential or Gaussian) psychological space.  In other words, in many cases the objective space can be mapped to psychological distances by simply taking the negative exponential of their Euclidean distances
\footnote{
    Though this geometrical relation in not necessary.  Shepard's insight can be re-derived using information theory \cite{Chatera:2003p9103}}.  Shepard's work suggests that whether the psychological space is exponential or Gaussian depends whether the stimuli were distinct or perceptually confusable, which was partly supported in theoretical studies of perceptual noise on generalization \cite{Ennis:1988p9359}.  Outside of Shepard's theory though the Gaussian metric has substantial empirical and theoretical support in studies of human category learning, even when confusability of the stimuli is low \cite{Nosofsky:1985p9356,Medin:2012p9358}.  As such the divide between exponential (as seen in the Pigeon studies above) and Gaussian metrics (often seen in Human studies) remains unbridged.

Based partly on Shepard's work the fields of perceptual and cognitive categorization have since made an ever evolving study of how animals create and use categories.  However there are still substantial controversies with, so far as I can tell, no one consensus view.  The two extreme theoretical positions are exemplar models and prototype models \cite{Ashby:2005p4764}.  Exemplar models, in their simplest form, are feature counters, i.e. their categories are completely defined as the sum of previously observed information \cite{Nosofsky:1988p9104}.  Prototype models instead hold only information about one ideal template \cite{Rosch:1973p9108}, often implemented as probability density \cite{Ashby:1995p9109}, i.e. a set of sufficient statistics over the relevant information.  A common example is the mean and covariance, assuming prototype following matching a Gaussian distribution.  In between the two extremes are hybrid approaches, of which there are many.  These include using mixture models \cite{Rossee:2002p9112}, neural networks implementing self-organised maps \cite{Love:2004p9110}, and other clustering techniques \cite{Kruschke:2012p9111} as well as kernel methods \cite{Jakel:2008p9113}, among other increasingly esoteric approaches \cite{Martin:2012p9185}; There are many more examples, as categorization has been an active area of mathematical psychologists investigation for over 40 years.

This deep mathematical literature is further complicated by empirical investigations of human category learning, which suggest that regardless of the underlying representation, humans treat different category structures in different ways \cite{Ashby:2011p9148}.  In their hands category learning has been subdivided
into systems to handle rule-based (i.e. verbalizable), information-integration (implicit, forces consideration of $>1$ dimension of information), prototype-distortion (perceptual categorization), and unstructured category (categories without perceptual or semantic overlap, akin to sets in that their only necessary commonality is the fact they're grouped together).  Like finding the neural correlates of these systems \cite{Ashby:2005p9152,Ashby:2006p9153}, integrating them with the many formal approaches is in its early days \cite{Ashby:2011p9148}.

% --
\subsection{Why Are We Here Again?}
\label{sub:goals}
I want to show whether cognitive rewards are represented as categories in the brain, and to examine what impact such representations have on the reinforcement learning process.  To that end participants completed a stimulus-response task where classical rewards (e.g. ``Correct!'' or ``Win \$1.'') were replaced with pre-trained visually and cognitively complex categories, one category for gains and one for losses.  To learn the correct stimulus-response relations subjects then had to infer outcome and value.   Behavioral and fMRI data collected under this task was then compared to a series of computational and functional connectivity models representing various possible algorithmic strategies for reward inference and assessing category membership.  These analyses are spread across three chapters.  The first covers the details of the task itself, the computational models and their parameter optimization.  Chapter two covers the fMRI methods and basic brain-mapping analyses, as well as the fMRI-to-model compairson.  Chapter three covers the functional analysis and its methods.

\newpage
\bibliography {bibmin}
%%%%%%%%%%%%%
\end{document}
%%%%%%%%%%%%%
