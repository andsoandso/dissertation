\documentclass[doc,12pt]{apa}        % use: 'man' for submission type; 'jou' for
                                % journal type, and 'doc' for typical latex
                                % but with figures inline with text
\usepackage{geometry} 
%\geometry{a4paper} 
\usepackage[parfill]{parskip}   % paragraphs delimited by an empty line

\usepackage{graphicx} 
\usepackage{amssymb}            % no idea what this does...
\usepackage{epstopdf}           % no idea what this does...
%\usepackage{gensymb}            % no idea what this does...

\usepackage{setspace}

\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png} \setcounter{secnumdepth}{0}  % no idea what this does...

\usepackage{apacite}
%%%%%%%%% END HEADER %%%%%%%%%

\title{Rewards are categories.} 
\author{Erik J. Peterson} \affiliation{Dept. of Psychology \\ Colorado State University \\ Fort Collins, CO} 

%%%%%%%%%%%%%%%%
\begin{document} 
%%%%%%%%%%%%%%%%
\maketitle
\doublespacing

\section{Introduction} % (fold)
\label{sec:introduction}
Birds will peck repeatedly, as mice will push levers, monkeys will hit buttons, and men will buy flowers, if each of these actions is followed by a primary reward -- food, drink or sex.  Buttons, levers and flowers have no value alone, so reinforcement theory goes, it is only by the \emph{statistically regular pairing} with primary rewards that value is transfered \cite{Rescorla:1988p8743}.  This is the classical view, and it has, in general, held up many years now \cite{iversen:2007aa}.  And indeed, the neural mechanisms of reinforcement learning are becoming increasingly clear following years of exciting and intense inquiry \cite{Glimcher:2011p8464, Montague:2006mz}.  However, due to their reliance on primary and secondary reward concepts, reinforcement learning theories can't account for two important facts.  One, rewarding effects are observed in the absence of primary and secondary reinforcers \cite{Hayden:2009p6545, Lohrenz:2007p7240, Tricomi:2008p6663, Jimura:2010p8305}. Two, value can be transfered by inference, no pairing is needed \cite{BrombergMartin:2010p7223, Hampton:2006p2577}.  Based on fMRI and behavioral data, I examine possible mechanisms of both of these aspects by modeling rewards as categories.  Categories are intrinsically cognitive and inferential.

This introduction has five parts.  First I review classical rewards and the reward prediction error account of dopamine function, along with its anatomical basis.  Second, I switch hats and (non-fatally) criticise these accounts. Third, I make a case for cognitive rewards -- rewarding activity observed outside primary and secondary reinforcement. Fourth I argue for the necessity of generalizable reward representations, covering stimulus generalization and categorization along the way.  In the fifth and final section, the exact goals and methods of this work are laid out.

\subsection{Classics, Expectations and Tissues} % (fold)
\label{sub:c_and_e}
\subsubsection{A pleasurable start} % (fold)
Classically rewards and reinforcers have been linked to (or simply were) food \cite{ODoherty:2006p2875}, pain \cite{Becerra:2011p7581,schultz:2007aa} and sex though for, err, logistical reasons this is less often used in the laboratory; They're certainly potent, being used for over 50 successful years to study learning in animal models \cite{iversen:2007aa} and people \cite{Kim:2010p7248,Montague:2006mz}.  In the 1950's the first clue how food and water neuronally cause reinforcement arose in the electrical self-stimulation studies of \citeNP{OLDS:1954p8747}, and, \citeNP{Crow:1972p8748}.  Olds and colleges observed that when electrodes were placed in the dopamenergic midbrain animals would vigorously and repeated self-stimulate, using the available button.  By the 1970s data from pharmacological studies of rats, electrochemical recordings, knowledge of the signalling mechanisms of dopamine receptors, as well as neoleptic drug actions in Schizophrenic patients, along with Old's shocking work, lead to the first major theoretical proposal for dopamine's role - a signal for pleasure, i.e. the anhedonia hypothesis, \cite{Wise:1978p8771}.  However within 10 years it became clear that dopamine's role extend beyond signaling primary rewards
\footnote{
    To which the hypothesis was bafflingly limited
} and pleasure. Activity was seen following secondary rewards, novelty, salience, and others \cite{Spanagel:1999p8515, Salamone:2005p8774, BrombergMartin:2010p8834}.  And, more importantly, dopamine depleted animals continued enjoy rewards, i.e. they still developed taste preferences, enhanced response vigor \cite{Cannon:2003p8513} and continued to respond to opiates \cite{Hnasko:2005p8832}.  However in 1994 there was a surprise that would eventually explain many of anhedonia's deficits.  \citeNP{Mirenowicz:1994p7185}, reported that dopamengeric firing depended on how expected a reward was, which blossomed into the reward prediction error theory under review here.

\subsubsection{Expectations Matter} % (fold)
\label{sub:the_classics}
\citeNP{Hollerman:1998qy} more fully explored, \citeNP{Mirenowicz:1994p7185}, observation showing that unexpected rewards lead to increases in the firing rate, fully expected rewards elicit no response, while expected rewards that failed to arrive lead to dip in the baseline firing rate.   \citeNP{Roesch:2007p2519} found the same patterns in rats while \citeNP{ODoherty:2003p6329}, found them too in fMRI studies of the human striatum; Striatal BOLD changes reflect phasic dopamine  activity \cite{Schonberg:2009p6669,Surmeier:2007p4435}
\footnote{
    Though this has recently come under question in rat models comparing single unit and field recordings to high resolution blood flow changes \cite{Mishra:2011p9095}}. In ground breaking work, \citeNP{Waelti:2001p6523}, showed that contrary to behaviorist expectations stimulus-response learning as well as the dopamenergic signal was maximized not be reward reliability (i.e. more rewards are not always better) but instead that learning was maximized when rewards were intermittent. \citeNP{Waelti:2001p6523}, along with, \citeNP{Fiorillo:2003p6375} and \citeNP{Bayer:2005ul}, successfully modelled changes in reward expectancy with a reward prediction error term derived from a reinforcement learning model
\footnote{
    Specifically the Rescorla-Wagner model
} fit to each animal's behavior. The reward prediction error from the model strongly correlated with the dopamine response - both its increases and decreases. However expectancy related changes are not the only important prediction reinforcement learning models make for dopamenergic activity.  Reward value must transfer.  If an initially neutral cue reliable predicts a reward the reinforcement learning equations require prediction error (and thus the dopamenergic response) to transfer to the cue, thus mimicking Pavlovian conditioning.  This behavior was observed in the dopamine response as well \cite{Roesch:2007p2519, McClure:2003p3346}.  In sum, every quantitative prediction made by reinforcement learning models has been observed in the dopamenergic signal.

Furthermore reinforcement learning models are statistically predictive of non-human animal's choice behaviors \cite{Hampton:2007p2983}.  Single doses of dopamine antagonists and agonists have also demonstrated a casual relationship between dopamine levels and learning rate \cite{Pizzagalli:2008p6521, Diaconescu:2010p7631}, which is broadly though not exclusively consistent with a reinforcement theory interpretation.  Reward prediction terms have also been shown to mediate cortical-striatal coupling \cite{denOuden:2010p7203}.  Not limited to the above predictions, the reinforcement learning account has extended substantially, both theoretically and empirically.

Based on novel findings about novelty \cite{Bunzeck:2006p5319, Blatter:2006p6372, GuitartMasip:2010p7244} the reward prediction hypothesis has been extended to incorporate activity observed following presentation of novel stimuli \cite{Kakade:2002p6414} as well as to explain reward anticipatory firing via an average reward prediction error \cite{Knutson:2007p1687}. Another variation allowed for the observation of simultaneous neural implementation of model-free and model-based reinforcement learning \cite{Smith:2006p7627, Daw:2011p7995}. Alternative but reconcilable accounts have also been offered that allow for dissociation of first and second order conditioning as well as pavlovian to instrumental transfer \cite{OReilly:2007p827}. The reward prediction hypothesis has also been incorporated into theoretical accounts of addiction \cite{Redish:2004p2531} and to predict the saliance/information of upcoming stimuli \cite{Behrens:2007p8839}, among others.   

Additionally there are several findings which are, as yet, are unaccounted for theoretically.  \citeNP{Matsumoto:2009p7219}, reported a very broad set of dopamenergic firing patterns. In the classical (bivalent) view dopamine neurons should fire more for rewards, negatively to reward omission, and positively to the omission of punishment, and negatively to aversive events.   Instead \citeNP{Kim:2006p1063, Matsumoto:2009p7219}, found that some neurons respond as expected but many others responded positively to aversive stimuli increasing as the punishment grew larger than expected, and decreasing as it grew smaller than expected.  Rather then demonstrating an inverse coding scheme (as above), many neurons also responded positively to \emph{both} appetitive and aversive conditions when expectations were exceeded and negatively when they predictions were optimistic.  In a separate experiment \citeNP{Smith:2011p8133} demonstrated \emph{simultaneous} tunings to both reward value, reward expectancy, salience as well as to novelty.   The dopamine response also appears to adaptively scale with past reward magnitudes, similar to the reward value divided by the cumulative variance \cite{Tobler:2005p6373}  If these new reports hold up and the dopamenergic response is this complex, the bivalent view needs substantial refinement, as do perhaps our analysis techniques; many different models or neural coding schemes may \emph{correctly} fit the same data, an issue which has received some prior attention outside of neuroimaging
\footnote{
    For my take on solving this problem in fMRI, see the methods section.
} \citeNP{Chamberlin:1965p8873}.

\subsubsection{Networked Plausibility}
\label{sub:net_plaus}
The dopamenergic firing patterns outlined above originate in the VTA/SNc, a small brainstem nucleus whose neurons project strongly to both the striatum, prefrontal cortex and the hippocampus.  Electrophysiological recordings of VTA/SNc neurons show two firing modes -- tonic and phasic \cite{DawNW:2006p6343}.  The phasic mode is of interest here, as it is this that putatively reflects the reward predictions error. A reward prediction \emph{error} signal of course requires predictions.  In this case predictions of total future reward, which itself requires both estimates of reward value, as well as the probability of receiving a reward.  How exactly such predictions are made is only partly understood.  Candidate regions for this calculation include the striatum, the limbic system routed through the habenula, as well at the orbital, ventral medial and the dorsal lateral frontal cortices.  I present each in turn, leaving the totality largely unintegrated, thus accurately reflecting the literature's state.

\subsubsection{Selecting Striatum}
\label{sub:sel_str}
The striatum is the input area of the basal ganglia, a brain region involved in categorization, logical inference, habit formation, working memory and feedback mediated stimulus-response learning \cite{Frank:2001p1996,Jin:2010p7199,SchmitzerTorbert:2004p5410,Seger:2008p6401,Seger:2010p7189,Yin:2006p5080}.  In stimulus-response learning, two of the five striatal subregions (the head of the caudate and the ventral striatum) process reward information \cite{Yin:2005p5101,Yin:2008p6347,Schonberg:2009p6669}.  These two are highly innervated by projections from the VTA/SNc, but only the ventral striatum correlates with the reward prediction error signal \cite{Haruno:2006p3979,Seger:2010p7189}.  The remaining three regions (the body and tail of the caudate and the putamen) are involved with stimulus-response pair formation, visual categorization and response selection, respectively \cite{Seger:2008p6401,Seger:2010p7189}.  Though these three also receive VTA/SNc projections and are sometimes sensitive to reward level \cite{BischoffGrethe:2009p4570}, the BOLD signal does not correlate with the reward prediction error \cite{Seger:2010p7189}; dopamine's exact role in these areas is less clear.  In addition to projecting to basal ganglia, VTA/SNc also receives input from the internal globus pallidus or GPi (a major output structure of the basal ganglia with widespread cortical connections), thalamus, and the central nucleus of the amygdala \cite{Botvinick:2008p6594}.   Thus the striatum may form a value evaluation \emph{and} action selecting loop, similar to the classic actor-critic reinforcement learning architecture \cite{Bornstein:2011p7996,Ito:2011p8146} though this view has recently received some strong criticisms \cite{Joel:2002p6593}.

Intact dopamine projections and complete striatal function is necessary for rapid stimulus-response learning.  Administering dopamine antagonists to human and non-human animals adversely affects stimulus-response learning \cite{Pizzagalli:2010p7205}, as does lesioning the VTA/SNc.  Complete lesions of the striatum also prevent stimulus-response learning \cite{Packard:2002p5074}.  Administering dopamine agonists or the readily converted precursor L-DOPA leads to increases in response vigor and the ability of a Pavlovian-conditioned stimulus to bias unrelated instrumental responses (i.e. pavlovian instrumental transfer) \cite{Winterbauer:2007p6352}. Both pavlovian instrumental transfer and response vigor are, in part, facilitated by phasic dopamine increasing activity in the ventral striatum.  Unmedicated Parkinson's patients, who have low striatal dopamine levels, show marked decreases in stimulus-response learning with rewarding outcomes when compared to patients on medication and healthy age and intellect matched controls \cite{Pizzagalli:2010p7205}.  These same patients show an enhanced capability to learn from negative feedback which suggests that decreases in dopamine convey negative outcome information \cite{Frank:2004p4709}.  Finally, there is a solid body of evidence suggesting that phasic dopamine alters the plasticity of neurons in the striatum which presumably facilitates stimulus-response learning \cite{Calabresi:2007p4284}.

\subsubsection{Linking with the Limbic}
\label{sub:limbic_inte}
Recent work has highlighted the habenula (a small nucleus posterior to the thalamus) as being especially important in generating reward prediction error-like phasic activity in VTA/SNc.  Besides its limbic connections (to the amygdala, hippocampus and the serotonergic dorsal raphe nucleus \cite{Hikosaka:2008p4455}) the lateral habenula has reciprocal connections to the GPi and projections into the VTA/SNc.  Based on this anatomy, it has been suggested that the habenula could serve a point of intersection between the striatum and the limbic system.  Inline with this proposed role, the habenula can act to tonically inhibit or disinhibit dopamine release in VTA/SNc neurons, making VTA/SNc activity inversely correlate compared to the habenula.  As habenula activity decreases, burst firing in VTA/SNc increases.  Likewise as habenula firing increases, VTA/SNc activity temporarily pauses.  Reversible chemical inhibition of habenula also increases VTA/SNc phasic activity \cite{Hikosaka:2008p4455}.  As lesions to the habenula result in marked increases in dopamine levels in the dorsal and ventral striatum \cite{BrombergMartin:2010p7221}.  Detailed dual recording studies of both areas \emph{hint} that combined these areas are calculating the value of stimulus-response pairs \cite{BrombergMartin:2010p7221}.  In summary, the GPi, withs its access to cortical inputs via the striatum, and habenula with is , with its capability for altering VTA/SNc activity, may form the physiological loop necessary to calculate (portions) of the reward prediction error.

\subsubsection{Front and Center, Lateral Too}
\label{sub:f_and_c}
Orbital frontal cortex has been repeatedly shown in neuroimaging \cite{ODoherty:2001p2423} and lesion \cite{Hornak:2004p6234} studies to encode the absolute value of rewarding or punishing outcomes, thus playing a pivtal role in the new field of neuroeconomics \cite{Glimcher:2005p863}.  However orbital frontal areas are more than a simple value store.  They also play a role in response and outcome recall as well as responses selection \cite{Rudebeck:2008p4712, Furuyashiki:2008p1631}, motivation, pain and pleasure \cite{Atlas:2010p7566}, outcome anticipation and prediction \cite{Tanaka:2006fk, Roesch:2007p7182} and causal attribution \cite{Tanaka:2008p3265}.  Additionally, orbital function is highly dependent on the basolateral amygdala \cite{ODoherty:2003p2616} offering a path for orbital activities to inform reward prediction error calculation.  

\emph{TODO}: Add ventral medial section; read more on it first.

Besides value, estimating the likelihood a reward will occur is the other key reward prediction calculation.  Correlations with both the chance of receiving a reward \cite{Tobler:2009p8297}, the variance of expected value \cite{Kahnt:2010p7677} as well as with risk-seeking behaviors \cite{Tobler:2007p1562} have been reported in the dorsolateral prefrontal areas.  These same areas have also bee implicated in inter-temporal choice, i.e. deciding between immediate and delayed rewards \cite{Kim:2009p8304,Kim:2008p2984}.  However despite a lack of complete or formal unification between striatal, prefrontal and limbic areas, several efforts have been made combining frontal and striatal function, i.e. the aforementioned cortical-striatal loops \cite{Frank:2011p8152, Seger:2010p7189, Frank:2001p1996, Ashby:2007p8986} as well as other normative \cite{BarGad:2003p4052, Botvinick:2008p6594} and data-driven approaches \cite{Bogacz:2007p753}.


% --
\subsection{Bad Prediction, No Cookie}
\label{sub:wrong}
My hypothesis that rewards represented categorically in the brain assumes that dopamine, specifically phasic projections from the VTA/SNc to striatal and cortical areas, acts to stamp in stimulus-response relationships.  I'll now take a critical look at that assumption.

\subsubsection{Not cortex, colliculus}
\label{sub:not_cor_colliculus}
Recent work by \citeNP{Dommett:2005p7263}, is a \emph{potentially} deadly issue for the reward prediction hypothesis.  The reward prediction theory requires very specific timing in order to map rewards to states and actions (i.e to solve the temporal credit assignment problem).  This requirement is satisfied, with dopamine activity peaking in a 100 ms long burst about 100 ms after the initial stimulus.  However 100 ms is not much time for visual processing, let alone prefrontal examination.  Concerned about the plausibility of such rapid processing, Dommet \emph{et al} disinhibited neurons in the brains of anesthetized rats in both the superior colliculus, a brainstem visual processing area, and early visual cortex with sa GABA antagonist (whichs temporarily restores neural activity in the normally unresponsive neurons of an anesthetized animal) the exposed the animals opened eyes to set of 2 Hz light pulses while recording dopamine cells in the SNc/VTA as well as neurons in both visual cortex and the superior colliculus.  Disinhibition of the visual cortex lead to no changes in dopamine firing.  While superior colliculus disinhibited resulted in about half the recorded cells in the VTA/SNc to display phasic firing similar in character to that typically observed following an unexpected rewarding event.  Another third of the dopamine cells displayed a pause in activity, similar to that observed when an expected reward fails to arrive \cite{Mirenowicz:1994p7185}.  The remaining cells responded first positively then negatively.  From this the authors concluded that the superior colliculus and not the visual cortex is an effective activator of VTA/SNC neurons
\footnote{
    Another reason for more cautious interpretation: Dommet \emph{et al} disinhibition experiments in visual cortex were very limited (N=4 cells compared to 30 for the superior colliculus experiments).  Nor did they assess the extent of disinhibition in visual cortex.
}. This is a problem as the superior colliculus responds only to very limited range of visual stimuli -- appearance, disappearance, or movement of objects as well as luminance changes.  It does not respond to contrast, velocity, wavelength or the geometrical configuration of stimuli \cite{Dommett:2005p7263}. That is the superior colliculus couldn't realistically extract reward information from the visual stimuli used in nearly all the studies of reward to date.  Instead, Dommett \emph{et al} argue that all of the many reward studies ``can be solved based on luminance changes and/or of the position of specific reward-related visual stimuli''.  In other words, the expectancy-of-reward related changes in phasic dopamine, is an artifact of the task design.  However this interpretation is too strong.  At best they have shown that there is not substantive direct connection between visual areas and the VTA/SNc.  If there was only a single downstream synapse between visual cortex and SNc/VTA (which would add around 2-10 ms in lag) their protocol would not have disinhibited it and so would have failed to elicit a dopamine response during visual stimulation.   That is, given the brain's high degree of inter-connectivity and probable small-world architecture \cite{bassett:2006aa} a failure to find a direct anatomical relation is not in and of itself conclusive.  That said, how the dopamine can be so quick and consistent remains an open and very important question.

\subsubsection{Oh, wanting....}
\label{salience}
Standing in opposition to both the ahedonia and reward prediction hypotheses is the incentive salience account, which is derived from the brute fact that addicts often greatly want drugs of abuse, but once drugs are received addicts, or their animal model counterparts, do no report an excess of pleasure \cite{Robinson:1993p8987}.  And indeed pharmacological investigation of striatal areas supports distinction between wanting and liking \cite{Berridge:2003p8998}.  However recent experiments with dopamine deficient mice lead \citeNP{Berridge:2007p7235}, to argue the putative dopamenergic reward signal instead signals degree of desire, i.e. wanting or in their parlance incentive salience.  Tyrosine hydroxylase knockout (DD) mice, mice without detectable levels of dopamine in the brains, can learn a reward contingent T-maze tasks (where the animal must move left or right at the end of a corridor) -- though they do not act on that learning till dopamine is restored \cite{Berridge:2007p7235}.  DD mice do however display a reward preference when given the choice of a sweetened water versus untreated water. However unless dopamine is restored their overall desire for either is greatly decreased.  Based on these findings \citeNP{Berridge:2007p7235}, argue that dopamine is not necessary nor sufficient for reward driven learning.  That is dopamine is not a casual agent in stimulus-response learning.  Combined with studies of addicts (and there non-human animal equivalents) who display increased ``wanting'' of drugs but not ``liking'' (people rate the experience as no more pleasurable than controls, mice consume no more of the substance that controls) \citeNP{Berridge:2007p7235}, argue that phasic dopamine signals a stimulus' incentive salience, a synonym for wanting or desire.  Interpretations based on DD mice though should be, I think, more constrained, as these mice extraordinarily lethargic.  To pep them up, caffeine is administered.  And caffeine, through a cascade driven by adenosine A2A and cannabinoid CB1 receptors in ventral striatum, can have biochemical effects similar to dopamine \cite{Lazarus:2011p8137, Rossi:2010p7252}.

Taken in isolation these experiments in DD mice are quite damning to the notion of dopamine playing a casual role in stimulus-response learning.  There is however a substantial body of work suggesting the opposite.  Administering human and non-human animals dopamine antagonists adversely effects stimulus-response learning \cite{Pizzagalli:2010p7205}, as does lesioning either the VTA/SNc or portions of the striatum.  Complete lesions of the striatum prevent stimulus-response learning \cite{Packard:2002p5074}.  This is relevant as it is the interaction between phasic dopamine and the striatum that is proposed to guide (drive) stimulus-response learning. Administering dopamine agonists, or readily converted dopamine precursor L-DOPA, leads to increased pavlovian instrumental transfer, as well as response vigor \cite{Winterbauer:2007p6352}, both of which are thought to be facilitated by the interaction between phasic dopamine and activity in the ventral striatum.  Parkinson's patients when taken off medication, and therefore are lacking striatal dopamine, show marked decreases in stimulus-response learning \cite{Pizzagalli:2010p7205}.  These same off-medication patients show an enhanced (compared to normal age and intellect matched controls) capability to learn from negative feedback, suggesting their ability and desire to act is intact \cite{Frank:2004p4709}.

While it not clear how theoretically or functionally the evidence for incentive salience and reward prediction might be reconciled, it might not be necessary.  \citeNP{Smith:2011p8133}, showed distinct semi-overlapping tuning in VTA/SNc for both reward prediction, incentive salience as well as with measures of the animals enjoyment of the in-task reward (i.e. liking).  I believe there may in fact be no one correct theoretical accounting; the neurons of the VTA/SNc may signal instead a family of functions - for other supporting examples see, \citeNP{Ito:2011p8146, Smith:2011p8133, Bornstein:2011p7996, BrombergMartin:2010p7218, Matsumoto:2009p7219}.


% --
\subsection{Thinking About Thinking Rewarding Thoughts}
\label{sub:cog_rew}
As I stated at the outset, cognition alone can generate activity similar in appearance and effect to that seen following primary and secondary rewards.  For example, \citeNP{Tricomi:2008p6663}, showed ventral striatum BOLD signal changes in a declarative memory task in which subjects were initially trained with feedback to distinguish 60 correct from incorrect word pairs.  In the subsequent two rounds explicit feedback was withheld but activity in the caudate was observed when correct pairings were matched based on memory alone; Goal achievement, led to strong activity.  In two economic decision making tasks strong ventral striatum signals were observed when participants were required merely to imagine or consider alternative outcomes  \cite{Hayden:2009p6545, Lohrenz:2007p7240}.  Information about the future is rewarding as well; \citeNP{BrombergMartin:2009p7220}, showed that complex visual clues about an upcoming outcome were in themselves sufficient to cause bursts of firing the in VTA/SNc.  Inversly, neutral stimuli can prevent decreases in responding that normally accompany repeated delays in reward presentation \cite{Reed:1992p9094}, a phenomenon that was reproduced using a robotic rat, wherein the neutral stimuli were treated as intrinsically rewarding \cite{Fiore:2008p7249}.

Informative, or to change terms to keep with other literatures, salient
\footnote{
    Not to be confused with the ``incentive salience'', discussed above.
}, stimuli have been observed to have rewarding-like effects in people as well, though supporting data is limited to fMRI experiments.  Striatal BOLD increases have been observed in response to infrequently presented flashing images \cite{Zink:2003p5107}, and unexpected alarming tones (e.g. a siren replacing a constant 60Hz tone) \cite{Zink:2006p7210}.  Activity in the ventral striatum appeared in these tasks due to the stimuli alone, while dorsal activity was seen only when the stimuli had behavioral relevance.  A control task suggested this increase was due not to the additional motor demands of the response but was, the authors argued, due to the increased saliency of the active versus passive condition.  That is the context of behavioral response made the reward more salient.  A similar dorsal to ventral division has been reported when comparing passive reward receipt to reward receipt requiring a response \cite{ODoherty:2006p2875} though these were attributed directly to the need for an instrumental response and not (necessarily) contextual salience.  Additionally, like reward expectations salience-related activity scaled with intensity \cite{Zink:2006p7210}.

Novel (but not necessarily salient) stimuli also elicit reward prediction-like dopamenergic firing in monkeys \cite{Blatter:2006p6372} and in people \cite{Bunzeck:2006p5319}. Indeed, reward and novelty appear interchangeable.  \citeNP{GuitartMasip:2010p7227}, showed that when novel images proceed rewarding outcomes enhanced ventral striatal activity compared to reward alone.  Rewards proceeding a visual stimulus or word-pair leads to enhanced memory for that pair \cite{Lisman:2005p5455}.  Building on that \citeNP{Wittmann:2007p3328}, showed enhanced recognition of natural scenes, compared to control, when images were proceeded by novel images.  This effect was reproduced using high-resolution imaging of the VTA/SNc, with that area demonstrating a marked reward prediction signal during the task.  This effect was extended to the anticipation of novel images, similar to reward anticipation studies of striatal function \cite{Knutson:2001p5234}.  Novelty driven exploitation/exploration decision making relies on the striatum as well \cite{Wittmann:2008p541}.

Task completion, imagined rewards, neutrally valued informative cues, behaviorally salient but non-rewarding, as well as novel events have all been shown to act as reinforcers, and stimulate the dopamenergic midbrain into phasic firing.  None of these, individually or as a group, can be explained parsimoniously as secondary rewards
\footnote{
    Nor as primary rewards, but this is a definitional problem.
}.  They were never \emph{statistically regularly} paired with a primary reward.


% --
\subsection{Generally Generalizable.}
\label{sub:gen}
In a recent review \citeNP{Wimmer:2012p8836}, made the argument that a stimulus' value must be generalizable to other similar stimuli: there are far too many possible states in the world to explore each individually, \emph{and} you rarely if ever encounter the same stimulus twice.  In essence, they make an argument that categories are necessary.  I'll now extend that reasoning.  There are at least as manys outcomes as there are stimuli: there are therefore too many possible outcomes to ever experience them all, \emph{and} you'll rarely see the same outcome twice.  As a result, you often can't predict exactly what will happen. Therefore, outcome expectations must generalize.  As desirable outcomes are rewarding (see \ref{sub:cog_rew}), rewards must generalize as well.

\subsubsection{Needed similarities}
\label{sub:must_be_fuzzy}
And indeed many of the cognitive rewards outlined may employ or even require generalization, often in the form of similarity assessment.  \citeNP{Tricomi:2008p6663}, finding that successful task completion is rewarding used a cued recall task, which in part relies on familiarity \cite{Jacoby:1991p9096} and thus similarity \cite{Nosofsky:1988p9098}.  Likewise \citeNP{Hayden:2009p6545, Lohrenz:2007p7240} studies of fictive rewards, rewards that were experienced only in the participants imaginations, must have been derived from past memories.  Assessing both salience and incentive salience requires an understanding of the current context and an assessment general goals and desirable outcomes.  In fact \citeNP{Zink:2004p5108}, showed that reward sans salience elicited no striatal BOLD activity.  Even the relatively simple seeming cases of temporal discounting of rewards and the assessment of reward uncertainty requires cognitive intervention, which is reflected in several reports of complex, multi-valued, reward-related signals in both dorsal and ventral-medial prefrontal cortices \cite{Tobler:2009p8302,Wallis:2010p8303,Kim:2009p8304,Seymour:2008p6518}.  Outside of similarity, \citeNP{Roesch:2007p2519}, demonstrated rewarding inferences in rats. When novel stimuli were presented in the context of a familiar task with the same action options previously available, the dopamine spikes that resulted were significantly correlated with the most valued past action.

\subsubsection{Birds do it}
\label{sub:var_val}
While so far as I am aware my argument for cognitive-type rewards and their hypothesized category representation is novel, there are several non-human animal studies examining the generalizable properties of secondary reinforcers, though the neural mechanisms for such are unexamined.  As an example, \citeNP{Guttman:1956p8355}, varied an instrumentally conditioned 570 nm light from 480-610, showing that while the bird's pecking rate (i.e. response vigor) decreased as one moved farther from 540, the birds still responded, that they is generalized.  When novel variations of conditioned stimuli from two sensory modalities were mixed similar graded changes in vigor were observed.  Pigeons capability for generalization was not limited to simple cues, \cite{Nakamura:2006p9093} taught pigeons to discriminate categories of male and female birds.  While \citeNP{Smith:2011p9101}, taught pigeons to perceptually complex yet abstract categories.  Interestingly though, not all experiments were effective at producing generalization behavior.  Some combinations produced no responses at all \cite{Blough:2001p8408,Simmons:2008p8405,Urcuioli:2001p8359}.  The degree of generalization was not uniform, it depended on the stimulus, e.g. generalization between shapes decreased more rapidly than colors \citeNP{Shepard:1987p9102}.  The source of this variability remained unknown until \citeNP{Shepard:1987p9102} seminal 
\footnote{
    \dots not to mention painfully self-aggrandizing.
} insight.

\subsubsection{Curves and categories}
\citeNP{Shepard:1987p9102} demonstrated that the variability among instrumental response generalization curves in pigeons and other animals could be accounted for if one no longer measured generalized responding by an objective metric (e.g. wavelength) but instead estimated the psychological space of the animal's perception.  In his own words: ``I propose to start with the generalization data and to ask: Is there a unique monotone function whose inverse will transform those data into numbers interpretable as distances in some appropriate metric space''. The metric space he proposed required ``[...] a very strong additivity condition: For each subset of three points, the distance between the two most widely separated points equals the sum of the distances between those two points and the third point that lies between them''.  After making some additional assumptions
\footnote{
    $1.$ that a given stimulus is equiprobable over the psychological space and $2.$ that categories in the space are centrally symmetric and convex
} Shepard then goes onto show that the large variability among the many objective spaces he examined only minimally impacted the estimated (negative exponential) psychological space.  In other words, in many cases the objective space can be mapped to psychological distances by simply taking the negative exponential of their euclidean distances
\footnote{
    In fact, Shepard's work is correct free of this geometrical constraint, being we redone using information theory and kolmogorov complexity \cite{Chatera:2003p9103}; a lovely paper.}.

Based partly on Shepard's work the fields of perceptual and cognitive categorization have since made an every evolving study of how animals create and use categories.  However there are still substantial controversies with, so far as I can tell, no one consensus view.  The two extreme theoretical positions are exemplar models and prototype models \cite{Ashby:2005p4764}.  Exemplar models, in their simplest form, are feature counters, i.e. their categories are completely defined as the sum of previously observed information \cite{Nosofsky:1988p9104}.  Prototype models instead hold only information about one ideal template \cite{Rosch:1973p9108}, often implemented as probability density \cite{Ashby:1995p9109}, i.e. a set of sufficient statistics over the relevant information.  A common example is the mean and covariance, assuming prototype following matching a Gaussian distribution.  In between the two extremes are hybrid approaches, of which there are many.  These include using mixture models \cite{Rossee:2002p9112}, neural networks implementing self-organised maps \cite{Love:2004p9110}, and other clustering techniques \cite{Kruschke:2012p9111} as well as kernel methods \cite{Jakel:2008p9113}, among other increasingly esoteric approaches \cite{Martin:2012p9185}; There are many more examples, as categorization has been an active area of mathematical psychologists investigation for over 40 years.

This deep mathematical literature is further complicated the Ashby Lab's empirical investigations of human category learning, which suggests, regardless of the underlying representation, humans treat different category structures in different ways \cite{Ashby:2011p9148}.  In their hands category learning has been subdivided
into systems to handle rule-based (i.e. verbalizable), information-integration (implicit, forces consideration of $>1$ dimension of information), prototype-distortion (perceptual categorization), and unstructured category (categories without perceptual or semantic overlap, akin to sets in that their only necessary commonality is the fact they're grouped together).  Like finding the neural correlates of these systems \cite{Ashby:2005p9152,Ashby:2006p9153}, integrating them with the many formal approaches is in its early days \cite{Ashby:2011p9148}.

% --
\subsection{Why Are We Here Again?}
\label{sec:goals}
I want to show whether cognitive rewards are represented as categories in the brain, and to examine what impact such representations have on the reinforcement learning process.  To that end participants completed a stimulus-response task where classical rewards (e.g. ``Correct!'' or ``Win \$1.'') were replaced with pre-trained visually and cognitively complex categories, one category for gains and one for losses.  To learn the correct stimulus-response relations subjects then had to infer outcome and value.   Behavioral and fMRI data collected under this task was then compared to a series of computational and functional connectivity models representing various possible algorithmic strategies for reward inference and assessing category membership.  These analyses are spread across three chapters.  The first covers the details of the task itself, the computational models and their parameter optimization.  Chapter two covers the fMRI methods and basic brain-mapping analyses, as well as the fMRI-to-model compairson.  Chapter three covers the functional analysis and its methods.

\newpage
\bibliography {bibmin}
%%%%%%%%%%%%%
\end{document}
%%%%%%%%%%%%%
