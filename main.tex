\documentclass[doc,12pt]{apa}        % use: 'man' for submission type; 'jou' for
                                % journal type, and 'doc' for typical latex
                                % but with figures inline with text
\usepackage{geometry} 
%\geometry{a4paper} 
\usepackage[parfill]{parskip}   % paragraphs delimited by an empty line

\usepackage{graphicx} 
\usepackage{amssymb}            % no idea what this does...
\usepackage{epstopdf}           % no idea what this does...
%\usepackage{gensymb}            % no idea what this does...

\usepackage{setspace}

\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png} \setcounter{secnumdepth}{0}  % no idea what this does...

\usepackage{apacite}
%%%%%%%%% END HEADER %%%%%%%%%

\title{Categories of rewards.} 
\author{Erik J. Peterson} \affiliation{Dept. of Psychology \\ Colorado State University \\ Fort Collins, CO} 

%%%%%%%%%%%%%%%%
\begin{document} 
%%%%%%%%%%%%%%%%
\maketitle
\doublespacing

\section{Introduction} % (fold)
\label{sec:introduction}
Birds will peck repeatedly, as mice will push levers, monkeys will hit buttons, and men will buy flowers, if each of these actions is followed by a primary reward -- food, drink or sex.  Buttons, levers and flowers have no value alone, so reinforcement theory goes, it is only by the \emph{statistically regular pairing} with primary rewards that value is transfered \cite{Rescorla:1988p8743}.  This is the classical view, and it has, in general, held up many years now \cite{iversen:2007aa}.  In fact, the neural basis reinforcement learning currently receives substantial attention, with notable progress is being made \cite{Glimcher:2011p8464, Montague:2006mz}.  However, reinforcement learning theories can't at current account for two recent key findings in the neural correlates of human learning. (1) the neural correlates of reward can appear by cognition alone \cite{Hayden:2009p6545, Lohrenz:2007p7240, Tricomi:2008p6663, Jimura:2010p8305}. (2) Value can be transfered by inference, no pairing is needed \cite{BrombergMartin:2010p7223, Hampton:2006p2577}. Using a mixture of fMRI and computational modeling, I examine possible mechanisms of both of these aspects by modeling the reward representations as a kind of category; categories, i.e. abstract generalizable represenations, are exclusively cognitive and require inference.

This introduction has six parts.  First I discuss classical rewards and their neural correlates and what is known of their anatomical bases, including criticsims of the reward prediction account of dopamine function that underlies this work.  Second I make a case for cognitive rewards, discussing as an example novelty, then moving onto other examples, and finally arguing for the necessity of generalizable reward representations.  Third is a discussion of prior studies of value generalization in pigeons and other non-human animals as well as in humans, though the literature on the latter is sparse.  Fourth is a brief introduction to formal models of categorization, which leads into the fifth and final section, the specific goals and methods of this work.

\subsection{Classics, Expectations and Tissues} % (fold)
\label{sub:c_and_e}
\subsubsection{A pleasurable start} % (fold)
Classically rewards and reinforcers have been linked to (or simply were) food \cite{ODoherty:2006p2875}, pain \cite{Becerra:2011p7581,schultz:2007aa} and sex though for, err, logistical reasons this is less often used in the laboratory, especially in human subjects; They're certainly potent, being used for over 50 successful years to study learning in animal models \cite{iversen:2007aa} and people \cite{Kim:2010p7248,Montague:2006mz}.  In the 1950's the first clue how food and water cause reinforcement arose in the electrical self-stimulation studies of \citeNP{OLDS:1954p8747}, for a classic review see \citeNP{Crow:1972p8748}.  Olds and colleges observed that when electrodes, which could be activated by a self-determined button press, were placed in the midbrain and in limbic areas animals would vigorously and repeated self-stimulate.  By the 1970s data from pharmacological studies of rats, electrochemical recordings, knowledge of the signalling mechanisms of dopamine receptors, as well as neoleptic drug actions in Schizophrenic patients, along with Old's shocking work, lead to the first major theoretical proposal for dopamine's role - a signal for pleasure (i.e. the anhedonia hypothesis, \citeNP{Wise:1978p8771}).  However within 10 years it became clear that dopamine's role extend beyond signaling primary rewards. Activity was seen following secondary rewards, novelty, salience, and others \cite{Spanagel:1999p8515, Salamone:2005p8774, BrombergMartin:2010p8834}.  And, more importantly, dopamine depleted animals continued enjoy rewards, i.e. they still developed taste preferences, enhanced response vigor \cite{Cannon:2003p8513} and continued to respond to opiates \cite{Hnasko:2005p8832}
\footnote{
Though both these criticisms are constrained as dopamine deficient mice, sans caffeine, are extraordinarily lethargic. To pep them up, caffeine is administered.  And caffeine, through a cascade driven by adenosine A2A and cannabinoid CB1 receptors in ventral striatum, has biochemical effects similar to dopamine \cite{Lazarus:2011p8137, Rossi:2010p7252}
}.  
  However in 1994 there was a surprise that would eventually explain many of anhedonia's deficits.  Schultz \emph{et al} reported that dopamengeric firing depended on how expected a reward was \cite{Mirenowicz:1994p7185}, which blossomed into the reward prediction error theory under review here.

\subsubsection{Expectations Matter} % (fold)
\label{sub:the_classics}
As I said \citeNP{Mirenowicz:1994p7185}, was the first paper to demonstrate that phasic activity in the midbrain was dependent not just on the hedonic value of the reward, but was qualitatively related to both the value of the reward and how expected that reward was.  \citeNP{Hollerman:1998qy} more fully explored this observation, showing that unexpected rewards lead to increases in the firing rate, stronly expected rewards elicited no response, while expected rewards that failed to arrive lead to a pause.  \citeNP{Roesch:2007p2519} found the same patterns in rats. \citeNP{ODoherty:2003p6329}, found them too in fMRI studies of humans.  Meanwhile \citeNP{Fiorillo:2003p6375}, along with \citeNP{Bayer:2005ul}, quantified the relationship between the unexpectedness of the reward and phasic activity via a reward prediction error term derived from a reinforcement learning model fit to each animal's behavior. The reward prediction error from the model strongly correlated with the dopamine response - both its increases and decreases.  A second key similarity between reinforcement learning models and the dopamenergic response was transfer.  If a cue reliable predicts a reward the reinforcement learning equations require value to transfer from the primary reward to the cue, mimicking Pavlovian conditioning (which was the initial goal of these models).  This same behavior was observed in the dopamine response \cite{Roesch:2007p2519, McClure:2003p3346}. Furthermore RL models are statistically predictive of non-human animal's choice behaviors \cite{Hampton:2007p2983}. Dopamenergic firing patterns are consistent with optimal formal learning theories \cite{Waelti:2001p6523} and has been shown to mediate cortical-striatal coupling \cite{denOuden:2010p7203}.  Single doses of dopamine antagonists and agonists have also demonstrated a casual relationship between dopamine levels and learning rate \cite{Pizzagalli:2008p6521, Diaconescu:2010p7631}, which is braodly though not exlcusivily consistent with a reinforcement theory interpreation.  These findings are the backbone of the reward prediction theory, which has since been extended substantially.  

Based on novel findings about novelty \cite{Bunzeck:2006p5319, Blatter:2006p6372, GuitartMasip:2010p7244} the reward prediction hypothesis has been extended to incorporate activity observed following presentation of novel stimuli \cite{Kakade:2002p6414} as well as to explain reward anticipatory firing via an average reward prediction error \cite{Knutson:2007p1687}. Another variation allowed for the observation of simultaneous neural implementation of model-free and model-based reinforcement learning \cite{Smith:2006p7627, Daw:2011p7995}.   Alternative but reconcilable accounts have also been offered that allow for dissociation of first and second order conditioning as well as pavlovian to instrumental transfer \cite{OReilly:2007p827}. The reward prediction hypothesis has also been incorporated into theoretical accounts of addiction \cite{Redish:2004p2531} and to predict the saliance/information of upcoming stimuli \cite{Behrens:2007p8839}, among others.   

Additionally there are several findings which are, as yet, are unaccounted for theoretically.  The dopamine response adaptively scales with past reward magnitudes, which appeared similar to reward value divided by the cumulative variance \cite{Tobler:2005p6373}.  \item \citeNP{Matsumoto:2009p7219}, reported a very braod set of dopamenergic firing patterns. In the classical (bivalent) view dopamine neurons should fire more for rewards, negatively to reward omission, and positively to the omission of punishment, and negatively to aversive events \cite{Kim:2006p1063}. Matsumoto:2009p7219}, found that some neurons respond as expected buy many others responded positively to aversive stimuli (an air puff on the eye) increasing as the punishment grew larger (than expected), and decreasing as it grew smaller.  Unexpectedly many neurons responded positively to \emph{both} appetitive and aversive conditions when expectations were exceeded and negatively when they predictions were optimistic.  \citeNP{Matsumoto:2009p7219}, further reported that neurons that responded positively to aversive events were clustered in the substantia nigra pars compacta while appetitivily tuned neurons appeared more often in the ventral tagmental area.  Likewise \citeNP{Smith:2011p8133} demonstrated \emph{simultanous} tunings to both reward value, reward expectancy, saliance as well as to novelty.  If the dopamenergic response is universally this complex, the bivalent view needs substantial refinement, as do perhaps our analysis techniques; many different yet related models may \emph{correctly} fit the same data, an issue whic hhas recieved some prior attention outside of neuroimaging\footnote{for my take on solving this problem in fMRI, see the methods section.} \citeNP{Chamberlin:1965p8873}.

\subsubsection{Networked Plausibility}
\label{sub:net_plaus}
The dopamenergic firing patterns outline above orginate in the VTA/SNc is a small brainstem nucleus whose dopamine-releasing neurons project strongly to both the striatum and the hippocampus.  Electrophysiological recordings of VTA/SNc neurons show two firing modes -- tonic and phasic \cite{DawNW:2006p6343}.  The phasic mode is of interest here, at it relfect theoretically a reward predictions error signal. A reward predictions \emph{error} singal of course reuires predictions.  In this case predictions of future value.  Though how exactly such predictions are made is only partly understood.  Canditate regions for this calculation include the striatum, the limbic system (via the habenula) as well at the orbital frontal and ventral medial cortices.  I present each in turn, leaving the totality unitegrated, thus accurately reflecting the literature's state -- tempting and incomplete.

\subsubsection{Selecting Striatum}
\label{sub:sel_str}
The striatum is an input area of the basal ganglia, a brain region highly involved in categorization, logical inference, habit formation, working memory and feedback mediated S-R learning \cite{Frank:2001p1996,Jin:2010p7199,SchmitzerTorbert:2004p5410,Seger:2008p6401,Seger:2010p7189,Yin:2006p5080}.  In S-R learning, two of the five striatal subregions (the head of the caudate and the ventral striatum) process reward information \cite{Yin:2005p5101,Yin:2008p6347,Schonberg:2009p6669}.  These two are highly innervated by projections from the VTA/SNc, but only the ventral striatum correlates with the RPE signal \cite{Haruno:2006p3979,Seger:2010p7189}.  The remaining three regions (the body and tail of the caudate and the putamen) are involved with S-R pair formation, visual categorization and response selection, respectively \cite{Seger:2008p6401,Seger:2010p7189}.  Though these three also receive VTA/SNc projections and are sometimes sensitive to reward level \cite{BischoffGrethe:2009p4570}, the BOLD signal does not correlate with the RPE \cite{Seger:2010p7189}; dopamine's exact role in these areas is less clear.  Overall though, intact dopamine projections and complete striatal function is necessary for rapid S-R learning.
   
Administering dopamine antagonists to human and non-human animals adversely affects S-R learning \cite{Pizzagalli:2010p7205}, as does lesioning the VTA/SNc.  Complete lesions of the striatum also prevent S-R learning \cite{Packard:2002p5074}.  Administering dopamine agonists or the readily converted precursor L-DOPA leads to increases in response vigor and the ability of a Pavlovian-conditioned stimulus to bias unrelated instrumental responses (i.e. pavlovian instrumental transfer) \cite{Winterbauer:2007p6352}. Both pavlovian instrumental transfer and response vigor are, in part, facilitated by phasic dopamine increasing activity in the ventral striatum.  Unmedicated Parkinson's patients, who have low striatal dopamine levels, show marked decreases in S-R learning with rewarding outcomes when compared to patients on medication and healthy age and intellect matched controls \cite{Pizzagalli:2010p7205}.  These same patients show an enhanced capability to learn from negative feedback which suggests that decreases in dopamine convey negative outcome information \cite{Frank:2004p4709}.  Finally, there is a solid body of evidence suggesting that phasic dopamine alters the plasticity of neurons in the striatum which presumably facilitates stimulus-response learning \cite{Calabresi:2007p4284}.

\subsubsection{Limbic Integration}
\label{sub:limbic_inte}
VTA/SNc also receives input from the internal globus pallidus or GPi (a major output structure of the basal ganglia with widespread cortical connections), thalamus, and the central nucleus of the amygdala \cite{Botvinick:2008p6594}.  Recent work though has highlighted the habenula (a small nucleus posterior to the thalamus) as being especially important in generating RPE-like phasic activity.  The lateral habenula has reciprocal connections to the GPi and projections into the VTA/SNc. Based on this anatomy, it has been suggested that this nucleus serves a point of intersection between the striatum and the limbic system (e.g. the amygdala, hippocampus and the serotonergic dorsal raphe nucleus) \cite{Hikosaka:2008p4455}. The habenula acts to tonically inhibit or disinhibit dopamine release in VTA/SNc neurons.  As habenula activity decreases, burst firing in VTA/SNc results; as habenula firing increases VTA/SNc firing is temporarily paused.  Dual recordings of the habenula and GPi suggest they form a functional loop capable of calculating the value of S-R pairs \cite{BrombergMartin:2010p7221}.  Reversible chemical inhibition of the habenula also increases VTA/SNc phasic activity.  Lesions to the habenula also result in marked increases in dopamine levels in the dorsal and ventral striatum \cite{BrombergMartin:2010p7221}.  In summary, the GPi (withs its access to cortical inputs via the striatum) and habenula (with its capability for altering VTA/SNc activity) may form the physiological loop necessary to calculate the RPE; \citeNP{BrombergMartin:2010p7218}, hint that this loop can signal both initial value estimates and rewarding outcomes.

\subsubsection{Front and Center, Lateral Too}
\label{sub:f_and_c}
Orbital frontal cortex has been repeatedly shown, both in neuroimaging \cite{ODoherty:2001p2423} and lesion \cite{Hornak:2004p6234} studies to encode the absolute value of rewarding or punishing outcomes, thus playing a pivtal role in the new field of neuroeconomics \cite{Glimcher:2005p863}. However orbital frontal areas are more than a simple value store.  They also plays a role in response and outcome recall \cite{Furuyashiki:2008p1631} and selection \cite{Rudebeck:2008p4712} as well as in motivation, pain and pleasure \cite{Atlas:2010p7566} and outcome anticipation \cite{Roesch:2007p7182} and outcome prediction \cite{Tanaka:2006fk} and causal attribution \cite{Tanaka:2008p3265}.  Additionally, orbital function is highly dependent on the basolateral amygdala \cite{ODoherty:2003p2616} offering a path for influencing reward prediction error calculation.  Estimating value is one part of a reward prediction but so is estimating the likelihood a reward will occur.  Correlations with both the chance of receiving a reward \cite{Tobler:2009p8297}, the variance of expected value \cite{Kahnt:2010p7677} as well as with risk-seeking behaviors \cite{Tobler:2007p1562} have been reported in the dorsolateral areas. These same areas have also bee implcated in inter-temporal choice, i.e. deciding between immediate and delayed rewards \cite{Kim:2009p8304,Kim:2008p2984}.  However despite a lack of complete or formal unification between striatal, prefrontal and limbic areas, several effort have been made at based on function overlap between frontal and striatal areas, i.e. the aforementioned cortical-striatal loops \cite{Frank:2011p8152, Seger:2010p7189, Frank:2001p1996, Ashby:2007p8986} as well as other normative \cite{BarGad:2003p4052, Botvinick:2008p6594}, or biologically plausible approaches \cite{Bogacz:2007p753}.

% --
\subsection{Bad Prediction, No Cookie}
\label{sub:wrong}
The question under examination -- are rewards represented categorically in the brain -- assumes that dopamine, specifically phasic projections from the VTA/SNc to striatal and cortical areas, acts to stamp in stimulus-response relationships.  I'll now take a critical look at that assumption.

\subsubsection{Not cortex, colliculus}
\label{sub:not_cor_colliculu}
Recent work by \citeNP{Dommett:2005p7263}, is a \emph{potentially} deadly issue for the reward prediction hypothesis.  The reward prediction theory requires very specific timing in order to map reward to events (i.e to solve the temporal credit assignment problem).  This requirement is satisfied, with dopamine activity peaking in a 100 ms long burst about 100 ms after the initial stimulus.  However 100 ms is not much time for visual processing, let alone prefrontal examination.Concerned about the plausibility of such rapid processing, Dommet \emph{et al} disinhibited neurons in the brains of anesthetized rats in both the superior colliculus, a brainstem visual processing area, and early visual cortex with sa GABA antagonist (whichs temporarily restores neural activity in the normally unresponsive neurons of an anesthetized animal) the exposed the animals opened eyes to set of 2 Hz light pulses while recording dopamine cells in the SNc/VTA as well as neurons in both visual cortex and the superior colliculus.  Disinhibition of the visual cortex lead to no changes in dopamine firing.  While superior colliculus disinhibited resulted in about half the recorded cells in the VTA/SNc to display phasic firing similar in character to that typically observed following an unexpected rewarding event.  Another third of the dopamine cells displayed a pause in activity, similar to that observed when an expected reward fails to arrive \cite{Mirenowicz:1994p7185}.  The remaining cells responded first positively then negatively.  From this the authors concluded that the superior colliculus and not the visual cortex is an effective activator of VTA/SNC neurons\footnote{
It should be noted that their disinhibition experiments in visual cortex were very limited (N=4 cells compared to 30 for the superior colliculus experiments).  Nor did they assess the extent of disinhibition in visual cortex.
 }. 
This is a problem as the superior colliculus responds only to very limited range of visual stimuli -- appearance, disappearance, or movement of objects as well as luminance changes.  It does not respond to contrast, velocity, wavelength or the geometrical configuration of stimuli \cite{Dommett:2005p7263}. That is the superior colliculus couldn't realistically extract reward information from the visual stimuli used in nearly all the studies of reward to date.  Instead, Dommett \emph{et al} argue that all of the many reward studies ``can be solved based on luminance changes and/or of the position of specific reward-related visual stimuli''.  In other words, the expectancy-of-reward related changes in phasic dopamine, is an artifact of the task design.  However this interpretation is too strong.  At best they have shown that there is not substantive direct connection between visual areas and the VTA/SNc.  If there was only a single downstream synapse between visual cortex and SNc/VTA (which would add around 2-10 ms in lag) their protocol would not have disinhibited it and so would have failed to elicit a dopamine response during visual stimulation.   That is, given the brain's high degree of inter-connectivity and probable small-world architecture \cite{bassett:2006aa} a failure to find a direct anatomical relation is not in and of itself conclusive.  That said, how the dopamine can be so quick and consistent remains an open and very important question.

\subsubsection{On wanting}
\label{salience}
Standing in opposition to both the anhedonia and reward prediction hypotheses is the incentive salience account which is derived from the brute fact that addicts often greatly want drugs of abuse, but once received do no report an excess of pleasure \cite{Robinson:1993p8987}.  And indeed pharmacological investigation of striatal reward areas supports this distinction \cite{Berridge:2003p8998}.  However recent experiments lead \citeNP{Berridge:2007p7235}, to argue the putative reward signal instead signal degree of desire, i.e. wanting or incentive saliance.  Tyrosine hydroxylase knockout (DD) mice, mice without detectable levels of DA in the brains, can learn a reward contingent T-maze tasks (where the animal must move left or right at the end of a corridor) -- though they do not act on that learning till DA is restored \cite{Berridge:2007p7235}.  An important caveat: in order to get the mice to run the maze they were administered a stimulant, caffeine.  Caffeine likely acts to enhance long term potentiation in the striatum, similar to DA's proposed mechanism of action \cite{Rossi:2010p7252}.  DD mice do however display a reward preference when given the choice of a sweetened water versus untreated water. However unless DA is restored their overall desire for either is greatly decreased.  Based on these findings \citeNP{Berridge:2007p7235}, argue that DA is not necessary nor sufficient for reward driven learning.  That is DA is not a casual agent in S-R learning.  Combined with studies of addicts (and there non-human animal equivalents) who display increased ``wanting'' of drugs but not ``liking'' (people rate the experience as no more pleasurable than controls, mice consume no more of the substance that controls) \citeNP{Berridge:2007p7235}, argue that phasic DA signals a stimulus' incentive salience, a synonym for wanting or desire.  

Taken in isolation these experiments in DD mice are quite damning to the notion of DA playing a casual role in S-R learning.  There is however a substantial body of work suggesting the opposite.  Administering human and non-human animals dopamine antagonists adversely effects S-R learning \cite{Pizzagalli:2010p7205}, as does lesioning either the VTA/SNc or portions of the striatum.  Complete lesions of the striatum prevent S-R learning \cite{Packard:2002p5074}.  This is relevant as it is the interaction between phasic DA and the striatum that is proposed to guide (drive) S-R learning. Administering DA agonists, or readily converted DA precursor L-DOPA, leads to increased pavlovian instrumental transfer, as well as response vigor \cite{Winterbauer:2007p6352}, both of which are thought to be facilitated by the interaction between phasic DA and activity in the ventral striatum.  Parkinson's patients when taken off medication, and therefore are lacking striatal DA, show marked decreases in S-R learning \cite{Pizzagalli:2010p7205}.  These same off-medication patients show an enhanced (compared to normal age and intellect matched controls) capability to learn from negative feedback, suggesting their ability and desire to act is intact \cite{Frank:2004p4709}.  

While it not clear how theoretically or functionally the evidence for incentive salience and reward prediction might be reconciled, it might not be necessary.  \citeNP{Smith:2011p8133}, showed distinct semi-overlapping tuning in VTA/SNc for both reward prediction, incentive salience as well as with measures of the animals enjoyment of the in-task reward (i.e. liking).  The more the firing properties of dopamine neurons are assessed, the more complex the dynamics appear.  I believe there may in fact be no one correct theoretical accounting; the neurons of the VTA/SNc may signal instead a family of functions - for other supporting examples see, \citeNP{Ito:2011p8146, Smith:2011p8133, Bornstein:2011p7996, BrombergMartin:2010p7218, Matsumoto:2009p7219}.


% --
\subsection{Rewarding cognitions}
\label{sub:cog_rew}
As I stated at the outset, cognition alone can generate activity similar in appearance and effect to that seen following primary and secondary rewards.  \citeNP{Tricomi:2008p6663}, showed ventral striatum BOLD signal changes in a declarative memory task in which subjects were initially trained with feedback (``Right'' or ``Wrong'') to distinguish 60 correct from incorrect word pairs.  In the subsequent two rounds explicit feedback was withheld but activity in the caudate was observed when correct pairings were matched based on memory alone.  Correct matches, that is goal achievement, led to strong activity.  In two economic decision making tasks strong ventral striatum signals were observed when participants were required merely to imagine or consider alternative outcomes  \cite{Hayden:2009p6545, Lohrenz:2007p7240}.  Information about the future is rewarding as well; \citeNP{BrombergMartin:2009p7220}, showed that complex visual clues about an upcoming outcome were in themselves sufficient to cause bursts of firing the in VTA/SNc.  Which in agreement with rat studies where apparently neutral stimuli diminish decreases in responding that normally accompany delays in reward presentation \cite{Reed:1992p9094}.  Such behavior was recently reproduced using a robotic rat, wherein the neutral stimuli were treated as intrinsically rewarding \cite{Fiore:2008p7249}.

Informative, or to change terms to keep with other literatures, salient\footnote{
Not to be confused with the ``incentive salience'' discussed below
}, stimuli have been observed to have rewarding-like effects in people as well, though supporting data is limited to fMRI experiments where striatal BOLD changes are thought to, some degree, reflect phasic DA \cite{Schonberg:2009p6669,Surmeier:2007p4435} though this has recently come under question in transgeneic rat models \cite{Mishra:2011p9095}. Striatal BOLD increases have been observed in response to infrequently presented flashing images \cite{Zink:2003p5107}, and unexpected alarming tones (e.g. a siren replacing a constant 60Hz tone) \cite{Zink:2006p7210}.  Activity in the ventral striatum appeared in these tasks due to the stimuli alone, while dorsal activity was seen only when the stimuli had behavioral relevance.  A control task suggested this increase was due not to the additional motor demands of the response, but was, the authors argued, due to the increased saliency of the active versus passive condition.  That is the context of behavioral response made the reward more salient.  A similar dorsal to ventral division has been reported when comparing passive reward receipt to reward receipt requiring a response \cite{ODoherty:2006p2875} though these were attributed to directly to the need for an instrumental response and not (necessarily) contextual salience.  Additionally like reward expectations, salience activity scaled with intensity \cite{Zink:2006p7210}.

Novel (but not necessarily salient) stimuli also elicit reward prediction-like dopamenergic firing in monkeys \cite{Blatter:2006p6372} and in people \cite{Bunzeck:2006p5319}. Indeed, reward and novelty appear interchangeable.  \citeNP{GuitartMasip:2010p7227}, showed that when novel images proceed rewarding outcomes enhanced ventral striatal activity compared to reward alone.  Rewards proceeding a visual stimulus or word-pair leads to enhanced memory for that pair \cite{Lisman:2005p5455}.  Building on that \citeNP{Wittmann:2007p3328}, showed enhanced recognition of natural scenes, compared to control, when images were proceeded by novel images.  This effect was reproduced using high-resolution imaging of the VTA/SNc, with that area demonstrating a marked reward prediction signal during the task.  This effect was extended to the anticipation of novel images, similar to reward anticipation studies of striatal function \cite{Knutson:2001p5234}.  Novelty driven exploitation/exploration decision making relies on the striatum as well \cite{Wittmann:2008p541}.

Task completion, imagined rewards, neutrally valued informative cues, behaviorally salient but non-rewarding, as well as novel events have all been shown to act as reinforcers and stimulate the dopamenergic midbrain into phasic firing.  None of these, individually or as a group can be explained parsimoniously as secondary rewards\footnote{
Nor as primary rewards, but this is a definitional problem.
}.  They were never \emph{statistically regularly} paired with a primary reward.

% --
\subsection{Generally Generalizable.}
\label{sub:gen}
% TODO Reread the below

In a recent review \citeNP{Wimmer:2012p8836}, made the argument that the value of a given stimulus must be generalizable.  There are, they argued anecdotally, far too many possible states in the world to explore each individually and that you rarely if ever encounter the same exact stimulus twice.  So the brain must infer.  In essence, they make an argument that categories of stimuli are necessary.  I'll now turn that on its head.  There are too many possible outcomes for rewards to be just events.  You can never see all the outcomes.  And you can't predict exactly what will happen, mostly anyway.  The world is full of too many outcomes to see the same one many times.   So rewards too must generalize.

\subsubsection{Exact can't cut it}
% ...Generalization studies
% ITI, novelty (an inverse category), fictive, optimism, others?

\citeNP{Roesch:2007p2519}, also reported optimistic responses, that is when novel stimuli were presented in the context of a familiar task with the same action options previously available, the dopamine spike that resulted were significantly correlated with the best known past action.  No simple modification to the TD equation allows for the production of a prediction error to a single \emph{specific} past outcome.

 Even the relatively simple cases of temporal discounting of rewards and the assessment of their uncertainty likely requires cognitive intervention, which is reflected in several reports of complex, multi-valued, reward-related signals in both dorsal and ventral-medial prefrontal cortices \cite{Tobler:2009p8302,Wallis:2010p8303,Kim:2009p8304,Seymour:2008p6518}


\cite{Nakamura:2006p9093} pigeon decrimanate categories of male and female birds - read it.

 \citeNP{Zink:2004p5108}, showed that reward sans salience showd no striatal BOLD activity; salience assessment is intrinsically contextual and .

% --
\section{Goals and Methods}
\label{sec:goals}
The primary goal of this the experiments I review below was to establish whether rewards can be represented as categories in the brain, and to examine what impact such representations have on reinforcement learning.  

\newpage
\bibliography {bibmin}
%%%%%%%%%%%%%
\end{document}
%%%%%%%%%%%%%
