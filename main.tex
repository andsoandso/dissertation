\documentclass[doc,12pt]{apa}        % use: 'man' for submission type; 'jou' for
                                % journal type, and 'doc' for typical latex
                                % but with figures inline with text
\usepackage{geometry} 
%\geometry{a4paper} 
\usepackage[parfill]{parskip}   % paragraphs delimited by an empty line

\usepackage{graphicx} 
\usepackage{amssymb}            % no idea what this does...
\usepackage{epstopdf}           % no idea what this does...
%\usepackage{gensymb}            % no idea what this does...

\usepackage{setspace}

\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png} \setcounter{secnumdepth}{0}  % no idea what this does...

\usepackage{apacite}
%%%%%%%%% END HEADER %%%%%%%%%

\title{Categories of rewards.} 
\author{Erik J. Peterson} \affiliation{Dept. of Psychology \\ Colorado State University \\ Fort Collins, CO} 

%%%%%%%%%%%%%%%%
\begin{document} 
%%%%%%%%%%%%%%%%
\maketitle
%\doublespacing

\section{Introduction} % (fold)
\label{sec:introduction}
Birds will peck repeatedly, as mice will push levers, monkeys will hit buttons, and men will buy flowers, if each of these actions is followed by a primary reward -- food, drink or sex.  Buttons, levers and flowers have no value alone, so reinforcement theory goes, it is only by the \emph{statistically regular pairing} with primary rewards that value is transfered \cite{Rescorla:1988p8743}.  This is the classical view, and it has, in general, held up many years now \cite{iversen:2007aa}.  In fact, the neural basis reinforcement learning currently receives substantial attention, with notable progress is being made \cite{Glimcher:2011p8464, Montague:2006mz}.  However, reinforcement learning theories can't at current account for two recent key findings in the neural correlates of human learning. (1) the neural correlates of reward can appear by cognition alone \cite{Hayden:2009p6545, Lohrenz:2007p7240, Tricomi:2008p6663, Jimura:2010p8305}. (2) Value can be transfered by inference, no pairing is needed \cite{BrombergMartin:2010p7223, Hampton:2006p2577}. Using a mixture of fMRI and computational modeling, I examine possible mechanisms of both of these aspects by modeling the reward representations as a kind of category; categories, i.e. abstract generalizable represenations, are exclusively cognitive and require inference.

This introduction has six parts.  First I discuss classical rewards and their neural correlates and what is known of their anatomical bases, including criticsims of the reward prediction account of dopamine function that underlies this work.  Second I make a case for cognitive rewards, discussing as an example novelty, then moving onto other examples, and finally arguing for the necessity of generalizable reward representations.  Third is a discussion of prior studies of value generalization in pigeons and other non-human animals as well as in humans, though the literature on the latter is sparse.  Fourth is a brief introduction to formal models of categorization, which leads into the fifth and final section, the specific goals and methods of this work.

\subsection{Classics, Expectations and Tissues} % (fold)
\label{sub:c_and_e}
\subsubsection{A pleasurable start} % (fold)
Classically rewards and reinforcers have been linked to (or simply were) food \cite{ODoherty:2006p2875}, pain \cite{Becerra:2011p7581,schultz:2007aa} and sex though for, err, logistical reasons this is less often used in the laboratory, especially in human subjects; They're certainly potent, being used for over 50 successful years to study learning in animal models \cite{iversen:2007aa} and people \cite{Kim:2010p7248,Montague:2006mz}.  In the 1950's the first clue how food and water cause reinforcement arose in the electrical self-stimulation studies of \citeNP{OLDS:1954p8747}, for a classic review see \citeNP{Crow:1972p8748}.  Olds and colleges observed that when electrodes, which could be activated by a self-determined button press, were placed in the midbrain and in limbic areas animals would vigorously and repeated self-stimulate.  By the 1970s data from pharmacological studies of rats, electrochemical recordings, knowledge of the signalling mechanisms of dopamine receptors, as well as neoleptic drug actions in Schizophrenic patients, along with Old's shocking work, lead to the first major theoretical proposal for dopamine's role - a signal for pleasure (i.e. the anhedonia hypothesis, \citeNP{Wise:1978p8771}).  However within 10 years it became clear that dopamine's role extend beyond signaling primary rewards. Activity was seen folloing secondary rewards, novelty, salience, and others \cite{Spanagel:1999p8515, Salamone:2005p8774, BrombergMartin:2010p8834}.  And, more importantly, dopamine depleted animals continued enjoy rewards, i.e. they still developed taste preferences, enhanced response vigor \cite{Cannon:2003p8513} and continued to respond to opiates \cite{Hnasko:2005p8832}\footnote{Though both these criticisms are constrained as dopamine deficient mice, sans caffeine, are 
extraordinarily lethargic. To pep them up, caffeine is administered.  And caffeine, through a cascade driven by adenosine A2A and cannabinoid CB1 receptors in ventral striatum has biochemical effects similar to dopamine \cite{Lazarus:2011p8137, Rossi:2010p7252}}.}.  However in 1994 there was a surprise that would eventually explain many of anhedonia's deficits.  Schultz \emph{et al} reported that dopamengeric firing depended on how expected a reward was \cite{Mirenowicz:1994p7185}, which blossomed into the reward prediction error theory under review here.

\subsubsection{Expectations Matter} % (fold)
\label{sub:the_classics}
As I said \citeNP{Mirenowicz:1994p7185}, was the first paper to demonstrate that phasic activity in the midbrain was dependent not just on the hedonic value of the reward, but was qualitatively related to both the value of the reward and how expected that reward was.  \citeNP{Hollerman:1998qy} more fully explored this observation, showing that unexpected rewards lead to increases in the firing rate, stronly expected rewards elicited no response, while expected rewards that failed to arrive lead to a pause.  \citeNP{Roesch:2007p2519} found the same patterns in rats. \citeNP{ODoherty:2003p6329}, found them too in fMRI studies of humans.  Meanwhile \citeNP{Fiorillo:2003p6375}, along with \citeNP{Bayer:2005ul}, quantified the relationship between the unexpectedness of the reward and phasic activity via a reward prediction error term derived from a reinforcement learning model fit to each animal's behavior. The reward prediction error from the model strongly correlated with the dopamine response - both its increases and decreases.  A second key similarity between reinforcement learning models and the dopamenergic response was transfer.  If a cue reliable predicts a reward the reinforcement learning equations require value to transfer from the primary reward to the cue, mimicking Pavlovian conditioning (which was the initial goal of these models).  This same behavior was observed in the dopamine response \cite{Roesch:2007p2519, McClure:2003p3346}. Furthermore RL models are statistically predictive of non-human animal's choice behaviors \cite{Hampton:2007p2983}. Dopamenergic firing patterns are consistent with optimal formal learning theories \cite{Waelti:2001p6523} and has been shown to mediate cortical-striatal coupling \cite{denOuden:2010p7203}.  Single doses of dopamine antagonists and agonists have also demonstrated a casual relationship between dopamine levels and learning rate \cite{Pizzagalli:2008p6521, Diaconescu:2010p7631}, which is braodly though not exlcusivily consistent with a reinforcement theory interpreation.  These findings are the backbone of the reward prediction theory, which has since been extended substantially.  

Based on novel findings about novelty \cite{Bunzeck:2006p5319, Blatter:2006p6372, GuitartMasip:2010p7244} the reward prediction hypothesis has been extended to incorporate activity observed following presentation of novel stimuli \cite{Kakade:2002p6414} as well as to explain reward anticipatory firing via an average reward prediction error \cite{Knutson:2007p1687}. Another variation allowed for the observation of simultaneous neural implementation of model-free and model-based reinforcement learning \cite{Smith:2006p7627, Daw:2011p7995}.   Alternative but reconcilable accounts have also been offered that allow for dissociation of first and second order conditioning as well as pavlovian to instrumental transfer \cite{OReilly:2007p827}. The reward prediction hypothesis has also been incorporated into theoretical accounts of addiction \cite{Redish:2004p2531} and to predict the saliance/information of upcoming stimuli \cite{Behrens:2007p8839}, among others.   

Additionally there are several findings which are, as yet, are unaccounted for theoretically.  The dopamine response adaptively scales with past reward magnitudes, which appeared similar to reward value divided by the cumulative variance \cite{Tobler:2005p6373}.  \item \citeNP{Matsumoto:2009p7219}, reported a very braod set of dopamenergic firing patterns. In the classical (bivalent) view dopamine neurons should fire more for rewards, negatively to reward omission, and positively to the omission of punishment, and negatively to aversive events \cite{Kim:2006p1063}. Matsumoto:2009p7219}, found that some neurons respond as expected buy many others responded positively to aversive stimuli (an air puff on the eye) increasing as the punishment grew larger (than expected), and decreasing as it grew smaller.  Unexpectedly many neurons responded positively to \emph{both} appetitive and aversive conditions when expectations were exceeded and negatively when they predictions were optimistic.  \citeNP{Matsumoto:2009p7219}, further reported that neurons that responded positively to aversive events were clustered in the substantia nigra pars compacta while appetitivily tuned neurons appeared more often in the ventral tagmental area.  Likewise \citeNP{Smith:2011p8133} demonstrated \emph{simultanous} tunings to both reward value, reward expectancy, saliance as well as to novelty.  If the dopamenergic response is universally this complex, the bivalent view needs substantial refinement, as do perhaps our analysis techniques; many different yet related models may \emph{correctly} fit the same data, an issue whic hhas recieved some prior attention outside of neuroimaging\footnote{for my take on solving this problem in fMRI, see the methods section.} \citeNP{Chamberlin:1965p8873}.

\subsubsection{Networked Plausibility}
\label{sub:net_plaus}
The dopamenergic firing patterns outline above orginate in the VTA/SNc is a small brainstem nucleus whose dopamine-releasing neurons project strongly to both the striatum and the hippocampus.  Electrophysiological recordings of VTA/SNc neurons show two firing modes -- tonic and phasic \cite{DawNW:2006p6343}.  The phasic mode is of interest here, at it relfect theoretically a reward predictions error signal. A reward predictions \emph{error} singal of course reuires predictions.  In this case predictions of future value.  Though how exactly such predictions are made is only partly understood.  Canditate regions for this calculation include the striatum, the limbic system (via the habenula) as well at the orbital frontal and ventral medial cortices.  I present each in turn, leaving the totality unitegrated, thus accurately reflecting the literature's state -- tempting and incomplete.

\subsubsection{And In The Striatum}
\label{sub:in_the_str}
The striatum is an input area of the basal ganglia, a brain region highly involved in categorization, logical inference, habit formation, working memory and feedback mediated S-R learning \cite{Frank:2001p1996,Jin:2010p7199,SchmitzerTorbert:2004p5410,Seger:2008p6401,Seger:2010p7189,Yin:2006p5080}.  In S-R learning, two of the five striatal subregions (the head of the caudate and the ventral striatum) process reward information \cite{Yin:2005p5101,Yin:2008p6347,Schonberg:2009p6669}.  These two are highly innervated by projections from the VTA/SNc, but only the ventral striatum correlates with the RPE signal \cite{Haruno:2006p3979,Seger:2010p7189}.  The remaining three regions (the body and tail of the caudate and the putamen) are involved with S-R pair formation, visual categorization and response selection, respectively \cite{Seger:2008p6401,Seger:2010p7189}.  Though these three also receive VTA/SNc projections and are sometimes sensitive to reward level \cite{BischoffGrethe:2009p4570}, the BOLD signal does not correlate with the RPE \cite{Seger:2010p7189}; dopamine's exact role in these areas is less clear.  Overall though, intact dopamine projections and complete striatal function is necessary for rapid S-R learning.
   
Administering dopamine antagonists to human and non-human animals adversely affects S-R learning \cite{Pizzagalli:2010p7205}, as does lesioning the VTA/SNc.  Complete lesions of the striatum also prevent S-R learning \cite{Packard:2002p5074}.  Administering dopamine agonists or the readily converted precursor L-DOPA leads to increases in response vigor and the ability of a Pavlovian-conditioned stimulus to bias unrelated instrumental responses (i.e. pavlovian instrumental transfer) \cite{Winterbauer:2007p6352}. Both pavlovian instrumental transfer and response vigor are, in part, facilitated by phasic dopamine increasing activity in the ventral striatum.  Unmedicated Parkinson's patients, who have low striatal dopamine levels, show marked decreases in S-R learning with rewarding outcomes when compared to patients on medication and healthy age and intellect matched controls \cite{Pizzagalli:2010p7205}.  These same patients show an enhanced capability to learn from negative feedback which suggests that decreases in dopamine convey negative outcome information \cite{Frank:2004p4709}.  Finally, there is a solid body of evidence suggesting that phasic dopamine alters the plasticity of neurons in the striatum which presumably facilitates stimulus-response learning \cite{Calabresi:2007p4284}.

\subsubsection{Limbic Integration}
\label{sub:limbic_inte}
VTA/SNc also receives input from the internal globus pallidus or GPi (a major output structure of the basal ganglia with widespread cortical connections), thalamus, and the central nucleus of the amygdala \cite{Botvinick:2008p6594}.  Recent work though has highlighted the habenula (a small nucleus posterior to the thalamus) as being especially important in generating RPE-like phasic activity.  The lateral habenula has reciprocal connections to the GPi and projections into the VTA/SNc. Based on this anatomy, it has been suggested that this nucleus serves a point of intersection between the striatum and the limbic system (e.g. the amygdala, hippocampus and the serotonergic dorsal raphe nucleus) \cite{Hikosaka:2008p4455}. The habenula acts to tonically inhibit or disinhibit dopamine release in VTA/SNc neurons.  As habenula activity decreases, burst firing in VTA/SNc results; as habenula firing increases VTA/SNc firing is temporarily paused.  Dual recordings of the habenula and GPi suggest they form a functional loop capable of calculating the value of S-R pairs \cite{BrombergMartin:2010p7221}.  Reversible chemical inhibition of the habenula also increases VTA/SNc phasic activity.  Lesions to the habenula also result in marked increases in dopamine levels in the dorsal and ventral striatum \cite{BrombergMartin:2010p7221}.  In summary, the GPi (withs its access to cortical inputs via the striatum) and habenula (with its capability for altering VTA/SNc activity) may form the physiological loop necessary to calculate the RPE; \citeNP{BrombergMartin:2010p7218}, hint that this loop can signal both initial value estimates and rewarding outcomes.

\subsubsection{Front and Center}
\label{sub:f_and_c}
Orbital frontal cortex has been repeatedly shown, both in neuroimaging \cite{ODoherty:2001p2423} and lesion \cite{Hornak:2004p6234} studies to encode the absolute value of rewarding or punishing outcomes, thus playing a pivtal role in the new field of neuroeconomics \cite{Glimcher:2005p863}. However orbital frontal areas are more than a simple value store.  It also plays a role in response and outcome recall \cite{Furuyashiki:2008p1631} and selection \cite{Rudebeck:2008p4712} as well as in motivation, pain and pleasure \cite{Atlas:2010p7566} and outcome anticipation \cite{Roesch:2007p7182} and outcome prediction \cite{Tanaka:2006fk} and causal attribution \cite{Tanaka:2008p3265}.  Additonally, orbital function is highly dependent on the basolateral amygdala \cite{ODoherty:2003p2616} offering a path for influencing reward prediction error calculation.

Estimating value is one part of a reward prediction but so is the expectation, that is the probablity a reward will occur.  

\subsubsection{Let's Get Critical.}
\label{sub:in_the_str}




% Then do the cog section...
% ...Generalization studies
\citeNP{Roesch:2007p2519}, also reported optimistic responses, that is when novel stimuli were presented in the context of a familiar task with the same action options previously available, the dopamine spike that resulted were significantly correlated with the best known past action.  No simple modification to the TD equation allows for the production of a prediction error to a single \emph{specific} past outcome.  It is possible though that this firing may have been driven by an alternative circuit, one with access to specific past events (for example the hippocampus which is known to both receive from and project to the VTA/SNc \cite{Lisman:2005p5455}). 

While \item \citeNP{BrombergMartin:2009p7220}, demonstrated dopamine firing about informative (but not directly reawrding) cues. 

\cite{Aron:2004p5459} -- ``purely cognitive feedback apparently engages the same regions as rewarding stimuli''
\newpage
\bibliography {bibmin}
%%%%%%%%%%%%%
\end{document}
%%%%%%%%%%%%%
