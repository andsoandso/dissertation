\documentclass[doc]{apa}        % use: 'man' for submission type; 'jou' for
                                % journal type, and 'doc' for typical latex
                                % but with figures inline with text
\usepackage{geometry} 
\geometry{a4paper} 
\usepackage[parfill]{parskip}   % paragraphs delimited by an empty line

\usepackage{graphicx} 
\usepackage{amssymb}            % no idea what this does...
\usepackage{epstopdf}           % no idea what this does...
% \usepackage{gensymb}            % no idea what this does...

\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png} \setcounter{secnumdepth}{0}  % no idea what this does...

\usepackage{apacite}
%%%%%%%%% END HEADER %%%%%%%%%

\title{Answers to the comprehensive exam.} 
\author{Erik J. Peterson} \affiliation{Dept. of Psychology \\ Colorado State University \\ Fort Collins, CO} 

%%%%%%%%%%%%%%%%
\begin{document} 
%%%%%%%%%%%%%%%%
\maketitle

\section{Introduction} % (fold)
\label{sec:introduction}

Before answering the questions, explaining my logic as to what poses a significant challenge to RPE theory seems appropriate.  In my view, the power of reward prediction error hypothesis lies not in its semantics (i.e. ``dopamine (DA) codes for the difference between predicted and received rewards'') but in its specific implementation.  That is, the RPE is perhaps one of the first supportable insights into actual the math the brain does. As for implementation, I refer to the two related forms that have been most commonly employed: Rescorla's delta rule and temporal difference learning, specifically TD(0) (\emph{Eq 2.}), but versions of TD with eligibility traces seem equally applicable.  Note that the delta rule has only been employed for two state systems (where there is only a stimulus and a outcome).  For two state systems, TD(0) reduces to the the delta rule.  TD(0) is as follows:

\begin{equation} RPE = r(t) + v(s,a,t) - v(s,a,t-1) 	\end{equation}
\begin{equation} v(s,a,t) \leftarrow v(s,a,t) + {\alpha}RPE  	\end{equation}	

Where $v(a,s,t)$ represents the value estimate of the current state ($s$) at the current time ($t$) for the last action ($a$), $v(s,a,t-1)$ is the value for the same state and action at the last time-step, $r(t)$ is the reward and $RPE$ is the difference between the previous and current expectations ($v(s,a,t) - v(s,a,t-1)$) plus that actual outcome ($r(t): \{1,0\}$).  Alpha ($\alpha$) is a learning constant, controlling the rate at which $v(s,a,t)$ increases or decreases.  

All reinforcement learning systems operate in a state-space. States are the pieces of information that make that particular situation unique and identifiable.  It is assumed that the continuous sensory input animals receive is transformed into discrete identifiable packets.  Though how this process proceeds in poorly understood.  Experimentalists pragmatically assume that each of the unique stimuli presented over the course of an experiment exist as states, that is discrete units, within the subject.  

Based on preference for mathematical not semantical confirmation, I assert that findings requiring only that the inputs of the model be changed to accommodate a new findings to be evidentially neutral (for example Item 2 in the \emph{Reward related yet inconsistent} subsection below or novelty bonuses discussed in Q3). In this same mode, alterations to a terms meaning are also neutral.  For example, recent findings suggest that phasic dopaminergic firing occurs in the midbrain (i.e. the substantia nigra pars compacta (SNc) and ventral tagmental are (VTA)) following goal completion -- when no explicit reward is present \cite{Tricomi:2008p6663}.  As such, $r(t)$, semantics may need to be expanded from the traditional definitions of primary and secondary reward to something more inclusive, perhaps to the point of introducing the idea of fictive rewards (\cite{Lohrenz:2007p7240}), rewards invented in the brain based on some internal (and unknown) criterion being met.  Empirical findings that require adding an additional step before or after RPE calculation are also neutral.  They alter the input/output of the algorithm and not the algorithm itself (see Item 1 in the \emph{Reward related yet inconsistent}).  

A caveat, I have dismissed input/output modifications as irrelevant to the truth or falsehood of the RPE hypothesis, but do wish emphasize that understanding the inputs, and how the outputs are modified, is likely crucial to understand the ultimate functional role phasic DA plays.  Additionally understanding how the brain creates the ``states'' necessary for any reinforcement learning algorithm to function is an important and (as far I am aware) unaddressed question.  

However findings that require direct modification or replacement of the algorithm present a challenge.  Two examples: the incentive salience model proposed by \citeNP{Aldridge:2009p7243}, which models DA firing as quantitative measure of wanting (for more detail see \emph{Q4}) or the PVLV approach presented in \citeNP{OReilly:2007p827}.  This latter captures other aspects of DA firing TD/delta models could not, specifically the transfer of pavlovian conditioning to instrumental responding that has been observed in the aptly names pavlovian instrumental transfer (PIT) task \cite{Corbit:2007p6349}.  However it is worth noting that, \citeNP{Aldridge:2009p7243}, employed TD(0) but with an added parameter ($\kappa$) to estimate the degree of ``wanting'' and that the PVLV model does capture all the aspects of TD/delta rule and so is similar in mathematical function, and even to some degree in form. 

Additionally, an implicit constraint on the truth of the RPE hypothesis lies in its casual capacity for mediating feedback driven trial and error learning. The greatest threat to the RPE hypothesis may arrive not from its inability to predict some crucial cellular or behavioral change but from findings that (1) it may not play a casual role in stimulus response learning (see Q4) or (2) does not have the necessary inputs to make the proposed calculation (Q2).
% section introduction (end)

\newpage
\section{Q1} % (fold)
\label{sec:q1}
\emph{Across the papers on your reading list are a number of different accounts of what DA neurons from the VTA/SN respond to and code for.   Provide an organized summary or taxonomy of what DA neurons respond to, and how (e.g., though differences in firing timing and/or changes in firing rate).  Which of these types of responses can be accounted for by RPE theory, and which cannot?  What do we know about the inputs to the VTA/SN, and how might the information from these inputs constrain what DA neurons can code for?}

Given the ranking of importance outlined above, many of non-reward related responses/tunings detailed below present no significant issue.  However, in the interest of rigor, I have classified all findings which do not fit the original value/reward coding view of the RPE as inconsistent, often accompanied by a discussion of why this inconsistency is minor or readily fixed.  


\subsection{Reward and prediction related (consistent with the RPE hypothesis)} % (fold)
\label{sub:consistent_with_the_rpe}

\begin{itemize}
\item \citeNP{Mirenowicz:1994p7185}, was the first paper to demonstrate that phasic activity in VTA/SNc was dependent not on the value of the reward, but was qualitatively related to both the value of the reward and how expected that reward was.  It, in essence birthed the RPE hypothesis.
\item \citeNP{Fiorillo:2003p6375}, along with \citeNP{Bayer:2005ul}, quantified the relationship between unexpectedness of the reward and phasic activity.  They showed the RPE from their models was strongly correlated to the DA response - both its increases and decreases.
\item Complementing previous work in monkey (all the studies listed above along with human fMRI, for example \citeNP{ODoherty:2003p6329}), \citeNP{Roesch:2007p2519}, found the characteristic patterns of the RPE hypothesis in rats\footnote{This was a technical feat as the VTA/SNc equivalent in rats is very small and difficult to record cells from}.  That is, a phasic increase with unexpected rewards, a depression when rewards were omitted, and back-propagation from reward delivery to stimulus as learning proceeded.  
\end{itemize}
% subsection consistent_with_the_rpe (end)


\subsection{Reward related yet inconsistent} % (fold)
\label{sub:incosistent_with_the_rpe_hypothesis}

\begin{itemize}
	\item Phasic DA was sown to adaptively scale with reward value.  DA firing magnitude was dependent on the range of values experienced.  This scaling appeared similar to reward value divided by the variance \cite{Tobler:2005p6373}.  This scaling is inconsistent with all versions of the RPE I have encountered, as least formally.  However, as the authors of the study noted, the RPE hypothesis could be modified, introducing a variance normalization step following RPE calculation.
\item \citeNP{Matsumoto:2009p7219}, found a much broader set of tunings that the canonical value-coding RPE.   In the value-coding view, DA neurons in VTA/SNc respond positively to rewards, negatively to reward omission, positively to the omission of punishment (\cite{Kim:2006p1063}), and negatively to aversive events.  \citeNP{Matsumoto:2009p7219}, found that some neurons responded as expected based on a value coding scheme, others responded positively to aversive stimuli (an air puff on the eye) increasing as the punishment was larger than expected, decreasing when it was smaller.  However, many neurons responded positively to \emph{both} appetitive and aversive conditions when expectations were exceeded and negatively when they predictions were optimistic.  If true, these findings end the value-coding interpretation.  Though this is not necessarily a significant issue.  The RPE can be semantically reformulated (i.e. the inputs coding must be altered but the underlying equations are unchanged) in order to adapt. Instead of coding aversive events as -1 or 0, code them as either -1 or 1, introducing two concurrently running predictions with opposing codes.  Although the advantage of such a scheme is unclear, it is supported empirically.  \citeNP{Matsumoto:2009p7219}, further reported that neurons that responded positively to aversive events were clustered in the SNc while appetitivily tuned neurons appeared more often in the VTA.  
\item While listed above as supportive, \citeNP{Roesch:2007p2519}, also reported that when novel stimuli were presented in the context of a familiar task with the same action options previously available, the DA spike that resulted (similar, perhaps equivalent to, the novelty response discussed below) was significantly correlated with the best hypothetical action given previous trials.  No simple modification to the TD equation allows for the production of a prediction error to a single \emph{specific} past outcome.  This firing may have been driven by an alternative circuit, one with access to specific past events (for example the hippocampus which is known to both receive from and project to the VTA/SNc \cite{Lisman:2005p5455}).  Further research is certainly required, to both verify and locate the origin of this optimistic firing.  If it can be determined that it originated from the SNc/VTA-cortico-strital loop, the RPE hypothesis would require substantial modification, in the bast case.  In all cases this suggests the RPE hypothesis is an incomplete account.
\item \citeNP{BrombergMartin:2009p7220}, demonstrated DA burst firing and inhibition to reward-level cues that, respectively, signaled large or small rewards.  No firing changes observed for cues that were randomly presented thus offering no information about the upcoming reward.  This, the authors concluded, implied that DA neurons also signal information about upcoming rewards.  Like the case of novelty (discussed below) the input into the RPE equations could be modified such that useful information is treated a rewarding (a 1 not a 0).  The authors did no computational modeling, so this correction is hypothetical.  It might not match well with the DA firing rates they observed.
\end{itemize}
% subsection incosistent_with_the_rpe_hypothesis (end)

\subsection{Novelty (inconsistent)} % (fold)
\label{sub:novelty}
First demonstrated by Schultz, 1997, DA bursting in response to novel visual stimuli has been consistently reported \cite{Blatter:2006p6372, Wittmann:2007p3328, GuitartMasip:2010p7227}.  It was proposed by \cite{Kakade:2002p6414} and tentatively shown \cite{GuitartMasip:2010p7227} that the RPE cam account for this activity if novel events are treated as rewarding.  For a more complete discussion of novelty see \emph{Q3}.
% subsection novelty (end)

\subsection{Salience (inconsistent)} % (fold)
\label{sub:salience}
There are, as far as I am aware, three distinct salience accounts of DA phasic activity. All three share one common element, they all suppose the DA acts in a capacity \emph{other} than stamping in S-R associations.  That is they suppose that DA is not necessary for S-R learning.  It serves some other role.

Note: All the studies in the first two subsections below have employed novelty (in the guise of saliency) to elicit changes in phasic DA.  I am aware of no work to date that has isolated saliency (defined as ``expected input that elicits a behavioral switch and/or leads to arousal'') from novelty (``unexpected sense information'').  Though the authors of the work below propose that saliency is the common element linking both novelty and reward related phasic DA pulses.  All studies of novelty include and rely on measures of behavioral switching or arousal to support their findings of novelty (see \emph{Q3} for further discussion).  The distinction of novelty from salience then appears to rely not empirical support or experimental methods but on interpretation.  Novelty accounts are often folded into the RPE account (again see Q3), while saliency accounts suppose DA acts to ``(re)direct attentional resources''.  To further muddy the waters, a theoretical account of working memory set-switching (a likely act of attentional redistribution) has been created under the RPE hypothesis \cite{OReilly:2006p2615,Ponzi:2008p751}.  However as proponents of the first two salience accounts below insist on a distinction from novelty, I discuss their findings separately.

\emph{Sense salience.}
The first salience is characterized by DA firing following unexpected sensory input, referred to as ``sense salience''.  Supporting data is limited to indirect fMRI experiments (focused on striatal BOLD changes which are thought to, some degree, reflect phasic DA \cite{Schonberg:2009p6669,Surmeier:2007p4435} and to work with visual system in anesthetized rats.  Though the latter is covered in detail in \emph{Q2}, it is this work that allows for the distinction between the salience discussed in this and the next section, so a quick discussion is warranted.   By examining the visual inputs into the VTA/SNc \citeNP{Dommett:2005p7263}, found the superior colliculus -- a midbrain region that receives only basic retinotopic input (e.g. luminance changes but not contrast or edge information) -- not the major cortical visual areas projected \emph{directly} into the VTA/SNc.  Given the rapid response and limited window of phasic DA firing following rewards delivery \emph{direct} input is assumed to be required.  Thus, \citeNP{Dommett:2005p7263}, argued that phasic DA could not represent a RPE as the limited visual input could not adequately discriminate reward information.  They proposed instead that phasic DA represents a base attentional signal pointing directing attention to sudden changes in the visual stream (see \emph{Q2} for further information -- their case is not conclusive).

The fMRI studies of sense salience have shown striatal BOLD increases in response to infrequently presented rapidly flashing images \cite{Zink:2003p5107}, and unexpected alarming tones (e.g. a siren replacing a constant 60Hz tone) \cite{Zink:2006p7210}.  Activity in the ventral striatum appeared in these tasks due to the stimuli alone, while dorsal activity was seen only when the stimuli had behavioral relevance.  A similar division has been reported when comparing passive reward receipt to reward receipt requiring a response \cite{ODoherty:2006p2875}.  

\emph{Contextual salience.}
In a fMRI contrast of passively received rewards (rewards randomly awarded by the participant) to rewards requiring a correct response, \cite{Zink:2004p5108}, found increased BOLD ventral and dorsal striatal activity.  A control task suggested this increase was due not to the additional motor demands of the response, but was, the authors argued, due to the increased saliency of the active versus passive condition.  That is the context of behavioral response made the reward more salient. Unlike Zink \emph{et al's} previous work, which was compatible with the idea the SNC/VTA does not receive sufficiently complex visual input to identify rewards, this task like many of those supporting the RPE requires detailed visual processing.  Similar behavioral requirements have been shown lead to increased activity in the anterior dorsal caudate when a subject learns a task compared to when a subject was instructed as to the correct answer (i.e. observational learning) \cite{Cincotta:2007p6672}.  Graded contextual novelty-related effects have also been reported \cite{GuitartMasip:2010p7227}.  Finally passive viewing of stimuli that predicted an aversive outcome (a mild electric shock -- which must be quite salient) showed a strong correlation with a RPE model \cite{seymour:2004aa}.  This implies that even if signaling salience is phasic DA's role, it does so in a pattern highly similar to the numerical outputs of the RPE.  For this reason, along with its similarity to nearly all novelty experiments (see \emph{Q3}), I suggest that \emph{contextual salience} may fit well under the RPE hypothesis with a semantic realignment, from attentional switching to the novelty adapted RPE.

\emph{Incentive salience or ``wanting'' (consistence uncertain).}
Using data from both human experiments on addicts, addiction mouse models, along with mice lacking any cortical DA (via a gene knock-out of tyrosine hydroxylase), \citeNP{Berridge:2007p7235}, made a two-part inferential argument. He argued that (1) DA is not necessary nor sufficient for S-R learning and (2) experiments with addicts show they desire, or want, their substance of addiction, but do not like it better than controls. Therefore, he argues, phasic DA signals how much an animal wants an outcome.  This wanting is termed incentive salience.  This view is a serious challenge to the RPE hypothesis.  However other RPE-consistent explanations have been offered to explain dopamine's role in wanting, for a detailed discussion see \emph{Q4}.
% subsection salience (end)

\subsection{Sequence initiation and termination (inconsistent)} % (fold)
\label{sub:goal}
Employing a task where rodents learned that 8 freely selected and timed lever presses lead to a reward, \citeNP{Jin:2010p7199}, showed phasic DA firing rate increases proceeded each lever press.  However as the animal learned to perform the 8 presses as a series -- as indicated by decreasing inter-press intervals in the 8 press set -- larger DA rate increases were observed at the start and end of a sequence.  In a control experiment with two reward sizes (that switched randomly by trial) they observed a global increase in firing rate for large versus small rewards.  However the relative (when compared to inter-sequence levels) start/stop activity was the same as the single reward condition. If we assume that each action in the sequence was a ``state'', the RPE hypothesis posits that the DA bursting for start versus stop signals should shift as value back-propagates from reward to button press.  While this should have been observed in the initial (1 reward level) experiments, it should have been accentuated when the two reward levels were compared.  Additionally, small rewards were still positively coded.  According the RPE hypothesis, small rewards -- being less than the average reward -- should have lead to a depression in VTA/SNc activity.  Both these latter findings can't be easily incorporated into the RPE hypothesis.  Unless, that is, the state-space the rats employed was markedly different than the simple chain posited above.  Though very speculative, this very well may be the case as virtually nothing is known about how animals assemble their state-spaces.  It is also worth noting that this experimental design featured no visual salient stimuli, making these results incompatible with that hypothesis as well.
% subsection goal (end)

	
\subsection{Anatomical input into the SNc/VTA} % (fold)
\label{sub:anatomical_input_into_the_snc_vta}

The SNc receives input from the lateral habenula, the internal globus pallidus (GPi) (a major output structure of the basal ganglia) and the central nucleus of the amygdala\cite{Botvinick:2008p6594}.  The VTA receives similar inputs to the SNc but receives information from the hippocampus (which also projects back to the VTA/SNc, forming the novelty detection loop addressed in \emph{Q3}) and the ventral striatum \cite{Joel:2002p6593}.  

Recent work studying the lateral habenula, a small nucleus above the thalamus, is particularly relevant.  This region, with reciprocal connections to the GPi, and projections into the VTA/SNc has been suggested to serve a point of intersection between the striatum and the limbic system (e.g. the dorsal raphe and the lateral preoptic area) \cite{Hikosaka:2008p4455}.  The habenula tonically inhibits dopamine release in SNc/VTA neurons.  Lesions to the habenula results in marked increases in DA levels in the dorsal and ventral striatum.  Chemical inhibition of the habenula decreases VTA/SNc phasic activity \cite{BrombergMartin:2010p7221}. Dual recordings of the habenula and VTA/SNc show an inverse relation between these two region.  When habenula activity decreases, burst firing in VTA/SNc results.  The reverse case was also observed -- as habenula firing increased a ``pause'' was observed in in the SNc/VTA.  Dual recordings of the habenula and GPi show they form a functional loop capable of inferring the value of visual stimuli \cite{BrombergMartin:2010p7221}.  In total then the GPi (withs its access to cortical inputs via the striatum) and habenula (with its capability for altering SNc/VTA activity) may form the physiological loop necessary to calculate the RPE. Though the precise origin of the terms of Eq 1. and 2. remains unclear, \citeNP{BrombergMartin:2010p7218}, hint that this loop can signal both initial value estimates ($v(s,a,t)$) and rewarding outcomes ($r(t)$).

Still there are remaining functional-anatomical questions.  Particularly, the ongoing debate about the degree and type of sensory input the SNc/VTA can receive, with some initial data suggesting that visual input is insufficient to allow for reward information to be extracted from complex scenes \citeNP{Dommett:2005p7263}.  For details, see \emph{Q2}.
% subsection anatomical_input_into_the_snc_vta (end)
% section q1 (end)

\newpage
\section{Q2} % (fold)
\label{sec:q2}
\emph{Which one of the theories presented in the papers in the “Contradictory to the TD hypothesis” of “Unaccounted for by current theory” sections do you consider to be the most serious challenge to the RPE theory?  Can RPE be saved from this challenge?  If so, how would RPE need to change to accommodate the findings that are cited in support of the challenging theory?  (Note: Since incentive salience theory is discussed in question 4, please choose a different theory for this question).}

Though the argument for incentive salience (see \emph{Q4}) is certainly a potent challenge to the RPE hypothesis, the recent work by \citeNP{Dommett:2005p7263}, is \emph{potentially} deadly.  Dommet \emph{et al} disinhibited neurons in the brains of anesthetized rats in both the superior colliculus (SC) and visual cortex via a GABA antagonist (which temporarily restores neural activity in the normal unresponsive neurons of an anesthetized animal) followed by exposing the animals opened eyes to an extend set of 2 Hz light pulses while recording DA cells in the SNc/VTA as well as neurons in both visual cortex and the SC.  Disinhibition of the visual cortex lead to no changes in DA firing.  While SC disinhibited resulted in about half the recorded cells in the VAT/SNc to display phasic firing similar in character\footnote{A 100 ms wide burst in activity 100 ms after the initial stimulus.} to that typically observed in response to novel \cite{Axmacher:2010p7226} or an unexpected rewarding event \cite{Mirenowicz:1994p7185}.  Another third of the DA cells displayed a pause in activity, similar to that observed when an expected reward fails to arrive \cite{Mirenowicz:1994p7185}.  The remaining cells responded first positively then negatively.  From this the authors concluded that the SC and not the visual cortex is an effective activator of VTA/SNC neurons.  It should be noted that their disinhibition experiments in visual cortex were very limited (N=4 cells compared to 30 for the SC experiments).  Nor did they assess the extent of disinhibition in visual cortex.

The issue (as far as the RPE hypothesis is concerned) is that the though the SC is retinotopically mapped it responds to very limited range of visual stimuli -- appearance, disappearance, or movement of objects as well as luminance changes.  It does not respond to contrast, velocity, wavelength or the geometrical configuration of stimuli \cite{Dommett:2005p7263}. That is the SC couldn't realistically extract reward information from the visual stimuli used in nearly all the studies of reward to date.  Instead, Dommett \emph{et al} argue that all of the many reward studies ``can be solved based on luminance changes and/or of the position of specific reward-related visual stimuli''.  In other words, the expectancy-of-reward related changes in phasic DA, as well as the theory of contextual salience, and really any theory of DA function, but sensory salience, is an artifact of the task design!  

Fully rebutting the interpretation of \citeNP{Dommett:2005p7263},  requires the discovery of an alternative or complementary direct visual path (of which the habenula-GPi-SNc/VTA loop is a possibility, see \emph{Q1})) certain known aspects of DA phasic activity can't be easily described in terms of sensory salience.  No reports of SC driven phasic DA have shown back-propagation, that is the transfer of phasic DA firing from reward to stimulus, a key feature of both the RPE hypothesis and phasic DA activity \cite{ODoherty:2003p6329,seymour:2004aa}.  Additionally, Dommett \emph{et al's} DA recordings fail to exhibit attenuation in DA firing as the stimulus was repeated.  Attenuation with stimulus repetition is another established property of phasic DA neurons \cite{Mirenowicz:1994p7185}.  Though the authors attribute the lack of attenuation to their disinhibition protocol\footnote{``Given that such disinhibition can block both the behavioral and electrophysiological signs of habituation it is relevant that both measures of DA activation in the present study also showed consistent responses to predictable stimuli over hundreds of consecutive trials''}, they also cannot account for phasic DA firing decreases following the omission of an expected reward. Additionally 2/3 of their recorded neurons display either elevations \emph{or} decreases in activity, recordings in intact behaving animals show consistently both properties \cite{Bayer:2007p862}.

While not supportive of the RPE hypothesis \citeNP{Jin:2010p7199}, recordings are inconsistent with a sensory salience account.  Observations of increased DA spiking at the beginning and end of a sequence (see Q1); the visual input is identical when initiating a sequence versus the second or third or fourth lever press.  Also, visual input in this task when repeatedly selecting the large versus small reward was identical yet large rewards lead to overall increases in firing rate. 

Additional sensory salience inconsistent results include anticipation of novelty tasks.  FMRI imaging suggested\footnote{BOLD signal changes are thought to be driven by pre-synaptic activity \cite{Attwell:2002p2730}.  It is therefore possible that the sustained BOLD activity was due to sustained input that did not result in sustained phasic output.} \emph{sustained} BOLD increases in the VTA/SNc for a stimulus predicting the arrival of a novel image \cite{Wittmann:2007p3328}.  Sustained DA phasic activity is not predicted by the sensory salience account.  Given DAs proposed role in a sensory saliency account is to rapidly reorient attentional resources, anticipatory activity seems especially difficult to explain.  At current though there is no RPE consistent account of novelty anticipation. There is a viable model for reward anticipation \cite{Knutson:2007p1687} that could presumably function for novelty as well.  Indirect fMRI studies of striatal activity (of which phasic DA is thought contribute to overall activity \cite{Schonberg:2009p6669,Surmeier:2007p4435}) have shown BOLD increases for contextual novelty and salience  \cite{Zink:2004p5108,GuitartMasip:2010p7227}, as well as for active versus observed learning \cite{Cincotta:2007p6672}; None of these tasks are likely to be learned via positional or luminance changes.

The most direct challenge to the interpretation of \citeNP{Dommett:2005p7263}, is a recent paper that examined DA activity using complex visual stimuli to signal rewarding outcomes.  \citeNP{Nomoto:2010p7209}, employed a set of randomly generated yet coherently moving dot patterns.  These patterns would had either an overall leftward or rightward drift. In order to receive either a large of small juice reward, the animal had to indicate movement direction of two sets of these dots via saccade.  Due to the loose coherence of the moving dots, this task should not have solvable based on simple positional or luminance changes. Using this paradigm, Nomoto \emph{et al} reported DA neurons showed phasic reward-predictive response highly consistent with the RPE.  They further reported that this RPE-like activity changed the longer the monkeys examined the patterns.  Phasic activity became ever more ``refined'', as if DA was reflecting an ongoing RPE calculation.  The time-course of DA changes matched changes in neural activity associated with visual discrimination.

If the SC mediated sensory salience hypothesis were to somehow account for more complex DA firing patterns (back-propagation, suppressed firing at reward omission, etc) and could explain the data from Nomoto \emph{et al}, it is still the case that sensory salience related activity may be well described by the temporal difference learning equations \cite{ODoherty:2003p6329}.  That is the RPE (under my constraint of DA causing S-R learning) would be false but the math could still be right.
% section q2 (end)

\newpage
\section{Q3} % (fold)
\label{sec:q3}
\emph{“Novelty bonuses” are a component of some recent reinforcement learning models.    How are these typically defined and implemented computationally?  To what degree are they an accurate model of novelty related DA neuron firing and novelty related activity in the striatum?    How is novelty similar to and different from related concepts like salience, surprise, or expectation violation?   Can novelty and RPE be accounted for by the same theory? }

Novelty bonus: treating a new stimuli or context or feature as something to be sought, something intrinsically rewarding, was introduced by \citeNP{Sutton:1990p7239}, as means to sub-optimally solve the exploration/exploitation trade off - there is no optimal solution \cite{Dayan:1996p7238}.  That is once an agent has useful\footnote{In the context of reinforcement learning useful is synonymous with ``tending to lead to reward or away from punishment'' } knowledge an action's outcome, selecting this option will lead to consistent results.  As long as the world is unchanged, this outcome can be repeatedly realized.  However without further exploration the agent cannot know whether this is the ideal action.  There may be better rewards (or less punishment) elsewhere.  Thus in order to ensure maximization of the total rewards received through time (this being the underlying goal of all reinforcement learning techniques and, we assume, all animals) the agent must also explore alternatives.  However constantly exploring may be costly, many missed known good opportunities.  

Before continuing it is worth noting that novelty bonus are one of many ways to solve the exploration/exploitation trade-off.  In the canonical treatise of reinforcement learning \citeNP{Sutton:1998fk}, offered several alternatives.  No one solution is superior.  With that in mind the concept of the novelty bonus appears similar to the phasic firing increases changes observed in the DA afferents from SNc/VTA.  And (as reviewed below), there is some evidence for a casual relation between DA spikes and exploration.  Still. However well novelty bonuses may fit conceptually with the idea of a ``prediction error'', in that novel events are unexpected violations of the expectation of that nothing will happen, there are a good number of equal theoretical alternatives.  There is some tentative evidence that the ACC plays a part in mediating the exploration/exploration trade-off \cite{Quilodran:2008p2645}.

The mathematics and proposed mechanics of operation of the novelty bonus are as follows: 

\begin{equation} r(t) \rightarrow r(t) + n(u(t),T) 	\end{equation}		\begin{equation} RPE = r(t)+ n(u(t),t) + v(t) - v(t-1) 	\end{equation}

Using the equations presented in \citeNP{Kakade:2002p6414}\footnote{Full disclosure: \citeNP{Kakade:2002p6414}, along with the proposed maths described in this section, offered a functionally similar alternative -- ``shaping bonuses''.  The two approaches do make distinct predictions about suppression of phasic DA firing as learning proceeds.  Novelty bonuses will show a slight depression in DA activity right before the RPE reaches zero, while shaping bonuses show an initially strong DA suppression following a phasic burst.  This depression decreases with learning. A literature search suggested that no sufficiently detailed DA recordings exist to allow one to select one theory over the other.   The novelty bonus approach, in my view, was conceptually simpler and so was selected for discussion.}, novelty bonuses are expressed transforming the reward ($r(t)$) via the addition of a term representing the degree of novelty $n(u(t),t)$ at the current state ($u(t)$, \emph{Eq. 3}).  However $r(t)$ and $n(u(t),t)$ while distinct entities as far the math is concerned, they generally will take on the same values, either 1 (present) or 0 (absent).  With this formulation the equation for the RPE becomes \emph{Eq 4}.  Just like when reward is repeatedly experienced, continued exposure to novel stimuli will lead the RPE to zero.  The rate of decreases is dependent on the learning rate ($\alpha$, see \emph{Eq 1.}).  As the novelty bonus is calculated following initial exposure, behavioral effect can be expected only on the second exposure, that is assuming each exposure exists in a single state.  However if we assume that the RPE is continually updated, tracking continued visual inspection (consistent with the experimental work of \citeNP{Nomoto:2010p7209}) behavioral consequences could appear quite rapidly.  As in the case of reward, behavioral effects though would be mediated via the updated $v(t)$, the value of proposed or considered actions.  It is not clear in \citeNP{Kakade:2002p6414} account the specificity of  the novelty adapted RPE.  Would it apply to all ``online'' (that is currently considered actions) or be somehow more limited to a previous selected adaptive set.  For example, would novelty lead to enhanced grooming behavior if the animal had just recently engaged in that activity, or would it be isolated to (the more adaptive) ``approach behavior'' often quantified when rodents response to novelty is studied \cite{Hazy:2010p7217}.  While open theoretical questions, their answers need to necessarily alter the RPE.  Instead solutions could be found via state-space selection (via hierarchical reinforcement learning \cite{Botvinick:2008p6594}) and/or working memory updating \cite{OReilly:2006p2615}. 

Comparatively less experimental effort has been directed towards understanding the putative novelty response of DA neurons in the SNc/VTA.  The effect has been consistently reported \cite{Reed:1996p7250,Blatter:2006p6372}.  The exact dynamics -- how it fast it decays with repeated exposure, whether its omission leads to a pause in DA activity similar t reward, or whether it back-propagates to an informative cue -- of the novelty signal are mysterious.  There is however some compelling initial data suggesting the RPE hypothesis can be expanded to account for novelty.  By uniting reward and novelty in the same model, they should be able to directly interact.  In an fMRI experiment \citeNP{GuitartMasip:2010p7227}, showed precisely that.  They reported that novel images which proceeded rewarding outcomes lead to enhanced ventral striatal activity compared to reward alone.  Additionally, if reward and should be able to act in each others stead.  It has been previously shown that reward proceeding a visual stimulus or word-pair leads to enhanced memory for that pair \cite{Lisman:2005p5455}.  Building on that \citeNP{Wittmann:2007p3328}, showed enhanced recognition of natural scenes, compared to control, when images were proceeded by novel images.  This effect was extended to the simple anticipation of novel images, similar to reward anticipation studies of striatal function \cite{Knutson:2001p5234}.

The latter study though raises an important issue when considering novelty.  The hippocampus, not the striatum which is likely incapable of rapid recall of past visual events, is thought to play a key role in the DA novelty signal \cite{Lisman:2005p5455}.  The hippocampus and the SNc/VTA form a functional loop that begins with the hippocampus detecting novelty, and passing the signal through the subiculum, to ventral striatum, and ventral pallidum to the VTA/SNc; It is unclear whether the resulting RPE signal is calculated in SNc/VTA or in the hippocampus.  This raises the possibility that the hippocampus is bootstrapping onto phasic DA (and its role in adjusting expectations via the RPE) while carrying out some different calculation.  Addressing this question would require repeating many of the same experiments that support the RPE hypothesis -- detailed recordings of both DA and hippocampal neurons while novel stimulation is applied, repeated, and then omitted.

Additional experimentation is necessary to confirm the interchangeability of reward and novelty.  Fortunately, the novelty adapted RPE offers strong behavioral predictions.  Some speculative experimental designs: 

\begin{enumerate}
	\item Presenting novel images along side a varying degrees of negative feedback.  This approach could allow for a ``titration'' establishing the relative value of novelty.  For example a low valued piece of negative feedback could be paired with a novel image.  This might lead to a total positive to DA spike, which would act to reinforce what would have otherwise been a associative weakening.  However as the magnitude of negative feedback increases, negative outcomes may come to dominate.
	\item Omission of expected feedback typically leads to a depression in SNc/VTA/DA activity.  If omission is repeated, weakening of the previously established association occurs.  If however novel images replaced expected feedback, the resulting DA spike should prevent weakening.
	\item Following a similar line of thought, it might be possible to replace explicit reward in a typical S-R task (where say an abstract image is associated with one of two button presses based on post-press feedback) with images of novel stimuli. 
	\item Novel images might even have the potential to enhance learning speed, as learning progresses reward becomes expected and DA spiking correspondingly decreases.  However if novel images were to replace reward intermittently, especially along the top half of the learning curve before behavioral plateau when DA spiking is be beginning to decrease, the increased DA spiking might speed up learning.
\end{enumerate}

Separating novelty from expectation violation, I argue, might be possible on the basis of the neural systems they rely.  Expectation violation is detected and processed primarily in the striatum \cite{Brovelli:2008p1754}.  The novelty signal meanwhile originates in the hippocampus \cite{Lisman:2005p5455}.  However this division my prove problematic as the striatum is heavily involved in mediating novelty responses \cite{Wittmann:2008p541}, and the hippocampus is likely involved in storing or processing any incoming visual information, even expected information.  It be then a matter of degree -- novelty leads to a sharp pronounced increase in the hippocampus proceeding striatal change, while expectation violation leads to a sharp rise in activity in the striatum alone.

I have a difficult time distinguishing novelty from either sense or contextual salience as all are studied very similarly. An animals reaction (behavioral or physiological) is measured following unexpected events.  Following the literature, I have drawn a line between them where novelty is unexpected but does not necessarily result in detectable arousal or behavior  This distinction is thin, and likely not supportable.  Saliency might be better defined as simply an event that triggers a behavioral switch or leads to arousal, removing ``unexpected'' from the definition  This reformulated definition, as noted by \citeNP{ODoherty:2004p2581}, has the effect of empirically separating known DA firing patterns from sensory salience.  Repeated negative feedback (such as a electric shocks) will result, eventually, in no DA response, however it is difficult to see how each shock could could be anything but quite salient.  Additionally, the omission of an expected reward is certainly salient (arousing) but leads to a depression not an increase in DA firing \cite{ODoherty:2004p2581,Fiorillo:2003p6375}.  
 
Surprise, I argue, implicitly engenders a conscious (or entirely subjective) component -- the ``feeling of being surprised''.  While I have no doubt that novelty, saliency, and expectation violation all \emph{can} contribute to surprise, I am hesitant to ascribe a direct casual role for any or all of them.  Philosophically, I tend towards Greenfield's argument that conscious experience is derived from widespread patterns of neural activity, rather than from limited -- region specific -- activations, as Koch has argued \cite{Koch:2007p7251}.  
  
% section q3 (end)

\newpage
\section{Q4} % (fold)
\label{sec:q4}
\emph{Much research in dopamine function in addiction has been based on the incentive salience theory, which makes a distinction between “liking” and “wanting” drugs, and posits a primary role of dopamine in the latter process.  How does the “wanting” theory differ from RPE theories of dopamine function?  Can the two theories be reconciled, and if so, how?  More broadly, what aspects of addiction can be accounted for by RPE based theories, and where does RPE fall short?}

Tyrosine hydroxylase knockout (DD) mice, mice without detectable levels of DA in the brains, can learn a reward contingent T-maze tasks (where the animal must move left or right at the end of a corridor) -- though they do not act on that learning till DA is restored \cite{Berridge:2007p7235}.  An important caveat: in order to get the mice to run the maze they were administered a stimulant, caffeine.  Caffeine likely acts to enhance LTP in the striatum, similar to DA's proposed mechanism of action \cite{Rossi:2010p7252}.  DD mice do however display a reward preference when given the choice of a sweetened water versus untreated water. However unless DA is restored their overall desire for either is greatly decreased.  Based on these findings \citeNP{Berridge:2007p7235}, argue that DA is not necessary nor sufficient for reward driven learning.  That is DA is not a casual agent in S-R learning.  Combined with studies of addicts (and there non-human animal equivalents) who display increased ``wanting'' of drugs but not ``liking'' (people rate the experience as no more pleasurable than controls, mice consume no more of the substance that controls) \citeNP{Berridge:2007p7235}, argue that phasic DA signals a stimulus' incentive salience, a synonym for wanting or desire.  

Taken in isolation these experiments in DD mice are quite damning to the notion of DA playing a casual role in S-R learning.  There is however a substantial body of work suggesting the opposite.  Administering human and non-human animals dopamine antagonists adversely effects S-R learning \cite{Pizzagalli:2010p7205}, as does lesioning either the VTA/SNc or portions of the striatum.  Complete lesions of the striatum prevent S-R learning \cite{Packard:2002p5074}.  This is relevant as it is the interaction between phasic DA and the striatum that is proposed to guide (drive) S-R learning. Administering DA agonists, or readily converted DA precursor L-DOPA, leads to increased pavlovian instrumental transfer, as well as response vigor \cite{Winterbauer:2007p6352}, both of which are thought to be facilitated by the interaction between phasic DA and activity in the ventral striatum.  Parkinson's patients when taken off medication, and therefore are lacking striatal DA, show marked decreases in S-R learning \cite{Pizzagalli:2010p7205}.  These same off-medication patients show an enhanced (compared to normal age and intellect matched controls) capability to learn from negative feedback, suggesting their ability and desire to act is intact \cite{Frank:2004p4709}.  

Like the unfavored hedonia hypothesis of DA function, which suggested phasic DA signaled the pleasure or rewarding properties of a stimulus, the incentive salience hypothesis seems unable to deal with back-propagation of phasic DA from reward to a informative stimulus. If phasic DA signals incentive salience (or reward) directly, and DA activity shifts to an informative stimulus, or simply declines as it becomes well predicted, why does want (or pleasure) of feedback continue, more or less unabated?  

Regardless of the truth of the incentive salience hypothesis, the following is well established. (1) addicts want but do not like more than controls and that (2) DA is heavily involved in addiction.  It seems reasonable to ask how the RPE hypothesis can account for these facts, if it in fact can.  

Two theoretical accounts have been offered to reconcile the RPE with incentive salience.  The first of these is the model of \citeNP{Aldridge:2009p7243}, while nominally a model proposed to support incentive salience, it is a direct replication of the TD(0) reinforcement learning model (see \emph{Introduction}), with the addition a single free parameter ($\kappa$).  This parameter, they propose, acts multiplicatively on the reward term ($r(t)*\kappa$) to provide an estimate of desire.  Unfortunately they left the practical means to estimate $\kappa$, any proof towards the stability or optimality of their model, as well as the biological plausibility of such a parameter rather under-discussed.  As I outlined in the introduction, I do view any modifications to the underlying math a direct challenge to the truth of the RPE.  Their modification is minor enough, that if it were to prove more accurate in predicting phasic DA, I think it could be incorporated easily enough.  That is assuming the casual role of DA in S-R learning is maintained, and their parameter of wanting acts to modulate the overall rate of learning.  

The second unifying model leaves the RPE untouched but adds a prospective component to how the RPE is used. \citeNP{McClure:2003p6555}, proposed that the concept of incentive salience is embodied in the expected future reward and that an agent faced with several possible alternative serially considers their options.  Each considered hypothetical action is compared to its previous value, resulting in a RPE.  This prospected RPE then reflects how much each of the possible states is wanted by the agent; What it wants is equal to what each is worth.  The authors demonstrated that this model matched phasic DA firing taken from rats participating in a maze task designed to gauge and animals desire for reward.  Despite these two models, very little hard data exists on the link between wanting and the RPE hypothesis.  Though investigation will continue, I wonder if it will eventually be confounded by the subjective implications of wanting.  It is still far from clear how isolated neural events play out in the conscious mind \cite{Koch:2007p7251}.   

\citeNP{Redish:2004p2531}, offered an alternative account of addictive behavior, that while not explaining the tendency for wanting versus liking a drug, it does offer a mechanistic account for DAs role in habit formation.  He argued that as most addictive drugs elevate DA levels (directly in the case of cocaine or indirectly in the case of nicotine or heroin) this has the same effect as a positive prediction error, recent actions are reinforced. However unlike a normal RPEs, which decline as the reward is expected (which as side effect limits how rewarded an action can become) the drug induced DA increases do not diminish substantially with repeated exposure (and if it did the addict would consume more drug to compensate).  Thus, drug induced RPEs continue to reinforcement prior actions in a unbounded fashion, or at least until the dynamic range of cellular learning is exhausted.  This leads drug associated actions to gain a much greater relative value compared to ``naturally'' reinforced options.  This proposal has to date though received limited empirical support.  \citeNP{Menon:2007p6529}, found enhanced and diminished BOLD/RPE correlations when human subjects were given single doses of, respectively, DA agonist or antagonist.
% section q4 (end)

\newpage
\section{Q5} % (fold)
\label{sec:q5}
\emph{What has been the most significant contribution in cognitive neuroscience in the past 10 years?  Why is it the most significant?  What could not have been accomplished without this contribution?}

FMRI, until recent developments, has been primarily a means to map task-related activations across the whole human brain.  In that it has proved remarkably useful.  With the advent of event related techniques, these maps could be further refined to intra-task components (e.g. isolating stimulus-related activity from reward delivery \cite{Knutson:2001p5245}).  Despite rhetorical attacks (accusations of neo-phrenology \cite{Friston:2002p7254}) knowing where neural operation occurs does serve as a useful starting point in understanding the how the brain accomplishes its various operations.  Initial steps in employing fMRI to understand \emph{how} (and not where) the brain operates have certainly been taken.  Including monitoring how activity changed with learning \cite{Seger:2005pd} or how regions vary with tasks designed to isolate cognitive processes (e.g. recall,familiarity and novelty processing in the hippocampus \cite{Daselaar:2006p7256}).  Employing the output of computational models as regressors in the analysis of fMRI data also holds much promise \cite{ODoherty:2007p5537}. All these approaches and analyses could, in principle, be accomplished employing neurophysiological recordings of single cells in non-human animals\footnote{Though of course these findings would have to be confirmed in humans, to be generalized to our species anyway.}. Many might take longer, as cellular recordings are notoriously time consuming and technically difficult.  However they do offer much improved temporal resolution, as well as allow for much finer grained functional classifications.  Nearly all studies of cellular firing rates show that closely located cells often display multiple distinct tuning profiles.  For example, \citeNP{Lau:2007p742}, reported that some striatal cells code for the value of an upcoming action while others respond to reward receipt.  There is however one fMRI analysis that cannot be accomplished by any other available and widely practical means  -- effective or functional connectivity.

Creating a connectome (that is a complete map of neural connections in the brain) has been lauded as the next great step that is needed to begin to understand the distributed neural nets that make up the brain \cite{sporns:2005aa}.  And MRI may play an important role via diffusor tensor imaging of white matter tracts.  Still if the connectome were to be complete tomorrow it would still lack at least one important feature.  It is well established that the degree to which brain regions communicate vary depending on task demands \cite{Friston:2002p7254}.  It is also likely that how communication varies strongly impacts both the internal function of brain regions, but also their behavioral relevance \cite{Friston:2002p7254,Brezina:2010p7230}.  However tracking shifting communication patterns among brain regions is a difficult task for cellular recording studies (where recording more than three of four region simultaneously is a technical impossibility) and for EEG with its poor spatial localization.  Fortunately, over the last few years fMRI data analysis techniques have been developed to accomplish precisely this task.  Functional connectivity examines the statistical dependency between two BOLD time series recorded from different voxels or regions (example in coherence ananlysis \cite{Sun:2004p2973} and granger causality \cite{Goebel:2003p2019}).   Effective connectivity analysis involves the more ambitious attempt to model effects of one brain region on another, mediated by known anatomical connections between them. The latter is generally done with a structural equation model, thus connects are directional and specified \emph{a piori}.  The latter is more suitable for hypothesis testing, while the former is often used to locate novel task dependent interactions.

Examples of functional connectivity approaches include: \citeNP{Sun:2007p2974}, who reported that the sensorimotor, premotor, and supplementary motor cortices have significantly greater coupling during the early learning of a motor-sequence task compared to later in learning.  Most interesting though, the learned sequence displayed no cross-cortical communication.   Once learned, recall was localized. \citeNP{DRfel:2009p7260}, showed that the presumed separate memory systems underlying recall and familiarity share a common neural substrate, the precuneus. \citeNP{Burianova:2010p7261}, demonstrated a previously undiscovered network, linking the left hippocampus, the left lingual gyrus, and the right caudate nucleus, underlie autobiographical, episodic, and semantic memory retrieval (processes whose commonality or separability has been debated).  In a finding relevant to both the reconsolidation of memory and the treatment of post-traumatic stress disorder, \citeNP{Ritchey:2008p7262} reported that medial temporal lobes capability to store emotional memories overtime was dependent on the strength of its connection to the amygdala.  Functional connection measure have also been employed to further refine the role of the striatum in S-R learning.  \cite{Seger:2010p7188}, demonstrated that posterior caudate received input from visual cortices while the putamen influenced motor cortex and that putamen (and its motor related activity influenced feedback processing in the anterior caudate.

Effective connectivity analyses along with administration of a single dose of both a dopamine antagonist and antagonist found that directed connectivity from the VTA/SNc to the dorsal caudate was strongly modulated by DA level \cite{Honey:2003p1595}. Coupling increased and decreased with agonist and antagonist, respectively.  None of the other examined regions of interest (the thalamus and the dorsolateral prefrontal cortex -- both of which receive dopaminergic input) exhibited this modulation though all regions were active in standard a task-baseline contrast.  In a experiment relevant to the RPE hypothesis, \citeNP{denOuden:2010p7203}, employed a RPE estimate as a mediator in a structural equation model to show that trial-by-trial striatum-generated prediction errors alters the efficacy of connections between visual and motor cortices.  

In conclusion, understanding how various brain region's communications shift based on task demands, how communication changes alter a regions function as well as the role neuromodulation plays in these interactions will be crucial to understanding the how brain generates behavior.  In that regard,  functional and effective analysis for fMRI data offer great promise.  Still, many of these techniques are still new.  Their efficacy, and mutual compatibility (that is due different methods find similar connection patterns) is not fully established, though tools for rapidly comparing measures are on the horizon \cite{Cui:2008p944}.  However, important restrictions on their efficacy may emerge.  For example, \citeNP{Deshpande:2010p7259}, reported that regional variations in the BOLD response may lead to erroneous connectivity patterns.


% section q5 (end)
\newpage
\bibliography{bib}




%%%%%%%%%%%%%
\end{document}
%%%%%%%%%%%%%
